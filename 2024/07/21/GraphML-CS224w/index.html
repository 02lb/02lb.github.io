<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="图表示学习&#x2F;图机器学习CS224W: Machine Learning with Graphs GuideLines Methods for node embeddings: DeepWalk, Node2Vec  Graph Neural Networks: GCN, GraphSAGE, GAT…  Graph Transformers  Knowledge graphs and r">
<meta property="og:type" content="article">
<meta property="og:title" content="GraphML-CS224w[图机器学习]">
<meta property="og:url" content="https://02lb.github.io/2024/07/21/GraphML-CS224w/index.html">
<meta property="og:site_name" content="Bo Li’s Blog">
<meta property="og:description" content="图表示学习&#x2F;图机器学习CS224W: Machine Learning with Graphs GuideLines Methods for node embeddings: DeepWalk, Node2Vec  Graph Neural Networks: GCN, GraphSAGE, GAT…  Graph Transformers  Knowledge graphs and r">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085223291.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084716757.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084751025.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084807550.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-21%2017.02.31.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-21%2017.07.23.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085237389.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2015.41.27.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.11.46.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.18.05.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.20.58.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.31.53.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.38.30.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.42.25.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.43.26.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.44.18.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2017.05.24.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085117822.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2009.22.39.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723104152426.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723104136425.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723105202851.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2011.54.37.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723120112059.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2012.00.31.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723123526056.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.08.33.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-26%2010.50.16.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.02.59.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.05.34.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.18.21.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.20.11.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729111639298.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.16.31.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.18.03.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.18.22.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729111944075.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.22.37.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729122411091.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.05.59.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.10.50.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.18.02.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.22.48.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.28.52.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.28.08.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.30.35.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.40.29.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.43.28.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.52.13.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.53.39.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.25.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.44.51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.45.04.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.51.25.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.54.07.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2011.33.50.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2013.13.51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2013.12.57.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2006.28.40.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2007.40.51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2007.51.22.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.45.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2009.10.27.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2009.40.51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2010.36.13.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2011.12.09.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2011.28.00.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2014.01.56.png">
<meta property="og:image" content="https://02lb.github.io/Users/lbyyds/Library/Application%20Support/typora-user-images/%E6%88%AA%E5%B1%8F2024-08-06%2014.04.23.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2018.41.18.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.53.png">
<meta property="article:published_time" content="2024-07-21T09:26:48.000Z">
<meta property="article:modified_time" content="2024-08-09T01:42:58.015Z">
<meta property="article:author" content="Lee">
<meta property="article:tag" content="图机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085223291.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.png">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>GraphML-CS224w[图机器学习]</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 7.1.1"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/02lb">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/07/22/RecSys/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/04/12/docker/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://02lb.github.io/2024/07/21/GraphML-CS224w/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&text=GraphML-CS224w[图机器学习]"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&is_video=false&description=GraphML-CS224w[图机器学习]"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=GraphML-CS224w[图机器学习]&body=Check out this article: https://02lb.github.io/2024/07/21/GraphML-CS224w/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&name=GraphML-CS224w[图机器学习]&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://02lb.github.io/2024/07/21/GraphML-CS224w/&t=GraphML-CS224w[图机器学习]"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">图表示学习&#x2F;图机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GuideLines"><span class="toc-number">1.0.0.1.</span> <span class="toc-text">GuideLines</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day1"><span class="toc-number">1.1.</span> <span class="toc-text">Day1:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Node-Embedding"><span class="toc-number">1.1.1.</span> <span class="toc-text">Node Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Encoder-Decoder-Framework"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Encoder - Decoder Framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0%EF%BC%9ARandom-Walk%EF%BC%9A"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">随机游走：Random Walk：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%9D%E6%83%B3%EF%BC%88Idea%EF%BC%89"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text">思想（Idea）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%EF%BC%88Expressivity%EF%BC%89"><span class="toc-number">1.1.1.2.2.</span> <span class="toc-text">表达能力（Expressivity）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7-Negative-Sampling%EF%BC%9A"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">负采样 Negative Sampling：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.3.1.</span> <span class="toc-text">负采样的原理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%95%B0%E9%87%8F-K-%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.3.2.</span> <span class="toc-text">负样本数量 (K) 的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.3.3.</span> <span class="toc-text">负样本的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%B4%9F%E9%87%87%E6%A0%B7%E5%85%AC%E5%BC%8F%E5%8F%8A%E5%85%B6%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.3.4.</span> <span class="toc-text">解释负采样公式及其原理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Node-Embedding-%EF%BC%9A-DeepWalk-Node2Vec"><span class="toc-number">1.1.2.</span> <span class="toc-text">Node Embedding ： DeepWalk &#x2F; Node2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Embedding-%EF%BC%9A%E5%9B%BE%E5%B5%8C%E5%85%A5"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">Graph Embedding ：图嵌入</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day1-Lab"><span class="toc-number">1.2.</span> <span class="toc-text">Day1-Lab:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day2"><span class="toc-number">1.3.</span> <span class="toc-text">Day2:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CORE%EF%BC%9AGraph-Neural-Network"><span class="toc-number">1.3.1.</span> <span class="toc-text">CORE：Graph Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-vs-Image"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Graph vs Image</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-Basics"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">GNN Basics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Classic-GNN"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Classic GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day3"><span class="toc-number">1.4.</span> <span class="toc-text">Day3</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9%EF%BC%9AGNN"><span class="toc-number">1.4.0.1.</span> <span class="toc-text">主要内容：GNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Manipulation%E3%80%90%E5%9B%BE%E5%A4%84%E7%90%86%E3%80%91"><span class="toc-number">1.4.0.2.</span> <span class="toc-text">Graph Manipulation【图处理】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#unsupervised-self-supervised"><span class="toc-number">1.4.0.3.</span> <span class="toc-text">unsupervised &#x2F; self- supervised</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">1.4.0.4.</span> <span class="toc-text">评估指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%92%E5%88%86"><span class="toc-number">1.4.0.5.</span> <span class="toc-text">图数据的划分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Power-of-GNN"><span class="toc-number">1.4.0.6.</span> <span class="toc-text">The Power of GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day3-Lab"><span class="toc-number">1.5.</span> <span class="toc-text">Day3-Lab</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84GCN%E7%94%A8%E4%BA%8E%E8%8A%82%E7%82%B9%E9%A2%84%E6%B5%8B"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">实现一个简单的GCN用于节点预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day4"><span class="toc-number">1.6.</span> <span class="toc-text">Day4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Heterogenous-graphs-%E5%BC%82%E8%B4%A8%E5%9B%BE"><span class="toc-number">1.6.0.1.</span> <span class="toc-text">Heterogenous graphs[异质图]</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%88Relational-Graph-Convolutional-Network-R-GCN%EF%BC%89"><span class="toc-number">1.6.0.2.</span> <span class="toc-text">关系图卷积网络（Relational Graph Convolutional Network, R-GCN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-vs-Heterogenous-GNN"><span class="toc-number">1.6.0.3.</span> <span class="toc-text">GNN vs Heterogenous GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day5"><span class="toc-number">1.7.</span> <span class="toc-text">Day5</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Knowledge-Graphs"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">Knowledge Graphs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">关系模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KG-Completion"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">KG Completion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KG-Reasoning"><span class="toc-number">1.7.0.4.</span> <span class="toc-text">KG Reasoning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Query2Box"><span class="toc-number">1.7.0.5.</span> <span class="toc-text">Query2Box</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day6"><span class="toc-number">1.8.</span> <span class="toc-text">Day6</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Fast-neural-subgraph-matching-and-Counting"><span class="toc-number">1.8.0.1.</span> <span class="toc-text">Fast neural subgraph matching and Counting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%90%E5%9B%BE"><span class="toc-number">1.8.0.2.</span> <span class="toc-text">子图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Isomorphism-%E5%9B%BE%E5%90%8C%E6%9E%84"><span class="toc-number">1.8.0.3.</span> <span class="toc-text">Graph Isomorphism 图同构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-motifs-%E7%BD%91%E7%BB%9C%E5%9B%BE%E6%A1%88"><span class="toc-number">1.8.0.4.</span> <span class="toc-text">Network motifs 网络图案</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Neural-Subgraph-Match"><span class="toc-number">1.8.0.5.</span> <span class="toc-text">Neural Subgraph Match</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9A%E4%BD%BF%E7%94%A8Order-Embedding-%E6%9C%89%E5%BA%8F%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4"><span class="toc-number">1.8.0.6.</span> <span class="toc-text">方法：使用Order Embedding 有序嵌入空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%89%BE%E5%87%BA-Frequent-SubGraphs%E3%80%90%E5%8D%B3-motifs%E3%80%91"><span class="toc-number">1.8.0.7.</span> <span class="toc-text">如何找出 Frequent SubGraphs【即 motifs】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SPMiner"><span class="toc-number">1.8.0.8.</span> <span class="toc-text">SPMiner</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day7"><span class="toc-number">1.9.</span> <span class="toc-text">Day7</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8GNN%E8%BF%9B%E8%A1%8C%E6%8E%A8%E8%8D%90"><span class="toc-number">1.9.1.</span> <span class="toc-text">利用GNN进行推荐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84Graph"><span class="toc-number">1.9.1.1.</span> <span class="toc-text">推荐系统中的Graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Embedding-based-models"><span class="toc-number">1.9.1.2.</span> <span class="toc-text">Embedding-based models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#representative-GNN-approaches-for-recommender-systems"><span class="toc-number">1.9.1.3.</span> <span class="toc-text">representative GNN approaches for recommender systems</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81Neural-Graph-Collaborative-Filtering-NGCF"><span class="toc-number">1.9.1.4.</span> <span class="toc-text">一、Neural Graph Collaborative Filtering (NGCF)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81LightGCN"><span class="toc-number">1.9.1.5.</span> <span class="toc-text">二、LightGCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%B0%86%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%EF%BC%9FPinSage"><span class="toc-number">1.9.1.6.</span> <span class="toc-text">如何将扩展到大规模的推荐系统中？PinSage</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day8"><span class="toc-number">1.10.</span> <span class="toc-text">Day8</span></a></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        GraphML-CS224w[图机器学习]
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Lee</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-07-21T09:26:48.000Z" class="dt-published" itemprop="datePublished">2024-07-21</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">图机器学习</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="图表示学习-图机器学习"><a href="#图表示学习-图机器学习" class="headerlink" title="图表示学习&#x2F;图机器学习"></a>图表示学习&#x2F;图机器学习</h1><p><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs224w/">CS224W: Machine Learning with Graphs</a></p>
<h4 id="GuideLines"><a href="#GuideLines" class="headerlink" title="GuideLines"></a>GuideLines</h4><ul>
<li>Methods for node embeddings: DeepWalk, Node2Vec </li>
<li>Graph Neural Networks: GCN, GraphSAGE, GAT… </li>
<li>Graph Transformers </li>
<li>Knowledge graphs and reasoning: TransE, BetaE </li>
<li>Generative models for graphs: GraphRNN </li>
<li>Graphs in 3D: Molecules § Scaling up to large graphs </li>
<li>Applications to Biomedicine, Science, Technology</li>
</ul>
<h2 id="Day1"><a href="#Day1" class="headerlink" title="Day1:"></a>Day1:</h2><h3 id="Node-Embedding"><a href="#Node-Embedding" class="headerlink" title="Node Embedding"></a>Node Embedding</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085223291.png" alt="image-20240723085223291"></p>
<blockquote>
<p>Core idea: <strong>Embed nodes so that distances in embedding space reflect node similarities in the original network.</strong></p>
</blockquote>
<p>对节点进行自动的特征提取-自动嵌入&#x2F;编码：被下游任务利用</p>
<h4 id="Encoder-Decoder-Framework"><a href="#Encoder-Decoder-Framework" class="headerlink" title="Encoder - Decoder Framework"></a><strong>Encoder - Decoder Framework</strong></h4><ul>
<li>encoder 就是node embedding</li>
<li>decoder 就是对节点vector 做相似度提取：例如内积相似度</li>
</ul>
<h4 id="随机游走：Random-Walk："><a href="#随机游走：Random-Walk：" class="headerlink" title="随机游走：Random Walk："></a><strong>随机游走：Random Walk：</strong></h4><ul>
<li><h5 id="思想（Idea）"><a href="#思想（Idea）" class="headerlink" title="思想（Idea）"></a>思想（Idea）</h5><p>  核心思想是：如果从节点 u 开始的随机游走高概率访问节点 v，那么 u 和 v 是相似的。具体解释如下：</p>
<ol>
<li><strong>访问概率高</strong>：从节点 u 开始的随机游走如果高概率访问节点 v，这意味着 u 和 v 之间的路径很多，或者有很多共同的邻居。这说明 u 和 v 在图中的结构位置很相似，具有相似的网络结构特征。</li>
<li><strong>多跳信息</strong>：随机游走不仅考虑直接的邻居关系，还会通过多跳路径访问更远的节点。这种方式可以捕捉到网络中更复杂的结构信息和节点之间的关系。例如，两个节点即使不直接相连，但如果它们通过多个中间节点有较高的访问概率，那么它们之间仍然具有潜在的相似性。</li>
</ol>
</li>
<li><h5 id="表达能力（Expressivity）"><a href="#表达能力（Expressivity）" class="headerlink" title="表达能力（Expressivity）"></a>表达能力（Expressivity）</h5><p>  随机游走路径通过灵活的随机定义方式捕捉节点之间的相似度，能够同时包含局部和高阶的邻域信息：</p>
<ol>
<li><strong>局部信息</strong>：随机游走会优先访问与起始节点直接相连的节点，这样能够捕捉节点的局部结构信息。如果两个节点在局部结构上相似（例如，它们有很多共同邻居），那么随机游走路径会较高概率访问到这些共同邻居。</li>
<li><strong>高阶信息</strong>：随着随机游走的步数增加，它会逐渐扩展到更多的邻居节点，甚至是更远的节点。这样可以<strong>捕捉到多跳（multi-hop）的关系信息，即高阶邻域信息。</strong>如果两个节点在网络中的更大范围内具有相似的连接模式，随机游走路径也能反映这一点。</li>
</ol>
</li>
<li><p>随机游走最大化目标节点embedding的对数似然（使得目标节点周围节点的似然概率最大化，原理是周围节点在嵌入向量空间时距离也相对近），这个计算过程可以使用负采样进行优化。</p>
</li>
</ul>
<h4 id="负采样-Negative-Sampling："><a href="#负采样-Negative-Sampling：" class="headerlink" title="负采样 Negative Sampling："></a>负采样 Negative Sampling：</h4><ul>
<li><h5 id="负采样的原理"><a href="#负采样的原理" class="headerlink" title="负采样的原理"></a>负采样的原理</h5><p>  <strong>负采样的核心思想是用一部分负样本来近似整个负样本空间，从而减少计算开销。</strong>具体步骤如下：</p>
<ol>
<li><strong>选择正样本</strong>：即实际存在的节点对（例如，图中的实际边）。</li>
<li><strong>选择负样本</strong>：随机选择一些节点对（图中不存在的边），这些对作为负样本。</li>
</ol>
</li>
<li><h5 id="负样本数量-K-的选择"><a href="#负样本数量-K-的选择" class="headerlink" title="负样本数量 (K) 的选择"></a>负样本数量 (K) 的选择</h5><ul>
<li><strong>更高的 (K) 值</strong>：提供更鲁棒的估计，因为更多的负样本能够更好地近似整个负样本空间。但是这也会增加计算开销。</li>
<li><strong>更低的 (K) 值</strong>：减少计算开销，但可能会增加对负事件的偏差。</li>
</ul>
<p>  在实际应用中， (K) 通常选择在5到20之间，以平衡计算效率和估计精度。</p>
</li>
<li><h5 id="负样本的选择"><a href="#负样本的选择" class="headerlink" title="负样本的选择"></a>负样本的选择</h5><p>  负样本可以是任何节点，不一定要与随机游走无关。为了提高效率，常常会从所有节点中随机选择负样本，而不是仅从未在随机游走中出现的节点中选择。</p>
</li>
<li><h5 id="解释负采样公式及其原理"><a href="#解释负采样公式及其原理" class="headerlink" title="解释负采样公式及其原理"></a>解释负采样公式及其原理</h5><p>  负采样是一种用于<strong>提高训练效率和计算效率的方法</strong>，尤其是在处理大规模数据集时。下面我们详细解释给定的公式和相关概念。</p>
<p>  给定的公式：<br>  $$<br>  \log \left( \frac{\exp(\mathbf{z}_v^\top \mathbf{z}<em>u)}{\sum</em>{n \in N} \exp(\mathbf{z}_n^\top \mathbf{z}_u)} \right) \approx \log \sigma(\mathbf{z}_v^\top \mathbf{z}<em>u) + \sum</em>{k&#x3D;1}^K \log \sigma(-\mathbf{z}_n^\top \mathbf{z}_u)<br>  $$</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084716757.png" alt="image-20240723084716757" style="zoom: 50%;" />
  
<p>  其中：</p>
<ul>
<li>$\mathbf{z}_v$ 和 $\mathbf{z}_u$是节点$v$ 和 $u$ 的嵌入向量。</li>
<li>$\sigma(x)$是sigmoid函数，定义为 $\sigma(x) &#x3D; \frac{1}{1 + \exp(-x)}$。</li>
<li>$N$ 是所有节点的集合。</li>
<li>$K$ 是负样本的数量。</li>
<li>$\mathbf{z}_n$ 是负样本节点的嵌入向量。</li>
</ul>
<p>  公式表示：</p>
<ul>
<li><p>左边的<br>  $$<br>  \log \left( \frac{\exp(\mathbf{z}_v^\top \mathbf{z}<em>u)}{\sum</em>{n \in N} \exp(\mathbf{z}_n^\top \mathbf{z}_u)} \right)<br>  $$</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084751025.png" alt="image-20240723084751025" style="zoom:50%;" />

<p>  是softmax的对数。</p>
</li>
<li><p>右边的<br>  $$<br>  \log \sigma(\mathbf{z}_v^\top \mathbf{z}<em>u) + \sum</em>{k&#x3D;1}^K \log \sigma(-\mathbf{z}_n^\top \mathbf{z}_u)<br>  $$</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084807550.png" alt="image-20240723084807550" style="zoom:50%;" />
  
<p>  是<strong>负采样的对数近似。</strong></p>
</li>
</ul>
<p>  <strong>&#x3D;&#x3D;通过负采样，我们可以将计算从整个节点集合的softmax归约到少量负样本的log-sigmoid函数计算，从而大大减少计算复杂度。&#x3D;&#x3D;</strong></p>
</li>
</ul>
<h3 id="Node-Embedding-：-DeepWalk-Node2Vec"><a href="#Node-Embedding-：-DeepWalk-Node2Vec" class="headerlink" title="Node Embedding ： DeepWalk &#x2F; Node2Vec"></a>Node Embedding ： DeepWalk &#x2F; Node2Vec</h3><blockquote>
<p><strong>重点内容</strong></p>
</blockquote>
<ul>
<li><p>DeepWalk：RandomWalk + 词嵌入模型（如Word2Vec）</p>
</li>
<li><p>Node2Vec：DeepWalk + 利用了参数p、q进行BFS&#x2F;DFS的随机游走，提供了biased的路径选择。</p>
<blockquote>
<p>引入了更灵活的随机游走策略，通过调整参数 p 和 q 来控制游走的行为，从而在深度优先搜索（DFS）和广度优先搜索（BFS）之间进行平衡。</p>
</blockquote>
</li>
<li><p>利用随机游走得到的node-seq视为word2vec的词序列进行embedding</p>
</li>
<li><p><strong>Limitations：</strong></p>
<ul>
<li><p><strong>模型不能推广到训练和测试集中未见过的新节点或新结构：</strong>若new node到来，需要重新计算整个graph的node embedding，而不是增量可扩展的。</p>
</li>
<li><p><strong>无法捕捉结构相似性</strong>：它们只是用过节点的相邻性来判断空间相似性（距离），而不考虑任何结构信息</p>
<blockquote>
<p>主要捕捉的是<strong>节点的同质性（homophily）特征</strong>，而不是<strong>结构相似性（structural similarity）</strong>。这意味着它们倾向于将相邻或近邻节点（即在图中距离较近的节点）映射到相似的嵌入空间中，而不是将具有相似结构但在图中距离较远的节点映射到相似的嵌入空间中。</p>
<p>节点同质性假设是指在图中距离较近的节点往往具有相似的特征或属性。例如，在社交网络中，朋友之间的兴趣爱好往往相似。</p>
<p>结构相似性是指两个节点在图中的角色或位置相似，即使它们在图中距离较远。例如，在公司组织图中，两个不同部门的经理可能具有类似的结构角色，即使他们不直接连接。</p>
</blockquote>
</li>
<li><p>Solution to these limitations: <strong>Deep Representation Learning and Graph Neural Networks</strong></p>
</li>
</ul>
</li>
</ul>
<h4 id="Graph-Embedding-：图嵌入"><a href="#Graph-Embedding-：图嵌入" class="headerlink" title="Graph Embedding ：图嵌入"></a>Graph Embedding ：图嵌入</h4><ul>
<li><p>Naive：将所有node的嵌入向量进行结合操作（累加&#x2F;avg&#x2F;……)</p>
</li>
<li><p>Approach2: 添加一个 上帝虚节点 </p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-21%2017.02.31.png" style="zoom: 25%;" />
</li>
<li><p>Aprroach3 ：分层&#x2F;pool：<strong>DiffPool（Differentiable Pooling）</strong></p>
<blockquote>
<p>是一种图神经网络（GNN）池化方法，用于在图上进行层次化聚类，并根据这些聚类对节点嵌入进行求和或平均。</p>
</blockquote>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-21%2017.07.23.png" style="zoom: 33%;" />



<h2 id="Day1-Lab"><a href="#Day1-Lab" class="headerlink" title="Day1-Lab:"></a>Day1-Lab:</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1vvIoEqxGl1naopTZbh4bmCOLEiCxvcQq">Link</a></p>
</blockquote>
<p>熟悉 NetworkX 以及 PyG 的用法；简单的 NodeEmbedding方法</p>
<h2 id="Day2"><a href="#Day2" class="headerlink" title="Day2:"></a>Day2:</h2><h3 id="CORE：Graph-Neural-Network"><a href="#CORE：Graph-Neural-Network" class="headerlink" title="CORE：Graph Neural Network"></a>CORE：Graph Neural Network</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085237389.png" alt="image-20240723085237389"></p>
<blockquote>
<p>之前的Node Embedding方法（简单的 encoder - decoder）没有考虑节点结构的信息，只是使用节点距离进行空间嵌入；这里使用GNN深度学习的方法学习节点的嵌入；</p>
</blockquote>
<h4 id="Graph-vs-Image"><a href="#Graph-vs-Image" class="headerlink" title="Graph vs Image"></a>Graph vs Image</h4><ul>
<li><p>There is no fixed notion of locality or sliding window on the graph (图上<strong>没有固定的局部性或滑动窗口概念</strong>)</p>
</li>
<li><p>辨析：<strong>排列不变性（permutation invariant）</strong>，不存在一种对节点的权威的排序，任何排序应该有相同的结果；<strong>排列等变性（permutation equivariant）</strong>；</p>
<blockquote>
<p>排列不变性意味着，无论图的节点顺序如何，模型的输出结果应该保持不变。假设我们有一个图 $G$，其邻接矩阵为 $A$，节点特征矩阵为 $X$，则排列不变性可以表示为：</p>
<p>$ f(A, X) &#x3D; f(PAP^T, PX) $</p>
<p>其中，$P$ 是一个任意的排列矩阵，它可以重新排列图的节点。排列不变性的一个例子是将图映射到一个固定长度的向量。无论如何重新排列输入图的节点，输出向量始终保持不变。</p>
</blockquote>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2015.41.27.png" alt="截屏2024-07-22 15.41.27" style="zoom: 25%;" />



<h4 id="GNN-Basics"><a href="#GNN-Basics" class="headerlink" title="GNN Basics"></a>GNN Basics</h4><ul>
<li><p>Param sharing：参数共享</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.11.46.png" alt="截屏2024-07-22 16.11.46" style="zoom: 25%;" />


</li>
<li><p>GNN beyond CNN&#x2F;Transformers</p>
<ul>
<li><strong>CNNs can be seen as a special GNN with fixed neighbor</strong></li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.18.05.png" alt="截屏2024-07-22 16.18.05" style="zoom:25%;" />

<ul>
<li><strong>Transformer layer can be seen as a special GNN that runs on a fullyconnected “word” graph!</strong></li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.20.58.png" alt="截屏2024-07-22 16.20.58" style="zoom:25%;" /></li>
</ul>
<h4 id="Classic-GNN"><a href="#Classic-GNN" class="headerlink" title="Classic GNN"></a>Classic GNN</h4><ul>
<li><p>一个通用的GNN架构：</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.31.53.png" alt="截屏2024-07-22 16.31.53" style="zoom:25%;" />
</li>
<li><p><strong>GNN Layer &#x3D;（1） Message + （2）Aggregation</strong></p>
<blockquote>
<p>不同的策略 -&gt; 不同的实例：GCN, GraphSAGE, GAT…..</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.38.30.png" alt="截屏2024-07-22 16.38.30" style="zoom: 25%;" />
</li>
<li><p><strong>Graph Convolutional Networks (GCN)：</strong></p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.42.25.png" alt="截屏2024-07-22 16.42.25" style="zoom:25%;" />
</li>
<li><p><strong>GraphSAGE：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.02216">🔗</a></p>
<ul>
<li><p><strong>Transductive to Inductive</strong></p>
</li>
<li><p>GCN直接对整个图进行卷积操作，每一层都需要遍历所有节点及其邻居节点。这种全图卷积的方式在处理大规模图时会遇到内存和计算瓶颈。</p>
</li>
<li><p>GraphSAGE提出了一种基于<strong>采样与聚合</strong>的策略，通过在训练过程中<strong>对节点的邻居进行采样，只选择部分邻居来计算节点的表示</strong>，从而大大减小了计算量和内存需求。主要解决了GCN在大规模图上应用的可扩展性问题。</p>
</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.43.26.png" alt="截屏2024-07-22 16.43.26" style="zoom:25%;" />

<ul>
<li><p><strong>Graph Attention Networks（GAT）：</strong></p>
<blockquote>
<p>Idea: <strong>Not all node’s neighbors are equally important</strong>；the NN should devote more computing power on that small but important part of the data. 【NN应该在数据中那个小而重要的部分上投入更多的计算能力。】</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.44.18.png" alt="截屏2024-07-22 16.44.18" style="zoom:25%;" />

<ul>
<li>Key benefit: Allows for (implicitly) specifying different importance values ($\alpha_{vu}$) to different neighbors</li>
</ul>
</li>
<li><p><strong>over-smoothing problem【过度平滑</strong>】：当stack的GNN层过多时，节点的感知域【Receptive field】可能过大甚至覆盖整个Graph，导致所有节点都感知整个图，使得所有节点趋于相同的embedding；</p>
<blockquote>
<p>Key：the embedding of a node is <strong>determined by its receptive field</strong></p>
</blockquote>
<ul>
<li>过多堆叠GNN Layer往往没用（不像CNN Layer那么有效），所以需要在GNN Layer较少（Shallow）的时候试图增强其感知能力</li>
<li>通过增强其message&#x2F;Aggregation模块的NN的学习能力&#x2F;通过前后concat线性层&#x2F;通过添加 <strong>skip connection</strong>（集成学习视角）</li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2017.05.24.png" alt="截屏2024-07-22 17.05.24" style="zoom:25%;" />
</li>
<li><p>图增强&#x2F;处理：应对稀疏图&#x2F;稠密图&#x2F;图过大等问题进行特征增强&#x2F;图处理</p>
<ul>
<li>Graph Manipulation: Feature augmentation &#x2F; Structure manipulation</li>
<li>后面的内容会cover这部分</li>
</ul>
</li>
</ul>
<h2 id="Day3"><a href="#Day3" class="headerlink" title="Day3"></a>Day3</h2><h4 id="主要内容：GNN"><a href="#主要内容：GNN" class="headerlink" title="主要内容：GNN"></a>主要内容：GNN</h4><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085117822.png" alt="image-20240723085117822"></p>
<h4 id="Graph-Manipulation【图处理】"><a href="#Graph-Manipulation【图处理】" class="headerlink" title="Graph Manipulation【图处理】"></a>Graph Manipulation【图处理】</h4><ul>
<li><p>Graph Feature manipulation </p>
<ul>
<li>The input graph lacks features —— <strong>feature augmentation</strong></li>
</ul>
</li>
<li><p>Graph Structure manipulation </p>
<ul>
<li><p><strong>The graph is too sparse</strong> —— Add virtual nodes &#x2F; edges </p>
<blockquote>
<p>稀疏图：通常添加两跳虚边；例如在作者-文章的二部图中可以连接同一篇paper的coop的作者</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2009.22.39.png" alt="截屏2024-07-23 09.22.39" style="zoom:15%;" />
</li>
<li><p><strong>The graph is too dense</strong> —— Sample neighbors when doing message passing </p>
<blockquote>
<p>消息传递时随机采样而不使用所有的节点；和使用所有的子节点进行训练的期望一致，但是可以减少训练开销</p>
</blockquote>
</li>
<li><p><strong>The graph is too large</strong> —— Sample subgraphs to compute embeddings</p>
</li>
</ul>
</li>
</ul>
<h4 id="unsupervised-self-supervised"><a href="#unsupervised-self-supervised" class="headerlink" title="unsupervised &#x2F; self- supervised"></a>unsupervised &#x2F; self- supervised</h4><blockquote>
<p>我理解的这两个概念的区分：无监督更多的不设置标签例如clustering；自监督需要自己创建的标签；</p>
</blockquote>
<ol>
<li><strong>Node Level：</strong>使用节点的统计量作为标签进行训练， such as clustering coefficient, PageRank, …</li>
<li><strong>Edge Level：</strong>把一些edge进行遮蔽hide&#x2F;mask进行作为标签进行训练</li>
<li><strong>Graph Level：</strong>图同构信息</li>
</ol>
<h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ul>
<li>二分类模型的评估：混淆矩阵 &#x2F; TPR&#x3D;&#x3D;RecallRate &#x2F; ROC（AUC）</li>
<li>ROC曲线的xy坐标为两个rate：FPR以及TPR，性能越好的分类模型TPR&gt;&gt;FPR；如何绘制？对一个模型取不同的<strong>阈值</strong>分别计算FPR以及TPR坐标图中的一个点，形成曲线</li>
</ul>
<h4 id="图数据的划分"><a href="#图数据的划分" class="headerlink" title="图数据的划分"></a>图数据的划分</h4><blockquote>
<p>和其它的NN不相同，图数据集不是分离的数据点，所以需要特殊的数据划分(train&#x2F;valid&#x2F;test)；这一部分的内容设计更加复杂的划分方法，在很多论文有所涉及，这里论述的只是最基础的划分方法。</p>
</blockquote>
<ul>
<li><p><strong>Transductive Setting</strong>【直推】</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723104152426.png" alt="image-20240723104152426" style="zoom:33%;" />

<ol>
<li><p><strong>定义</strong>：在Transductive setting中，训练数据和测试数据的节点都属于同一个图。<strong>模型可以在训练时访问整个图的结构信息</strong>，包括测试节点及其连接，但不能访问测试节点的标签。</p>
</li>
<li><p><strong>特点</strong>：</p>
<ul>
<li><strong>同一个图</strong>：训练和测试都在同一个图上进行，测试节点在训练时是已知的。</li>
<li><strong>全局信息</strong>：模型可以利用整个图的结构信息来进行学习，这包括训练节点和测试节点之间的连接。</li>
<li><strong>目标</strong>：学习节点的嵌入或特征，使得在测试节点上的分类或回归任务表现良好。【Only applicable to node &#x2F; edge prediction tasks】无法在Graph-Level分类相关的任务使用。</li>
</ul>
</li>
</ol>
</li>
<li><p><strong>Inductive Setting</strong>【归纳】</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723104136425.png" alt="image-20240723104136425" style="zoom: 33%;" />

<ol>
<li><p><strong>定义</strong>：在Inductive setting中，训练数据和测试数据的节点属于不同的图。<strong>模型在训练时只能访问训练图的结构和标签信息，测试时模型需要在完全未知的图或新节点上进行预测。</strong></p>
</li>
<li><p>特点：</p>
<ul>
<li><strong>不同的图</strong>：训练和测试在不同的图上进行，或者在同一个图上但测试节点在训练时是未知的。</li>
<li><strong>局部信息</strong>：模型不能利用测试图的结构信息进行训练，只能基于训练图进行学习。</li>
<li><strong>目标</strong>：在新的图或新节点上<strong>泛化良好</strong>，即使这些图或节点在训练时不可见。【Applicable to node &#x2F; edge &#x2F; graph tasks】</li>
</ul>
</li>
</ol>
</li>
<li><p>具体示例：<strong>Transductive</strong> link prediction split：分为四类边【训练用信息边、训练用监督边、验证用边、测试用边】</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723105202851.png" alt="image-20240723105202851" style="zoom: 25%;" />



<h4 id="The-Power-of-GNN"><a href="#The-Power-of-GNN" class="headerlink" title="The Power of GNN"></a>The Power of GNN</h4><ul>
<li>对每个节点的计算可以构建一个计算图【Computational Graph】，对应一颗以该节点为根的子树；在该结构的<strong>aggragation聚合层</strong>为<strong>单射</strong>时其表达能力最强【单射可以区分不同的结构】</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2011.54.37.png" alt="截屏2024-07-23 11.54.37" style="zoom: 25%;" />

<ul>
<li><p>Key observation: **Expressive power of GNNs can be characterized by that of neighbor aggregation functions they use.**【GNNs 的表达能力可以用其使用的 neighbor aggregation functions 所表征】</p>
</li>
<li><p><strong>e.g. failure 案例研究：</strong></p>
<ul>
<li><strong>GCN：</strong>使用平均池化</li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723120112059.png" alt="image-20240723120112059" style="zoom:25%;" />

<ul>
<li><strong>GraphSAGE：</strong>使用最大池化</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2012.00.31.png" alt="截屏2024-07-23 12.00.31" style="zoom:25%;" />

<ul>
<li><p>如上案例所示，GCN and GraphSAGE’s aggregation functions <strong>fail to distinguish some basic multi-sets【可重复元素集合】</strong>; <strong>hence not injective.【因此非most powerful】</strong></p>
</li>
<li><p>THE <strong>most expressive GNN</strong> in the class of message-passing GNNs：<strong>Graph Isomorphism Network (GIN)</strong> </p>
<blockquote>
<p>GIN‘s neighbor aggregation function is injective.</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723123526056.png" alt="image-20240723123526056" style="zoom:25%;" /></li>
</ul>
<h2 id="Day3-Lab"><a href="#Day3-Lab" class="headerlink" title="Day3-Lab"></a>Day3-Lab</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1zunZQaGzLr782y3tkq3492rvw9UyY30I">Link</a></p>
<p>在Colab 2中，将<strong>使用PyTorch Geometric (PyG) 构建自己的图神经网络</strong>，并将该模型<strong>应用于两个Open Graph Benchmark (OGB)数据集</strong>。这两个数据集将用于基准测试你的模型在两个不同图任务上的性能：1）<strong>节点属性预测</strong>，预测单个节点的属性；2）<strong>图属性预测</strong>，预测整个图或子图的属性。</p>
</blockquote>
<h4 id="实现一个简单的GCN用于节点预测"><a href="#实现一个简单的GCN用于节点预测" class="headerlink" title="实现一个简单的GCN用于节点预测"></a>实现一个简单的GCN用于节点预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GCN</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, output_dim, num_layers, dropout, return_embeds=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GCN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># GCNConv层列表</span></span><br><span class="line">        self.convs = torch.nn.ModuleList()</span><br><span class="line">        <span class="comment"># 1D批归一化层列表</span></span><br><span class="line">        self.bns = torch.nn.ModuleList()</span><br><span class="line">        <span class="comment"># log softmax层</span></span><br><span class="line">        self.softmax = torch.nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建第一个GCN层</span></span><br><span class="line">        self.convs.append(torch_geometric.nn.GCNConv(input_dim, hidden_dim))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建剩余的GCN层和批归一化层</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers - <span class="number">1</span>):</span><br><span class="line">            self.convs.append(torch_geometric.nn.GCNConv(hidden_dim, hidden_dim))</span><br><span class="line">            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建输出层</span></span><br><span class="line">        self.convs.append(torch_geometric.nn.GCNConv(hidden_dim, output_dim))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 零化概率</span></span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 是否返回节点嵌入</span></span><br><span class="line">        self.return_embeds = return_embeds</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 重置所有GCN层的参数</span></span><br><span class="line">        <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs:</span><br><span class="line">            conv.reset_parameters()</span><br><span class="line">        <span class="comment"># 重置所有批归一化层的参数</span></span><br><span class="line">        <span class="keyword">for</span> bn <span class="keyword">in</span> self.bns:</span><br><span class="line">            bn.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj_t</span>):</span><br><span class="line">        out = x</span><br><span class="line">        <span class="comment"># 应用第一个GCN层和ReLU激活</span></span><br><span class="line">        out = self.convs[<span class="number">0</span>](out, adj_t)</span><br><span class="line">        out = torch.nn.functional.relu(out)</span><br><span class="line">        out = torch.nn.functional.dropout(out, p=self.dropout, training=self.training)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用剩余的GCN层、批归一化、ReLU和dropout</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.convs) - <span class="number">1</span>):</span><br><span class="line">            out = self.convs[i](out, adj_t)</span><br><span class="line">            out = self.bns[i-<span class="number">1</span>](out)</span><br><span class="line">            out = torch.nn.functional.relu(out)</span><br><span class="line">            out = torch.nn.functional.dropout(out, p=self.dropout, training=self.training)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用最终的GCN层</span></span><br><span class="line">        out = self.convs[-<span class="number">1</span>](out, adj_t)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果return_embeds为False，则应用log softmax</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.return_embeds:</span><br><span class="line">            out = self.softmax(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>



<h2 id="Day4"><a href="#Day4" class="headerlink" title="Day4"></a>Day4</h2><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.08.33.png" alt="截屏2024-07-29 11.08.33"></p>
<h4 id="Heterogenous-graphs-异质图"><a href="#Heterogenous-graphs-异质图" class="headerlink" title="Heterogenous graphs[异质图]"></a>Heterogenous graphs[异质图]</h4><ul>
<li>异质图（Heterogeneous Graph）是指<strong>由不同类型的节点和边构成的图结构</strong>(a graph with multiple relation types)。 在异质图中，节点和边可以具有多样化的属性和关系，代表了不同实体以及它们之间的复杂关联。</li>
<li><strong>定义：</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-26%2010.50.16.png" alt="截屏2024-07-26 10.50.16" style="zoom:25%;" />

<ul>
<li><p>现实问题的Graph大多是异质图</p>
</li>
<li><p>可以将异质图中的节点&#x2F;边的类型编码为节点&#x2F;边的特征（例如one-hot）使得其变为标准图</p>
</li>
<li><p>How To Solve？</p>
</li>
</ul>
<h4 id="关系图卷积网络（Relational-Graph-Convolutional-Network-R-GCN）"><a href="#关系图卷积网络（Relational-Graph-Convolutional-Network-R-GCN）" class="headerlink" title="关系图卷积网络（Relational Graph Convolutional Network, R-GCN）"></a><strong>关系图卷积网络（Relational Graph Convolutional Network, R-GCN）</strong></h4><blockquote>
<p>是一种扩展了传统图卷积网络（GCN）的模型，专门用于处理异质图中的多种关系。<strong>R-GCN通过在图卷积操作中引入关系类型来捕捉不同类型节点和边之间的复杂关系。</strong>这使得R-GCN在处理包含多种关系的复杂图数据时更加有效。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.02.59.png" alt="截屏2024-07-29 10.02.59" style="zoom:25%;" />

<ul>
<li>R-GCN存在问题？参数过多：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.05.34.png" alt="截屏2024-07-29 10.05.34" style="zoom:25%;" />

<ul>
<li><p>每一层的每一种relation类型都对应一个权重$\mathbf{W}_r^{(l)}$，如果关系类型过多，<strong>模型参数量会非常大，容易导致过拟合。</strong></p>
</li>
<li><p>为了防止过拟合，常用的两种正则化方法是<strong>使用块对角矩阵Block Diagonal Matrices</strong>和<strong>基&#x2F;字典学习</strong>：</p>
<ol>
<li><p><strong>使用块对角矩阵：</strong>这种方法通过强制权重矩阵 $\mathbf{W}_r^{(l)}$ 具有块对角结构，从而减少参数数量。具体来说，块对角矩阵<strong>将权重矩阵分成若干个较小的子矩阵（块</strong>），并使得这些子矩阵之间没有相互影响。</p>
<p> 关键思想：<strong>稀疏化权重矩阵</strong></p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.18.21.png" alt="截屏2024-07-29 10.18.21" style="zoom:25%;" />
</li>
<li><p><strong>基&#x2F;字典学习（Basis&#x2F;Dictionary Learning）</strong>:每种关系类型的权重矩阵$\mathbf{W}_r^{(l)}$<strong>由若干基权重矩阵的线性组合表示</strong>。具体来说，定义一组基权重矩阵${ \mathbf{B}_1, \mathbf{B}_2, \ldots, \mathbf{B}_K }$，然后通过关系特定的系数组合这些基权重矩阵来构造每个关系类型的权重矩阵。</p>
<p> 关键思想：使不同的权重矩阵<strong>共享参数</strong></p>
</li>
</ol>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.20.11.png" alt="截屏2024-07-29 10.20.11" style="zoom:25%;" /></li>
</ul>
<h4 id="GNN-vs-Heterogenous-GNN"><a href="#GNN-vs-Heterogenous-GNN" class="headerlink" title="GNN vs Heterogenous GNN"></a>GNN vs Heterogenous GNN</h4><ul>
<li><p>两者的区别？</p>
<ol>
<li><p><strong>Message：</strong>消息传递阶段，H-GNN对关系类型建模，不同的关系类型具有不同的linear权重</p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729111639298.png" alt="image-20240729111639298" style="zoom:25%;" />

 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.16.31.png" alt="截屏2024-07-29 11.16.31" style="zoom: 25%;" />
</li>
<li><p><strong>Aggregation：</strong>聚合阶段，H-GNN同样对关系类型进行建模，使用<strong>两阶段的聚合方法</strong>：先对同种关系类型的消息进行聚合，然后进行总的聚合（例如concat）</p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.18.03.png" alt="截屏2024-07-29 11.18.03" style="zoom:25%;" />

 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.18.22.png" alt="截屏2024-07-29 11.18.22" style="zoom:25%;" />
</li>
<li><p><strong>Prediction：</strong>在预测阶段，需要考虑不同的关系类型</p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729111944075.png" alt="image-20240729111944075" style="zoom:25%;" /></li>
</ol>
</li>
</ul>
<h2 id="Day5"><a href="#Day5" class="headerlink" title="Day5"></a>Day5</h2><h4 id="Knowledge-Graphs"><a href="#Knowledge-Graphs" class="headerlink" title="Knowledge Graphs"></a>Knowledge Graphs</h4><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.22.37.png" alt="截屏2024-07-29 11.22.37"></p>
<h4 id="关系模式"><a href="#关系模式" class="headerlink" title="关系模式"></a>关系模式</h4><ol>
<li><p><strong>对称关系（Symmetric Relations）</strong></p>
<p> <strong>定义</strong>：如果关系 ( r(h, t) ) 成立，那么 ( r(t, h) ) 也成立。</p>
<p> <strong>符号表示</strong>：$$ r(h, t) \Rightarrow r(t, h) \forall h, t $$</p>
<p> <strong>示例</strong>：</p>
<ul>
<li><strong>家人（Family）</strong>：如果Alice是Bob的家人，那么Bob也是Alice的家人。</li>
<li><strong>室友（Roommate）</strong>：如果Alice是Bob的室友，那么Bob也是Alice的室友。</li>
</ul>
</li>
<li><p><strong>反对称关系（Antisymmetric Relations）</strong></p>
<p> <strong>定义</strong>：如果关系 ( r(h, t) ) 成立，那么 ( r(t, h) ) 不成立。</p>
<p> <strong>符号表示</strong>：$$ r(h, t) \Rightarrow \neg r(t, h) \forall h, t $$</p>
<p> <strong>示例</strong>：</p>
<ul>
<li><strong>上位词（Hypernym）</strong>：一个词具有更广泛的意义。例如，“狗（dog）”是“贵宾犬（poodle）”的上位词，但反之不成立。</li>
</ul>
</li>
<li><p><strong>逆关系（Inverse Relations）</strong></p>
<p> <strong>定义</strong>：如果关系 ( r(h, t) ) 成立，那么存在一个逆关系 ( r’(t, h) ) 也成立。</p>
<p> <strong>符号表示</strong>：$$ r(h, t) \Rightarrow r’(t, h) $$</p>
<p> <strong>示例</strong>：</p>
<ul>
<li><strong>导师与学生（Advisor and Advisee）</strong>：如果Dr. Smith是John的导师，那么John是Dr. Smith的学生。</li>
</ul>
</li>
<li><p><strong>传递关系（Composition&#x2F;Transitive Relations）</strong></p>
<p> <strong>定义</strong>：如果 ( r(x, y) ) 和 ( r(y, z) ) 都成立，那么 ( r(x, z) ) 也成立。</p>
<p> <strong>符号表示</strong>：$$ r(x, y) \land r(y, z) \Rightarrow r(x, z) \forall x, y, z $$</p>
<p> <strong>示例</strong>：</p>
<ul>
<li><strong>家庭关系</strong>：我母亲的丈夫是我的父亲。如果Alice是Bob的母亲，Bob是Charlie的丈夫，那么Alice是Charlie的母亲。</li>
</ul>
</li>
<li><p><strong>一对多关系（1-to-N Relations）</strong></p>
<p><strong>定义</strong>：一个实体可以与多个实体通过相同关系相关联。</p>
<p><strong>符号表示</strong>：$$ r(h, t_1) , r(h, t_2), \ldots , r(h, t_n) $$ 都成立。</p>
<p><strong>示例</strong>：</p>
<ul>
<li><strong>学生关系（StudentsOf）</strong>：一个教师有多个学生。例如，教师Dr. Smith有学生John、Emily、Sarah。</li>
</ul>
</li>
</ol>
<h4 id="KG-Completion"><a href="#KG-Completion" class="headerlink" title="KG Completion"></a>KG Completion</h4><ul>
<li><p>KG的补全：例如Is link (h,r,t) in the KG?</p>
</li>
<li><p>Introduce <strong>TransE &#x2F; TransR &#x2F; DistMult &#x2F; ComplEx models</strong>【四种<strong>知识图谱嵌入</strong>的方法】 with different embedding space and expressiveness</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729122411091.png" alt="image-20240729122411091" style="zoom: 25%;" />



<h4 id="KG-Reasoning"><a href="#KG-Reasoning" class="headerlink" title="KG Reasoning"></a>KG Reasoning</h4><ul>
<li><strong>知识图谱具体的推理任务需要根据一系列的query来表述</strong>，也就是说在给定的<strong>不完整并且大规模的知识图谱</strong>中对输入的自然语言形式的query进行推理并返回结果，而query可以分为<strong>One-hop Queries，Path Queries和Conjunctive Queries</strong>三种，分别是<strong>单次跳转的推理、多次跳转的推理、联合推理</strong>，可以用下面的图来表示三种query之间的关系：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.05.59.png" alt="截屏2024-07-29 13.05.59" style="zoom: 33%;" />

<ul>
<li><p><strong>可以使用知识图谱嵌入</strong>：考虑将知识图谱上的问答转化到<strong>向量空间</strong>中进行，而具体的方法就是将query也转换成一个向量，并使用嵌入模型的打分函数来评估结果。一个查询在向量空间中可以表示为：</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.10.50.png" alt="截屏2024-07-29 13.10.50" style="zoom: 33%;" /></li>
</ul>
<h4 id="Query2Box"><a href="#Query2Box" class="headerlink" title="Query2Box"></a>Query2Box</h4><blockquote>
<p>论文🔗：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.05969">QUERY2BOX: REASONING OVER KNOWLEDGE GRAPHS IN VECTOR SPACE USING BOX EMBEDDINGS</a></p>
</blockquote>
<ul>
<li>问题：在处理<strong>联合推理</strong>时每个节点都代表多个entity，如何<strong>在隐空间定义其交叉操作？</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.18.02.png" alt="截屏2024-07-29 13.18.02" style="zoom:25%;" />

<ul>
<li><strong>核心思想：</strong>Embed queries with hyper-rectangles (boxes)【<strong>将查询嵌入为超矩形即box</strong>】</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.22.48.png" alt="截屏2024-07-29 13.22.48" style="zoom:33%;" />

<ul>
<li>如何定义多个Box的<strong>交集【intersection】操作</strong>？</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.28.52.png" alt="截屏2024-07-29 13.28.52" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.28.08.png" alt="截屏2024-07-29 13.28.08" style="zoom:25%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.30.35.png" alt="截屏2024-07-29 13.30.35" style="zoom:25%;" />



<ul>
<li><strong>对于Union操作的扩展</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.40.29.png" alt="截屏2024-07-29 13.40.29" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.43.28.png" alt="截屏2024-07-29 13.43.28" style="zoom:25%;" />

<ul>
<li><p><strong>Query2Box的训练</strong></p>
<ol>
<li>在Query2Box模型中，需要<strong>训练的参数主要有所有的实体的嵌入向量，所有的关系的嵌入向量和Intersection运算中的各种参数。</strong></li>
<li>一个很直观的想法是，在训练Qeury2Box模型的过程中，对于一个查询q的嵌入向量，我们要让属于q中的实体v对应的打分函数最大化，而要让不在其中的打分函数最小化，为此需要用到负采样，也就是在训练的过程中，对于每个正样本v随机选取一个负样本与之对应，<strong>具体的训练过程可以分为以下几个步骤：</strong></li>
</ol>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.52.13.png" alt="截屏2024-07-29 13.52.13" style="zoom: 50%;" />

<ol start="3">
<li>在训练之前我们需要提取出一系列查询，而这个过程称为<strong>Query Generation</strong>，可以通过一系列模板生成：</li>
</ol>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.53.39.png" alt="截屏2024-07-29 13.53.39" style="zoom:25%;" /></li>
</ul>
<h2 id="Day6"><a href="#Day6" class="headerlink" title="Day6"></a>Day6</h2><h4 id="Fast-neural-subgraph-matching-and-Counting"><a href="#Fast-neural-subgraph-matching-and-Counting" class="headerlink" title="Fast neural subgraph matching and Counting"></a>Fast neural subgraph matching and Counting</h4><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.25.png" alt="截屏2024-07-29 13.58.25"></p>
<h4 id="子图"><a href="#子图" class="headerlink" title="子图"></a>子图</h4><ul>
<li><p>图的组成结构，可以区分和描述一个图</p>
</li>
<li><p>两种定义方法：</p>
<ol>
<li><p><strong>Node-induced subgraph：节点诱导子图</strong></p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.44.51.png" alt="截屏2024-07-30 15.44.51" style="zoom: 25%;" />
</li>
<li><p><strong>Edge-induced subgraph：边诱导子图</strong></p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.45.04.png" alt="截屏2024-07-30 15.45.04" style="zoom: 25%;" />
</li>
<li><p>两种方法生成的子图都有其应用范围：例如<strong>化学结构适合节点诱导</strong>，而<strong>知识图谱适合边诱导</strong>（因为知识图谱focus on边表示的逻辑关系）</p>
</li>
</ol>
</li>
</ul>
<h4 id="Graph-Isomorphism-图同构"><a href="#Graph-Isomorphism-图同构" class="headerlink" title="Graph Isomorphism 图同构"></a>Graph Isomorphism 图同构</h4><ul>
<li>定义：core idea <strong>双射 bijection</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.51.25.png" alt="截屏2024-07-30 15.51.25" style="zoom: 25%;" />

<ul>
<li>图同构的判断问题是NP-hard</li>
<li>引申：<strong>子图同构问题：</strong>图G1 和 图G2的子图 同构：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.54.07.png" alt="截屏2024-07-30 15.54.07" style="zoom:25%;" />

<h4 id="Network-motifs-网络图案"><a href="#Network-motifs-网络图案" class="headerlink" title="Network motifs 网络图案"></a>Network motifs 网络图案</h4><ul>
<li><p>定义：<strong>recurring, significant patterns of interconnections <strong>在网络中</strong>反复出现的、具有显著意义的</strong>连接模式。</p>
</li>
<li><p>特点特性：</p>
<ol>
<li><strong>Pattern:</strong> Small (node-induced) Subgraph <strong>较小的节点诱导子图</strong>，节点诱导必须满足所有诱导节点之间的边都一致</li>
<li><strong>Recurring:</strong> Found Many Times, i.e., with High Frequency <strong>重复出现：多次被发现，即具有高频率</strong><ul>
<li><strong>定义</strong>：一个模式在网络中被认为是motif，如果它在网络中多次出现，其出现频率明显高于偶然情况。</li>
<li><strong>如何定义频率</strong>：通过在网络中计算特定模式的出现次数来定义频率。</li>
</ul>
</li>
<li><strong>Significant:</strong> More Frequent Than Expected, i.e., in Randomly Generated Graphs<strong>显著性：出现频率高于预期，即在随机生成的图中</strong></li>
</ol>
</li>
<li><p><strong>如何判断motifs？</strong>使用<strong>相同的图统计指标</strong>生成随机图，利用<strong>统计指标</strong>（例如z分数：对所有的待评估图案作为一个向量，计算z分数后归一化）评估 motif 的显著性。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2011.33.50.png" alt="截屏2024-08-05 11.33.50" style="zoom:33%;" />

<h4 id="Neural-Subgraph-Match"><a href="#Neural-Subgraph-Match" class="headerlink" title="Neural Subgraph Match"></a>Neural Subgraph Match</h4><ul>
<li>如何基于神经网络（GNN）进行子图匹配（which is np-hard）</li>
</ul>
<h4 id="方法：使用Order-Embedding-有序嵌入空间"><a href="#方法：使用Order-Embedding-有序嵌入空间" class="headerlink" title="方法：使用Order Embedding 有序嵌入空间"></a>方法：使用Order Embedding 有序嵌入空间</h4><ul>
<li>核心思想：把所有的图<strong>嵌入为高维向量</strong>，把<strong>子图同构的关系转换为向量的所有维数均小于的关系</strong>。</li>
<li>优点：<ol>
<li>这种嵌入空间可以表示子图同构的关系，<strong>符合传递性、反对称性等要求；</strong></li>
<li>由于空图嵌入为0，并且空图是所有图的子图，所以<strong>所有图嵌入向量应该是全正向量</strong>。</li>
</ol>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2013.13.51.png" alt="截屏2024-08-05 13.13.51" style="zoom:33%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2013.12.57.png" alt="截屏2024-08-05 13.12.57" style="zoom:33%;" />

<ul>
<li><p>如何训练？</p>
<ol>
<li><p>利用GNN对查询图和目标图进行嵌入。</p>
</li>
<li><p>构建训练样本，包括<strong>正样本和负样本：</strong>其中正样本包括一个查询图$G_q$以及一个目标图$G_t$，并且查询图是目标图的同构子图，可以通过BFS等策略进行构建。</p>
</li>
<li><p><strong>最大化边缘损失。</strong>只有当任意维数不符合小于条件时，存在损失。</p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2006.28.40.png" alt="截屏2024-08-06 06.28.40" style="zoom:33%;" /></li>
</ol>
</li>
</ul>
<h4 id="如何找出-Frequent-SubGraphs【即-motifs】"><a href="#如何找出-Frequent-SubGraphs【即-motifs】" class="headerlink" title="如何找出 Frequent SubGraphs【即 motifs】"></a>如何找出 Frequent SubGraphs【即 motifs】</h4><ul>
<li>一般的办法是：枚举一幅图的所有相同size的子图，然后数其中motifs的数量。[computationally hard]</li>
</ul>
<h4 id="SPMiner"><a href="#SPMiner" class="headerlink" title="SPMiner"></a>SPMiner</h4><ul>
<li>SPMiner 使用神经网络模型将图结构分解、编码，并通过模式增长搜索频繁子图。<ol>
<li>将图 ( G ) 分解成节点锚定的重叠邻域。具体来说，即围绕每个节点及其邻居构造一个子图，这些子图可以彼此重叠。</li>
<li>将这些子图嵌入到一个有序的嵌入空间中。</li>
<li><strong>搜索过程</strong>：通过扩展模式来找到频繁子图。</li>
</ol>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2007.40.51.png" alt="截屏2024-08-06 07.40.51" style="zoom: 25%;" />

<ul>
<li><strong>第三步：搜索过程详解</strong><ul>
<li>从初始节点出发，每一步进行扩展。目标是在指定的k步之后，其右上方区域的节点数量最大化。</li>
<li>简单来说，就是找到一种最优的游走方法，使得右上方区域的节点数量最大化（代表该motif是最多出现的子图模式）。</li>
<li><strong>每一步该如何选择：启发式</strong>规则：例如贪心（每次最大化）、动态规划。</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2007.51.22.png" alt="截屏2024-08-06 07.51.22" style="zoom:33%;" />





<h2 id="Day7"><a href="#Day7" class="headerlink" title="Day7"></a>Day7</h2><h3 id="利用GNN进行推荐"><a href="#利用GNN进行推荐" class="headerlink" title="利用GNN进行推荐"></a>利用GNN进行推荐</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.45.png" alt="截屏2024-07-29 13.58.45"></p>
<h4 id="推荐系统中的Graph"><a href="#推荐系统中的Graph" class="headerlink" title="推荐系统中的Graph"></a>推荐系统中的Graph</h4><ul>
<li>推荐系统可以自然得被建模为一个user-item的二部图</li>
<li>从而使得推荐的过程作为一个<strong>Link-prediction</strong>的任务：预测user-item的缺失边从而进行推荐</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2009.10.27.png" alt="截屏2024-08-06 09.10.27" style="zoom: 25%;" />

<ul>
<li>局限性：无法计算每个user和item之间的f分数，计算量过大；解决：使用两阶段方法，即先召回后排序</li>
</ul>
<h4 id="Embedding-based-models"><a href="#Embedding-based-models" class="headerlink" title="Embedding-based models"></a>Embedding-based models</h4><ul>
<li><p>为了实现 u 和 v 之间的 score，将 u 和 v 嵌入相同维数的向量空间</p>
</li>
<li><p><strong>训练目标：使得 recall@K 召回的成功率最大。</strong>（但这是<strong>不可微分</strong>的，无法使用梯度下降进行优化）</p>
<blockquote>
<p>ML-based的优化目标要求可微，可以使用等价的优化目标[surrogate loss functions]，which should align well with the original training objective[这应该与最初的优化目标非常一致。]</p>
</blockquote>
<ol>
<li><p><strong>Binary Loss</strong></p>
<ul>
<li>对于正负样本而言，就是一个二分类问题。</li>
</ul>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2009.40.51.png" alt="截屏2024-08-06 09.40.51" style="zoom:33%;" />

<ul>
<li>局限性：这样考虑所有user的loss之和，没有考虑user之间的个性差异。</li>
</ul>
</li>
<li><p><strong>Bayesian Personalized Ranking (BPR) loss</strong> </p>
<ul>
<li>defined in a <strong>personalized</strong> manner.</li>
<li>对于每一个user分别计算损失，<strong>目标是使得正边分数减去负边分数尽可能大</strong></li>
</ul>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2010.36.13.png" alt="截屏2024-08-06 10.36.13" style="zoom: 33%;" /></li>
</ol>
</li>
<li><p><strong>性能：</strong></p>
<ul>
<li>Embedding-based models have achieved SoTA in recommender systems. [性能很好]</li>
<li>为什么 Embedding-based models work well？<ul>
<li>潜在的思想：<strong>利用协同过滤的思想，对一个user的推荐借助了其它特征相似的user。</strong></li>
<li>embedding <strong>进行嵌入实际上是嵌入为相对低维的向量，这种低维的向量无法对所有的用户&#x2F;商品特征直接记忆或者建模，必须通过捕捉到其相似性&#x2F;相似特征</strong>，这也是vector-embedding的优势和作用所在。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="representative-GNN-approaches-for-recommender-systems"><a href="#representative-GNN-approaches-for-recommender-systems" class="headerlink" title="representative GNN approaches for recommender systems"></a>representative GNN approaches for recommender systems</h4><blockquote>
<p>推荐阅读：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39388410/article/details/106970194">图神经网络用于推荐系统问题（NGCF，LightGCN，UltraGCN）</a></p>
</blockquote>
<ul>
<li><p>传统的协同过滤方式[使用简单的binaryloss作为优化目标进行优化]：通过score function训练vector encoder，这种方法比较shallow浅层，无法捕捉到<strong>高阶的图结构信息</strong>[例如多跳信息]。</p>
</li>
<li><p>两种优化方法：(1）NGCF：使用GNN捕捉u-i二部图的多跳结构信息 （2）</p>
</li>
</ul>
<h4 id="一、Neural-Graph-Collaborative-Filtering-NGCF"><a href="#一、Neural-Graph-Collaborative-Filtering-NGCF" class="headerlink" title="一、Neural Graph Collaborative Filtering (NGCF)"></a>一、Neural Graph Collaborative Filtering (NGCF)</h4><ul>
<li>主要的思想是<strong>通过GNN来学习user&#x2F;item的embedding向量</strong>，从而捕捉到更深层次的图结构信息[High-order graph structure is captured through iterative neighbor aggregation.]。【例如对于GNN可以通过一个item来连接不同的用户，从而捕捉该信息】</li>
<li>框架：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2011.12.09.png" alt="截屏2024-08-06 11.12.09" style="zoom:25%;" />

<ul>
<li>其中的item&#x2F;user embedding以及GNN parameters 都是可以学习的。</li>
</ul>
<h4 id="二、LightGCN"><a href="#二、LightGCN" class="headerlink" title="二、LightGCN"></a>二、LightGCN</h4><blockquote>
<p>论文🔗：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.02126">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</a></p>
<p>一种简化版的图卷积神经网络（GCN），专门用于推荐系统。与传统的GCN相比，LightGCN通过简化模型结构，<strong>去掉非线性激活和特征变换部分</strong>，使得模型更加轻量级且易于训练，同时保持了推荐效果。</p>
</blockquote>
<ul>
<li><strong>动机：</strong>前述的 NGCF 同时迭代学习：（1）item&#x2F;user embedding （2）GNN parameters  <ul>
<li>存在问题：<strong>embedding的参数量  &gt;&gt; GNN的参数量</strong> [原因：embedding参数量达到（#user&#x2F;item * embedding dim），其中#user&#x2F;item非常非常大]，说明embedding已经有足够表达能力，试图消除GNN中的可学习参数。</li>
</ul>
</li>
<li>user-item二部图的<strong>邻接矩阵、Embedding矩阵</strong>：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2011.28.00.png" alt="截屏2024-08-06 11.28.00" style="zoom: 25%;" />

<ul>
<li>LightGCN直接把GCN中的<strong>非线性激活、特征变换部分、自信息部分</strong>删除[作者通过消融实验验证了这个部分是没有帮助的]</li>
<li>embedding的传递仅通过邻居节点embedding的聚合实现【不包含自信息部分】，并且这个聚合使用平均效果就已经很好了。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2014.01.56.png" alt="截屏2024-08-06 14.01.56" style="zoom: 50%;" />

<ul>
<li>最后的embedding通过<strong>每一层计算的embedding的加权进行计算：</strong></li>
</ul>
<img src="/Users/lbyyds/Library/Application Support/typora-user-images/截屏2024-08-06 14.04.23.png" alt="截屏2024-08-06 14.04.23" style="zoom:50%;" />



<h4 id="如何将扩展到大规模的推荐系统中？PinSage"><a href="#如何将扩展到大规模的推荐系统中？PinSage" class="headerlink" title="如何将扩展到大规模的推荐系统中？PinSage"></a>如何将扩展到大规模的推荐系统中？PinSage</h4><blockquote>
<p><strong>Pinterest</strong> 是一款类似于微博的社交软件，需要推荐<strong>超大规模（web-scale）的数据</strong>。<strong>PinSage</strong>成功应用在了<strong>Pinterest的推荐系统</strong></p>
</blockquote>
<ul>
<li>主要的思想类似于 GraphSAGE</li>
<li>主要的改进包括：<ol>
<li><strong>对比学习</strong>的思想：选取两篇博文作为样本，相似的作为正样本。学习目标在于将相似样本的嵌入空间尽可能接近，否则尽可能远离。</li>
<li>不使用全部的邻节点，而是进行采样。并且和GraphSAGE不同的是，进行<strong>重要性采样</strong>，即根据重要性排序后进行top-n采样。</li>
<li>在BRP损失中，需要为每一个user采样负样本，开销较大。这里使用了<strong>Shared negative samples</strong> across users in a mini-batch，即负样本共享的方法，所有用户共享负样本，只采样一次。</li>
<li>由于推荐系统在工业上需要进行细粒度的召回[从百万中取回数十商品]，所以随机采样的negative samples过于简单，导致推荐系统性能比较差。PinSage使用了<strong>Hard negative samples</strong>的方法，迫使模型可以区分困难的负样本。<ul>
<li><strong>如何生成</strong>困难的负样本？从用户节点进行<strong>随机游走打分</strong>，找出<strong>关联度较高但不是最高</strong>的节点。</li>
<li><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2018.41.18.png" alt="截屏2024-08-06 18.41.18" style="zoom: 25%;" /></li>
</ul>
</li>
<li>**渐进式训练(Curriculum training)**：如果训练全程都使用hard负样本，会导致模型收敛速度减半，训练时长加倍，因此PinSage采用了一种Curriculum训练的方式，即第一轮训练只使用简单负样本，帮助模型参数快速收敛到一个loss比较低的范围；后续训练中逐步加入hard负样本，让模型学会将很相似的物品与些微相似的区分开，方式是第n轮训练时给每个物品的负样本集合中增加n-1个hard负样本。</li>
</ol>
</li>
</ul>
<h2 id="Day8"><a href="#Day8" class="headerlink" title="Day8"></a>Day8</h2><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.53.png" alt="截屏2024-07-29 13.58.53"></p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="https://github.com/02lb">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">图表示学习&#x2F;图机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GuideLines"><span class="toc-number">1.0.0.1.</span> <span class="toc-text">GuideLines</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day1"><span class="toc-number">1.1.</span> <span class="toc-text">Day1:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Node-Embedding"><span class="toc-number">1.1.1.</span> <span class="toc-text">Node Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Encoder-Decoder-Framework"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Encoder - Decoder Framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0%EF%BC%9ARandom-Walk%EF%BC%9A"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">随机游走：Random Walk：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%9D%E6%83%B3%EF%BC%88Idea%EF%BC%89"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text">思想（Idea）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%EF%BC%88Expressivity%EF%BC%89"><span class="toc-number">1.1.1.2.2.</span> <span class="toc-text">表达能力（Expressivity）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7-Negative-Sampling%EF%BC%9A"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">负采样 Negative Sampling：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.3.1.</span> <span class="toc-text">负采样的原理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%95%B0%E9%87%8F-K-%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.3.2.</span> <span class="toc-text">负样本数量 (K) 的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.3.3.</span> <span class="toc-text">负样本的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%B4%9F%E9%87%87%E6%A0%B7%E5%85%AC%E5%BC%8F%E5%8F%8A%E5%85%B6%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.3.4.</span> <span class="toc-text">解释负采样公式及其原理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Node-Embedding-%EF%BC%9A-DeepWalk-Node2Vec"><span class="toc-number">1.1.2.</span> <span class="toc-text">Node Embedding ： DeepWalk &#x2F; Node2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Embedding-%EF%BC%9A%E5%9B%BE%E5%B5%8C%E5%85%A5"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">Graph Embedding ：图嵌入</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day1-Lab"><span class="toc-number">1.2.</span> <span class="toc-text">Day1-Lab:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day2"><span class="toc-number">1.3.</span> <span class="toc-text">Day2:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CORE%EF%BC%9AGraph-Neural-Network"><span class="toc-number">1.3.1.</span> <span class="toc-text">CORE：Graph Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-vs-Image"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Graph vs Image</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-Basics"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">GNN Basics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Classic-GNN"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Classic GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day3"><span class="toc-number">1.4.</span> <span class="toc-text">Day3</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9%EF%BC%9AGNN"><span class="toc-number">1.4.0.1.</span> <span class="toc-text">主要内容：GNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Manipulation%E3%80%90%E5%9B%BE%E5%A4%84%E7%90%86%E3%80%91"><span class="toc-number">1.4.0.2.</span> <span class="toc-text">Graph Manipulation【图处理】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#unsupervised-self-supervised"><span class="toc-number">1.4.0.3.</span> <span class="toc-text">unsupervised &#x2F; self- supervised</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">1.4.0.4.</span> <span class="toc-text">评估指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%92%E5%88%86"><span class="toc-number">1.4.0.5.</span> <span class="toc-text">图数据的划分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Power-of-GNN"><span class="toc-number">1.4.0.6.</span> <span class="toc-text">The Power of GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day3-Lab"><span class="toc-number">1.5.</span> <span class="toc-text">Day3-Lab</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84GCN%E7%94%A8%E4%BA%8E%E8%8A%82%E7%82%B9%E9%A2%84%E6%B5%8B"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">实现一个简单的GCN用于节点预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day4"><span class="toc-number">1.6.</span> <span class="toc-text">Day4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Heterogenous-graphs-%E5%BC%82%E8%B4%A8%E5%9B%BE"><span class="toc-number">1.6.0.1.</span> <span class="toc-text">Heterogenous graphs[异质图]</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%88Relational-Graph-Convolutional-Network-R-GCN%EF%BC%89"><span class="toc-number">1.6.0.2.</span> <span class="toc-text">关系图卷积网络（Relational Graph Convolutional Network, R-GCN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-vs-Heterogenous-GNN"><span class="toc-number">1.6.0.3.</span> <span class="toc-text">GNN vs Heterogenous GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day5"><span class="toc-number">1.7.</span> <span class="toc-text">Day5</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Knowledge-Graphs"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">Knowledge Graphs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">关系模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KG-Completion"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">KG Completion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KG-Reasoning"><span class="toc-number">1.7.0.4.</span> <span class="toc-text">KG Reasoning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Query2Box"><span class="toc-number">1.7.0.5.</span> <span class="toc-text">Query2Box</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day6"><span class="toc-number">1.8.</span> <span class="toc-text">Day6</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Fast-neural-subgraph-matching-and-Counting"><span class="toc-number">1.8.0.1.</span> <span class="toc-text">Fast neural subgraph matching and Counting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%90%E5%9B%BE"><span class="toc-number">1.8.0.2.</span> <span class="toc-text">子图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Isomorphism-%E5%9B%BE%E5%90%8C%E6%9E%84"><span class="toc-number">1.8.0.3.</span> <span class="toc-text">Graph Isomorphism 图同构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-motifs-%E7%BD%91%E7%BB%9C%E5%9B%BE%E6%A1%88"><span class="toc-number">1.8.0.4.</span> <span class="toc-text">Network motifs 网络图案</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Neural-Subgraph-Match"><span class="toc-number">1.8.0.5.</span> <span class="toc-text">Neural Subgraph Match</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9A%E4%BD%BF%E7%94%A8Order-Embedding-%E6%9C%89%E5%BA%8F%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4"><span class="toc-number">1.8.0.6.</span> <span class="toc-text">方法：使用Order Embedding 有序嵌入空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%89%BE%E5%87%BA-Frequent-SubGraphs%E3%80%90%E5%8D%B3-motifs%E3%80%91"><span class="toc-number">1.8.0.7.</span> <span class="toc-text">如何找出 Frequent SubGraphs【即 motifs】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SPMiner"><span class="toc-number">1.8.0.8.</span> <span class="toc-text">SPMiner</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day7"><span class="toc-number">1.9.</span> <span class="toc-text">Day7</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8GNN%E8%BF%9B%E8%A1%8C%E6%8E%A8%E8%8D%90"><span class="toc-number">1.9.1.</span> <span class="toc-text">利用GNN进行推荐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84Graph"><span class="toc-number">1.9.1.1.</span> <span class="toc-text">推荐系统中的Graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Embedding-based-models"><span class="toc-number">1.9.1.2.</span> <span class="toc-text">Embedding-based models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#representative-GNN-approaches-for-recommender-systems"><span class="toc-number">1.9.1.3.</span> <span class="toc-text">representative GNN approaches for recommender systems</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81Neural-Graph-Collaborative-Filtering-NGCF"><span class="toc-number">1.9.1.4.</span> <span class="toc-text">一、Neural Graph Collaborative Filtering (NGCF)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81LightGCN"><span class="toc-number">1.9.1.5.</span> <span class="toc-text">二、LightGCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%B0%86%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%EF%BC%9FPinSage"><span class="toc-number">1.9.1.6.</span> <span class="toc-text">如何将扩展到大规模的推荐系统中？PinSage</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day8"><span class="toc-number">1.10.</span> <span class="toc-text">Day8</span></a></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://02lb.github.io/2024/07/21/GraphML-CS224w/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&text=GraphML-CS224w[图机器学习]"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&is_video=false&description=GraphML-CS224w[图机器学习]"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=GraphML-CS224w[图机器学习]&body=Check out this article: https://02lb.github.io/2024/07/21/GraphML-CS224w/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&name=GraphML-CS224w[图机器学习]&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://02lb.github.io/2024/07/21/GraphML-CS224w/&t=GraphML-CS224w[图机器学习]"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2024
    Lee
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/02lb">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
