<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="图表示学习&#x2F;图机器学习CS224W: Machine Learning with Graphs GuideLines Methods for node embeddings: DeepWalk, Node2Vec  Graph Neural Networks: GCN, GraphSAGE, GAT…  Graph Transformers  Knowledge graphs and r">
<meta property="og:type" content="article">
<meta property="og:title" content="GraphML-CS224w[图机器学习]">
<meta property="og:url" content="https://02lb.github.io/2024/07/21/GraphML-CS224w/index.html">
<meta property="og:site_name" content="Bo Li’s Blog">
<meta property="og:description" content="图表示学习&#x2F;图机器学习CS224W: Machine Learning with Graphs GuideLines Methods for node embeddings: DeepWalk, Node2Vec  Graph Neural Networks: GCN, GraphSAGE, GAT…  Graph Transformers  Knowledge graphs and r">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085223291.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084716757.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084751025.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084807550.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-21%2017.02.31.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-21%2017.07.23.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085237389.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2015.41.27.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.11.46.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.18.05.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.20.58.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.31.53.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.38.30.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.42.25.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.43.26.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.44.18.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2017.05.24.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085117822.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2009.22.39.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723104152426.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723104136425.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723105202851.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2011.54.37.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723120112059.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2012.00.31.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723123526056.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.08.33.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-26%2010.50.16.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.02.59.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.05.34.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.18.21.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.20.11.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729111639298.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.16.31.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.18.03.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.18.22.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729111944075.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.22.37.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729122411091.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.05.59.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.10.50.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.18.02.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.22.48.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.28.52.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.28.08.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.30.35.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.40.29.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.43.28.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.52.13.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.53.39.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.25.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.44.51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.45.04.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.51.25.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.54.07.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2011.33.50.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2013.13.51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2013.12.57.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2006.28.40.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2007.40.51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2007.51.22.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.45.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2009.10.27.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2009.40.51.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2010.36.13.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2011.12.09.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2011.28.00.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2014.01.56.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2014.04.23.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2018.41.18.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.53.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.01.10.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.04.19.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.11.01.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.26.05.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.30.13.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.12.05.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.15.08.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.17.04.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.50.05.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.54.46.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.55.58.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2008.51.38.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2012.16.22.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2012.19.11.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2012.21.14.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2012.22.13.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2014.12.23.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2014.38.38.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2016.18.49.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2016.25.03.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2016.33.54.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2017.38.05.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2018.07.48.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2014.31.36.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2015.14.54.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2015.15.39.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2015.48.57.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2016.14.59.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2016.25.47.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2016.33.05.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2016.39.45.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2017.06.09.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2018.11.00.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2018.15.23.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2018.16.30.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2018.20.19.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2019.39.15.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2022.49.47.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-13%2011.15.08.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-13%2011.25.16.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-14%2010.43.12.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-14%2012.29.23.png">
<meta property="og:image" content="https://02lb.github.io/Users/lbyyds/Library/Application%20Support/typora-user-images/%E6%88%AA%E5%B1%8F2024-08-14%2012.38.34.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-14%2012.41.40.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-14%2012.46.47.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2013.39.02.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2013.53.06.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.31.38.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.35.45.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.39.47.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.43.29.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.44.45.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.52.20.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.59.20.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2015.00.16.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2015.15.52.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2015.50.54.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2015.52.33.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2016.09.18.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2016.17.07.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2016.18.59.png">
<meta property="article:published_time" content="2024-07-21T09:26:48.000Z">
<meta property="article:modified_time" content="2024-08-20T08:26:07.950Z">
<meta property="article:author" content="Lee">
<meta property="article:tag" content="图机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085223291.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.png">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>GraphML-CS224w[图机器学习]</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 7.1.1"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/02lb">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/07/22/RecSys/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/04/12/docker/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://02lb.github.io/2024/07/21/GraphML-CS224w/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&text=GraphML-CS224w[图机器学习]"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&is_video=false&description=GraphML-CS224w[图机器学习]"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=GraphML-CS224w[图机器学习]&body=Check out this article: https://02lb.github.io/2024/07/21/GraphML-CS224w/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&name=GraphML-CS224w[图机器学习]&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://02lb.github.io/2024/07/21/GraphML-CS224w/&t=GraphML-CS224w[图机器学习]"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">图表示学习&#x2F;图机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GuideLines"><span class="toc-number">1.0.0.1.</span> <span class="toc-text">GuideLines</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day1"><span class="toc-number">1.1.</span> <span class="toc-text">Day1:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Node-Embedding"><span class="toc-number">1.1.1.</span> <span class="toc-text">Node Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Encoder-Decoder-Framework"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Encoder - Decoder Framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0%EF%BC%9ARandom-Walk%EF%BC%9A"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">随机游走：Random Walk：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%9D%E6%83%B3%EF%BC%88Idea%EF%BC%89"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text">思想（Idea）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%EF%BC%88Expressivity%EF%BC%89"><span class="toc-number">1.1.1.2.2.</span> <span class="toc-text">表达能力（Expressivity）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7-Negative-Sampling%EF%BC%9A"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">负采样 Negative Sampling：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.3.1.</span> <span class="toc-text">负采样的原理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%95%B0%E9%87%8F-K-%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.3.2.</span> <span class="toc-text">负样本数量 (K) 的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.3.3.</span> <span class="toc-text">负样本的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%B4%9F%E9%87%87%E6%A0%B7%E5%85%AC%E5%BC%8F%E5%8F%8A%E5%85%B6%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.3.4.</span> <span class="toc-text">解释负采样公式及其原理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Node-Embedding-%EF%BC%9A-DeepWalk-Node2Vec"><span class="toc-number">1.1.2.</span> <span class="toc-text">Node Embedding ： DeepWalk &#x2F; Node2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Embedding-%EF%BC%9A%E5%9B%BE%E5%B5%8C%E5%85%A5"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">Graph Embedding ：图嵌入</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day1-Lab"><span class="toc-number">1.2.</span> <span class="toc-text">Day1-Lab:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day2"><span class="toc-number">1.3.</span> <span class="toc-text">Day2:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CORE%EF%BC%9AGraph-Neural-Network"><span class="toc-number">1.3.1.</span> <span class="toc-text">CORE：Graph Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-vs-Image"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Graph vs Image</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-Basics"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">GNN Basics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Classic-GNN"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Classic GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day3"><span class="toc-number">1.4.</span> <span class="toc-text">Day3</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9%EF%BC%9AGNN"><span class="toc-number">1.4.0.1.</span> <span class="toc-text">主要内容：GNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Manipulation%E3%80%90%E5%9B%BE%E5%A4%84%E7%90%86%E3%80%91"><span class="toc-number">1.4.0.2.</span> <span class="toc-text">Graph Manipulation【图处理】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#unsupervised-self-supervised"><span class="toc-number">1.4.0.3.</span> <span class="toc-text">unsupervised &#x2F; self- supervised</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">1.4.0.4.</span> <span class="toc-text">评估指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%92%E5%88%86"><span class="toc-number">1.4.0.5.</span> <span class="toc-text">图数据的划分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Power-of-GNN"><span class="toc-number">1.4.0.6.</span> <span class="toc-text">The Power of GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day3-Lab"><span class="toc-number">1.5.</span> <span class="toc-text">Day3-Lab</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84GCN%E7%94%A8%E4%BA%8E%E8%8A%82%E7%82%B9%E9%A2%84%E6%B5%8B"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">实现一个简单的GCN用于节点预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day4"><span class="toc-number">1.6.</span> <span class="toc-text">Day4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Heterogenous-graphs-%E5%BC%82%E8%B4%A8%E5%9B%BE"><span class="toc-number">1.6.0.1.</span> <span class="toc-text">Heterogenous graphs[异质图]</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%88Relational-Graph-Convolutional-Network-R-GCN%EF%BC%89"><span class="toc-number">1.6.0.2.</span> <span class="toc-text">关系图卷积网络（Relational Graph Convolutional Network, R-GCN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-vs-Heterogenous-GNN"><span class="toc-number">1.6.0.3.</span> <span class="toc-text">GNN vs Heterogenous GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day5"><span class="toc-number">1.7.</span> <span class="toc-text">Day5</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Knowledge-Graphs"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">Knowledge Graphs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">关系模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KG-Completion"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">KG Completion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KG-Reasoning"><span class="toc-number">1.7.0.4.</span> <span class="toc-text">KG Reasoning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Query2Box"><span class="toc-number">1.7.0.5.</span> <span class="toc-text">Query2Box</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day6"><span class="toc-number">1.8.</span> <span class="toc-text">Day6</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Fast-neural-subgraph-matching-and-Counting"><span class="toc-number">1.8.0.1.</span> <span class="toc-text">Fast neural subgraph matching and Counting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%90%E5%9B%BE"><span class="toc-number">1.8.0.2.</span> <span class="toc-text">子图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Isomorphism-%E5%9B%BE%E5%90%8C%E6%9E%84"><span class="toc-number">1.8.0.3.</span> <span class="toc-text">Graph Isomorphism 图同构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-motifs-%E7%BD%91%E7%BB%9C%E5%9B%BE%E6%A1%88"><span class="toc-number">1.8.0.4.</span> <span class="toc-text">Network motifs 网络图案</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Neural-Subgraph-Match"><span class="toc-number">1.8.0.5.</span> <span class="toc-text">Neural Subgraph Match</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9A%E4%BD%BF%E7%94%A8Order-Embedding-%E6%9C%89%E5%BA%8F%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4"><span class="toc-number">1.8.0.6.</span> <span class="toc-text">方法：使用Order Embedding 有序嵌入空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%89%BE%E5%87%BA-Frequent-SubGraphs%E3%80%90%E5%8D%B3-motifs%E3%80%91"><span class="toc-number">1.8.0.7.</span> <span class="toc-text">如何找出 Frequent SubGraphs【即 motifs】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SPMiner"><span class="toc-number">1.8.0.8.</span> <span class="toc-text">SPMiner</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day7"><span class="toc-number">1.9.</span> <span class="toc-text">Day7</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8GNN%E8%BF%9B%E8%A1%8C%E6%8E%A8%E8%8D%90"><span class="toc-number">1.9.1.</span> <span class="toc-text">利用GNN进行推荐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84Graph"><span class="toc-number">1.9.1.1.</span> <span class="toc-text">推荐系统中的Graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Embedding-based-models"><span class="toc-number">1.9.1.2.</span> <span class="toc-text">Embedding-based models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#representative-GNN-approaches-for-recommender-systems"><span class="toc-number">1.9.1.3.</span> <span class="toc-text">representative GNN approaches for recommender systems</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81Neural-Graph-Collaborative-Filtering-NGCF"><span class="toc-number">1.9.1.4.</span> <span class="toc-text">一、Neural Graph Collaborative Filtering (NGCF)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81LightGCN"><span class="toc-number">1.9.1.5.</span> <span class="toc-text">二、LightGCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%B0%86%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%EF%BC%9FPinSage"><span class="toc-number">1.9.1.6.</span> <span class="toc-text">如何将扩展到大规模的推荐系统中？PinSage</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day8"><span class="toc-number">1.10.</span> <span class="toc-text">Day8</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-generative-models-for-graphs"><span class="toc-number">1.10.1.</span> <span class="toc-text">Deep generative models for graphs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E7%94%9F%E6%88%90%E9%97%AE%E9%A2%98"><span class="toc-number">1.10.1.1.</span> <span class="toc-text">图生成问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E5%9B%BE%E7%9A%84%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B%E5%BB%BA%E6%A8%A1%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%BA%8F%E5%88%97%E7%9A%84%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B"><span class="toc-number">1.10.1.2.</span> <span class="toc-text">将图的生成过程建模为一个序列的生成过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8RNN%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%EF%BC%9AGraphRNN"><span class="toc-number">1.10.1.3.</span> <span class="toc-text">使用RNN进行序列生成：GraphRNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Convolutional-Policy-Network-GCPN"><span class="toc-number">1.10.1.4.</span> <span class="toc-text">Graph Convolutional Policy Network (GCPN)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day8-1"><span class="toc-number">1.11.</span> <span class="toc-text">Day8</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Advanced-topics-in-GNNs"><span class="toc-number">1.11.1.</span> <span class="toc-text">Advanced topics in GNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-%E5%AD%98%E5%9C%A8%E7%9A%84%E4%B8%8D%E8%B6%B3%E4%B9%8B%E5%A4%84"><span class="toc-number">1.11.1.1.</span> <span class="toc-text">GNNs 存在的不足之处</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Position-aware-GNNs"><span class="toc-number">1.11.1.2.</span> <span class="toc-text">Position-aware GNNs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GPT-for-This%EF%BC%9A"><span class="toc-number">1.11.1.3.</span> <span class="toc-text">GPT for This：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.11.1.3.1.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E7%89%B9%E6%80%A7"><span class="toc-number">1.11.1.3.2.</span> <span class="toc-text">关键特性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%94%9A%E7%82%B9"><span class="toc-number">1.11.1.3.3.</span> <span class="toc-text">锚点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#P-GNN-%E8%BF%87%E7%A8%8B"><span class="toc-number">1.11.1.3.4.</span> <span class="toc-text">P-GNN 过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Identity-aware-GNNs"><span class="toc-number">1.11.1.4.</span> <span class="toc-text">Identity-aware GNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8-ID-GNN-%E4%B8%AD%E8%BF%90%E7%94%A8%E8%8A%82%E7%82%B9%E7%9D%80%E8%89%B2%E7%9A%84%E6%80%9D%E6%83%B3%EF%BC%9A%E5%BC%82%E6%9E%84%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E3%80%90Heterogenous-message-passing%E3%80%91"><span class="toc-number">1.11.1.4.1.</span> <span class="toc-text">如何在 ID-GNN 中运用节点着色的思想：异构消息传递【Heterogenous message passing】</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#ID-GNN-Fast"><span class="toc-number">1.11.1.4.2.</span> <span class="toc-text">ID-GNN-Fast</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.11.1.4.3.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7%E7%A0%94%E7%A9%B6"><span class="toc-number">1.11.1.5.</span> <span class="toc-text">GNNs 的鲁棒性研究</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day9"><span class="toc-number">1.12.</span> <span class="toc-text">Day9</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Graph-Transformers"><span class="toc-number">1.12.1.</span> <span class="toc-text">Graph Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.12.1.1.</span> <span class="toc-text">自注意力机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-v-s-Transformer"><span class="toc-number">1.12.1.2.</span> <span class="toc-text">GNN v.s. Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.12.1.3.</span> <span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Transformers%E5%A4%84%E7%90%86Graph"><span class="toc-number">1.12.1.4.</span> <span class="toc-text">使用Transformers处理Graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SignNet"><span class="toc-number">1.12.1.5.</span> <span class="toc-text">SignNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B%EF%BC%88%E4%BD%BF%E7%94%A8SignNet%EF%BC%89"><span class="toc-number">1.12.1.6.</span> <span class="toc-text">流程（使用SignNet）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day10"><span class="toc-number">1.13.</span> <span class="toc-text">Day10</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scaling-to-large-graphs"><span class="toc-number">1.13.1.</span> <span class="toc-text">Scaling to large graphs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E5%9B%BE%E5%85%A8%E6%89%B9%E6%AC%A1%EF%BC%88full-batch%EF%BC%89%E8%AE%AD%E7%BB%83"><span class="toc-number">1.13.1.1.</span> <span class="toc-text">整图全批次（full-batch）训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GPU%E5%86%85%E5%AD%98%E4%B8%8D%E8%B6%B3%EF%BC%9FSubGraph-Sampling%E5%AD%90%E5%9B%BE%E9%87%87%E6%A0%B7"><span class="toc-number">1.13.1.2.</span> <span class="toc-text">GPU内存不足？SubGraph Sampling子图采样</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Cluster-GCN"><span class="toc-number">1.13.1.3.</span> <span class="toc-text">Cluster-GCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E7%AE%80%E5%8C%96-GNNs-%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%9D%A5%E5%A2%9E%E5%BC%BA%E5%85%B6%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-number">1.13.1.4.</span> <span class="toc-text">通过简化 GNNs 的模型架构来增强其可扩展性</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#LightGCN"><span class="toc-number">1.13.1.4.1.</span> <span class="toc-text">LightGCN</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day11-In-Context-Learning-Over-Graph"><span class="toc-number">1.14.</span> <span class="toc-text">Day11 In-Context Learning Over Graph</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0%EF%BC%88ICL%EF%BC%89"><span class="toc-number">1.14.0.1.</span> <span class="toc-text">上下文学习（ICL）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%B8%8B%E5%9C%A8Graph%E4%B8%8A%E8%BF%9B%E8%A1%8CICL%EF%BC%9F"><span class="toc-number">1.14.0.2.</span> <span class="toc-text">如下在Graph上进行ICL？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day12-Algorithmic-reasoning-with-GNNs"><span class="toc-number">1.15.</span> <span class="toc-text">Day12 Algorithmic reasoning with GNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%8E%A8%E7%90%86%E5%92%8CGNNs"><span class="toc-number">1.15.0.1.</span> <span class="toc-text">算法推理和GNNs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-%E5%92%8C-1-WL-%E5%90%8C%E6%9E%84%E6%B5%8B%E8%AF%95"><span class="toc-number">1.15.0.2.</span> <span class="toc-text">GNNs 和 1-WL 同构测试</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.15.0.2.1.</span> <span class="toc-text">具体步骤</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%BA%E7%AE%97%E6%B3%95"><span class="toc-number">1.15.0.3.</span> <span class="toc-text">神经网络作为算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E9%97%AE%E9%A2%98%EF%BC%9AMLP%E5%8F%AF%E4%BB%A5%E8%BD%BB%E6%9D%BE%E6%A8%A1%E6%8B%9F"><span class="toc-number">1.15.0.3.1.</span> <span class="toc-text">特征抽取问题：MLP可以轻松模拟</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%A6%82%E8%A6%81%E7%BB%9F%E8%AE%A1%E9%97%AE%E9%A2%98%EF%BC%9AMLP%E6%97%A0%E6%B3%95%E8%BD%BB%E6%9D%BE%E6%A8%A1%E6%8B%9F"><span class="toc-number">1.15.0.3.2.</span> <span class="toc-text">对于概要统计问题：MLP无法轻松模拟</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%B5%8C%E5%A5%97MLP"><span class="toc-number">1.15.0.3.3.</span> <span class="toc-text">转换：使用嵌套MLP</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Relational-Argmax%E9%97%AE%E9%A2%98"><span class="toc-number">1.15.0.3.4.</span> <span class="toc-text">Relational Argmax问题</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8GNNs%EF%BC%9A"><span class="toc-number">1.15.0.3.5.</span> <span class="toc-text">使用GNNs：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">1.15.0.3.6.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E7%82%B9%EF%BC%9AAlgorithmic-Alignment"><span class="toc-number">1.15.0.4.</span> <span class="toc-text">重点：Algorithmic Alignment</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day13"><span class="toc-number">1.16.</span> <span class="toc-text">Day13</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-Design-space"><span class="toc-number">1.16.0.1.</span> <span class="toc-text">GNNs Design space</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-Task-Space"><span class="toc-number">1.16.0.2.</span> <span class="toc-text">GNNs Task Space</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%94%9A%E7%82%B9%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.16.0.2.1.</span> <span class="toc-text">锚点模型</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0GNNs%E7%9A%84Design%EF%BC%9F"><span class="toc-number">1.16.0.3.</span> <span class="toc-text">如何评估GNNs的Design？</span></a></li></ol></li></ol></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        GraphML-CS224w[图机器学习]
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Lee</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-07-21T09:26:48.000Z" class="dt-published" itemprop="datePublished">2024-07-21</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">图机器学习</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="图表示学习-图机器学习"><a href="#图表示学习-图机器学习" class="headerlink" title="图表示学习&#x2F;图机器学习"></a>图表示学习&#x2F;图机器学习</h1><p><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs224w/">CS224W: Machine Learning with Graphs</a></p>
<h4 id="GuideLines"><a href="#GuideLines" class="headerlink" title="GuideLines"></a>GuideLines</h4><ul>
<li>Methods for node embeddings: DeepWalk, Node2Vec </li>
<li>Graph Neural Networks: GCN, GraphSAGE, GAT… </li>
<li>Graph Transformers </li>
<li>Knowledge graphs and reasoning: TransE, BetaE </li>
<li>Generative models for graphs: GraphRNN </li>
<li>Graphs in 3D: Molecules § Scaling up to large graphs </li>
<li>Applications to Biomedicine, Science, Technology</li>
</ul>
<h2 id="Day1"><a href="#Day1" class="headerlink" title="Day1:"></a>Day1:</h2><h3 id="Node-Embedding"><a href="#Node-Embedding" class="headerlink" title="Node Embedding"></a>Node Embedding</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085223291.png" alt="image-20240723085223291"></p>
<blockquote>
<p>Core idea: <strong>Embed nodes so that distances in embedding space reflect node similarities in the original network.</strong></p>
</blockquote>
<p>对节点进行自动的特征提取-自动嵌入&#x2F;编码：被下游任务利用</p>
<h4 id="Encoder-Decoder-Framework"><a href="#Encoder-Decoder-Framework" class="headerlink" title="Encoder - Decoder Framework"></a><strong>Encoder - Decoder Framework</strong></h4><ul>
<li>encoder 就是node embedding</li>
<li>decoder 就是对节点vector 做相似度提取：例如内积相似度</li>
</ul>
<h4 id="随机游走：Random-Walk："><a href="#随机游走：Random-Walk：" class="headerlink" title="随机游走：Random Walk："></a><strong>随机游走：Random Walk：</strong></h4><ul>
<li><h5 id="思想（Idea）"><a href="#思想（Idea）" class="headerlink" title="思想（Idea）"></a>思想（Idea）</h5><p>  核心思想是：如果从节点 u 开始的随机游走高概率访问节点 v，那么 u 和 v 是相似的。具体解释如下：</p>
<ol>
<li><strong>访问概率高</strong>：从节点 u 开始的随机游走如果高概率访问节点 v，这意味着 u 和 v 之间的路径很多，或者有很多共同的邻居。这说明 u 和 v 在图中的结构位置很相似，具有相似的网络结构特征。</li>
<li><strong>多跳信息</strong>：随机游走不仅考虑直接的邻居关系，还会通过多跳路径访问更远的节点。这种方式可以捕捉到网络中更复杂的结构信息和节点之间的关系。例如，两个节点即使不直接相连，但如果它们通过多个中间节点有较高的访问概率，那么它们之间仍然具有潜在的相似性。</li>
</ol>
</li>
<li><h5 id="表达能力（Expressivity）"><a href="#表达能力（Expressivity）" class="headerlink" title="表达能力（Expressivity）"></a>表达能力（Expressivity）</h5><p>  随机游走路径通过灵活的随机定义方式捕捉节点之间的相似度，能够同时包含局部和高阶的邻域信息：</p>
<ol>
<li><strong>局部信息</strong>：随机游走会优先访问与起始节点直接相连的节点，这样能够捕捉节点的局部结构信息。如果两个节点在局部结构上相似（例如，它们有很多共同邻居），那么随机游走路径会较高概率访问到这些共同邻居。</li>
<li><strong>高阶信息</strong>：随着随机游走的步数增加，它会逐渐扩展到更多的邻居节点，甚至是更远的节点。这样可以<strong>捕捉到多跳（multi-hop）的关系信息，即高阶邻域信息。</strong>如果两个节点在网络中的更大范围内具有相似的连接模式，随机游走路径也能反映这一点。</li>
</ol>
</li>
<li><p>随机游走最大化目标节点embedding的对数似然（使得目标节点周围节点的似然概率最大化，原理是周围节点在嵌入向量空间时距离也相对近），这个计算过程可以使用负采样进行优化。</p>
</li>
</ul>
<h4 id="负采样-Negative-Sampling："><a href="#负采样-Negative-Sampling：" class="headerlink" title="负采样 Negative Sampling："></a>负采样 Negative Sampling：</h4><ul>
<li><h5 id="负采样的原理"><a href="#负采样的原理" class="headerlink" title="负采样的原理"></a>负采样的原理</h5><p>  <strong>负采样的核心思想是用一部分负样本来近似整个负样本空间，从而减少计算开销。</strong>具体步骤如下：</p>
<ol>
<li><strong>选择正样本</strong>：即实际存在的节点对（例如，图中的实际边）。</li>
<li><strong>选择负样本</strong>：随机选择一些节点对（图中不存在的边），这些对作为负样本。</li>
</ol>
</li>
<li><h5 id="负样本数量-K-的选择"><a href="#负样本数量-K-的选择" class="headerlink" title="负样本数量 (K) 的选择"></a>负样本数量 (K) 的选择</h5><ul>
<li><strong>更高的 (K) 值</strong>：提供更鲁棒的估计，因为更多的负样本能够更好地近似整个负样本空间。但是这也会增加计算开销。</li>
<li><strong>更低的 (K) 值</strong>：减少计算开销，但可能会增加对负事件的偏差。</li>
</ul>
<p>  在实际应用中， (K) 通常选择在5到20之间，以平衡计算效率和估计精度。</p>
</li>
<li><h5 id="负样本的选择"><a href="#负样本的选择" class="headerlink" title="负样本的选择"></a>负样本的选择</h5><p>  负样本可以是任何节点，不一定要与随机游走无关。为了提高效率，常常会从所有节点中随机选择负样本，而不是仅从未在随机游走中出现的节点中选择。</p>
</li>
<li><h5 id="解释负采样公式及其原理"><a href="#解释负采样公式及其原理" class="headerlink" title="解释负采样公式及其原理"></a>解释负采样公式及其原理</h5><p>  负采样是一种用于<strong>提高训练效率和计算效率的方法</strong>，尤其是在处理大规模数据集时。下面我们详细解释给定的公式和相关概念。</p>
<p>  给定的公式：<br>  $$<br>  \log \left( \frac{\exp(\mathbf{z}_v^\top \mathbf{z}<em>u)}{\sum</em>{n \in N} \exp(\mathbf{z}_n^\top \mathbf{z}_u)} \right) \approx \log \sigma(\mathbf{z}_v^\top \mathbf{z}<em>u) + \sum</em>{k&#x3D;1}^K \log \sigma(-\mathbf{z}_n^\top \mathbf{z}_u)<br>  $$</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084716757.png" alt="image-20240723084716757" style="zoom: 50%;" />
  
<p>  其中：</p>
<ul>
<li>$\mathbf{z}_v$ 和 $\mathbf{z}_u$是节点$v$ 和 $u$ 的嵌入向量。</li>
<li>$\sigma(x)$是sigmoid函数，定义为 $\sigma(x) &#x3D; \frac{1}{1 + \exp(-x)}$。</li>
<li>$N$ 是所有节点的集合。</li>
<li>$K$ 是负样本的数量。</li>
<li>$\mathbf{z}_n$ 是负样本节点的嵌入向量。</li>
</ul>
<p>  公式表示：</p>
<ul>
<li><p>左边的<br>  $$<br>  \log \left( \frac{\exp(\mathbf{z}_v^\top \mathbf{z}<em>u)}{\sum</em>{n \in N} \exp(\mathbf{z}_n^\top \mathbf{z}_u)} \right)<br>  $$</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084751025.png" alt="image-20240723084751025" style="zoom:50%;" />

<p>  是softmax的对数。</p>
</li>
<li><p>右边的<br>  $$<br>  \log \sigma(\mathbf{z}_v^\top \mathbf{z}<em>u) + \sum</em>{k&#x3D;1}^K \log \sigma(-\mathbf{z}_n^\top \mathbf{z}_u)<br>  $$</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723084807550.png" alt="image-20240723084807550" style="zoom:50%;" />
  
<p>  是<strong>负采样的对数近似。</strong></p>
</li>
</ul>
<p>  <strong>&#x3D;&#x3D;通过负采样，我们可以将计算从整个节点集合的softmax归约到少量负样本的log-sigmoid函数计算，从而大大减少计算复杂度。&#x3D;&#x3D;</strong></p>
</li>
</ul>
<h3 id="Node-Embedding-：-DeepWalk-Node2Vec"><a href="#Node-Embedding-：-DeepWalk-Node2Vec" class="headerlink" title="Node Embedding ： DeepWalk &#x2F; Node2Vec"></a>Node Embedding ： DeepWalk &#x2F; Node2Vec</h3><blockquote>
<p><strong>重点内容</strong></p>
</blockquote>
<ul>
<li><p>DeepWalk：RandomWalk + 词嵌入模型（如Word2Vec）</p>
</li>
<li><p>Node2Vec：DeepWalk + 利用了参数p、q进行BFS&#x2F;DFS的随机游走，提供了biased的路径选择。</p>
<blockquote>
<p>引入了更灵活的随机游走策略，通过调整参数 p 和 q 来控制游走的行为，从而在深度优先搜索（DFS）和广度优先搜索（BFS）之间进行平衡。</p>
</blockquote>
</li>
<li><p>利用随机游走得到的node-seq视为word2vec的词序列进行embedding</p>
</li>
<li><p><strong>Limitations：</strong></p>
<ul>
<li><p><strong>模型不能推广到训练和测试集中未见过的新节点或新结构：</strong>若new node到来，需要重新计算整个graph的node embedding，而不是增量可扩展的。</p>
</li>
<li><p><strong>无法捕捉结构相似性</strong>：它们只是用过节点的相邻性来判断空间相似性（距离），而不考虑任何结构信息</p>
<blockquote>
<p>主要捕捉的是<strong>节点的同质性（homophily）特征</strong>，而不是<strong>结构相似性（structural similarity）</strong>。这意味着它们倾向于将相邻或近邻节点（即在图中距离较近的节点）映射到相似的嵌入空间中，而不是将具有相似结构但在图中距离较远的节点映射到相似的嵌入空间中。</p>
<p>节点同质性假设是指在图中距离较近的节点往往具有相似的特征或属性。例如，在社交网络中，朋友之间的兴趣爱好往往相似。</p>
<p>结构相似性是指两个节点在图中的角色或位置相似，即使它们在图中距离较远。例如，在公司组织图中，两个不同部门的经理可能具有类似的结构角色，即使他们不直接连接。</p>
</blockquote>
</li>
<li><p>Solution to these limitations: <strong>Deep Representation Learning and Graph Neural Networks</strong></p>
</li>
</ul>
</li>
</ul>
<h4 id="Graph-Embedding-：图嵌入"><a href="#Graph-Embedding-：图嵌入" class="headerlink" title="Graph Embedding ：图嵌入"></a>Graph Embedding ：图嵌入</h4><ul>
<li><p>Naive：将所有node的嵌入向量进行结合操作（累加&#x2F;avg&#x2F;……)</p>
</li>
<li><p>Approach2: 添加一个 上帝虚节点 </p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-21%2017.02.31.png" style="zoom: 25%;" />
</li>
<li><p>Aprroach3 ：分层&#x2F;pool：<strong>DiffPool（Differentiable Pooling）</strong></p>
<blockquote>
<p>是一种图神经网络（GNN）池化方法，用于在图上进行层次化聚类，并根据这些聚类对节点嵌入进行求和或平均。</p>
</blockquote>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-21%2017.07.23.png" style="zoom: 33%;" />



<h2 id="Day1-Lab"><a href="#Day1-Lab" class="headerlink" title="Day1-Lab:"></a>Day1-Lab:</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1vvIoEqxGl1naopTZbh4bmCOLEiCxvcQq">Link</a></p>
</blockquote>
<p>熟悉 NetworkX 以及 PyG 的用法；简单的 NodeEmbedding方法</p>
<h2 id="Day2"><a href="#Day2" class="headerlink" title="Day2:"></a>Day2:</h2><h3 id="CORE：Graph-Neural-Network"><a href="#CORE：Graph-Neural-Network" class="headerlink" title="CORE：Graph Neural Network"></a>CORE：Graph Neural Network</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085237389.png" alt="image-20240723085237389"></p>
<blockquote>
<p>之前的Node Embedding方法（简单的 encoder - decoder）没有考虑节点结构的信息，只是使用节点距离进行空间嵌入；这里使用GNN深度学习的方法学习节点的嵌入；</p>
</blockquote>
<h4 id="Graph-vs-Image"><a href="#Graph-vs-Image" class="headerlink" title="Graph vs Image"></a>Graph vs Image</h4><ul>
<li><p>There is no fixed notion of locality or sliding window on the graph (图上<strong>没有固定的局部性或滑动窗口概念</strong>)</p>
</li>
<li><p>辨析：<strong>排列不变性（permutation invariant）</strong>，不存在一种对节点的权威的排序，任何排序应该有相同的结果；<strong>排列等变性（permutation equivariant）</strong>；</p>
<blockquote>
<p>排列不变性意味着，无论图的节点顺序如何，模型的输出结果应该保持不变。假设我们有一个图 $G$，其邻接矩阵为 $A$，节点特征矩阵为 $X$，则排列不变性可以表示为：</p>
<p>$ f(A, X) &#x3D; f(PAP^T, PX) $</p>
<p>其中，$P$ 是一个任意的排列矩阵，它可以重新排列图的节点。排列不变性的一个例子是将图映射到一个固定长度的向量。无论如何重新排列输入图的节点，输出向量始终保持不变。</p>
</blockquote>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2015.41.27.png" alt="截屏2024-07-22 15.41.27" style="zoom: 25%;" />



<h4 id="GNN-Basics"><a href="#GNN-Basics" class="headerlink" title="GNN Basics"></a>GNN Basics</h4><ul>
<li><p>Param sharing：参数共享</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.11.46.png" alt="截屏2024-07-22 16.11.46" style="zoom: 25%;" />


</li>
<li><p>GNN beyond CNN&#x2F;Transformers</p>
<ul>
<li><strong>CNNs can be seen as a special GNN with fixed neighbor</strong></li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.18.05.png" alt="截屏2024-07-22 16.18.05" style="zoom:25%;" />

<ul>
<li><strong>Transformer layer can be seen as a special GNN that runs on a fullyconnected “word” graph!</strong></li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.20.58.png" alt="截屏2024-07-22 16.20.58" style="zoom:25%;" /></li>
</ul>
<h4 id="Classic-GNN"><a href="#Classic-GNN" class="headerlink" title="Classic GNN"></a>Classic GNN</h4><ul>
<li><p>一个通用的GNN架构：</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.31.53.png" alt="截屏2024-07-22 16.31.53" style="zoom:25%;" />
</li>
<li><p><strong>GNN Layer &#x3D;（1） Message + （2）Aggregation</strong></p>
<blockquote>
<p>不同的策略 -&gt; 不同的实例：GCN, GraphSAGE, GAT…..</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.38.30.png" alt="截屏2024-07-22 16.38.30" style="zoom: 25%;" />
</li>
<li><p><strong>Graph Convolutional Networks (GCN)：</strong></p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.42.25.png" alt="截屏2024-07-22 16.42.25" style="zoom:25%;" />
</li>
<li><p><strong>GraphSAGE：</strong><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.02216">🔗</a></p>
<ul>
<li><p><strong>Transductive to Inductive</strong></p>
</li>
<li><p>GCN直接对整个图进行卷积操作，每一层都需要遍历所有节点及其邻居节点。这种全图卷积的方式在处理大规模图时会遇到内存和计算瓶颈。</p>
</li>
<li><p>GraphSAGE提出了一种基于<strong>采样与聚合</strong>的策略，通过在训练过程中<strong>对节点的邻居进行采样，只选择部分邻居来计算节点的表示</strong>，从而大大减小了计算量和内存需求。主要解决了GCN在大规模图上应用的可扩展性问题。</p>
</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.43.26.png" alt="截屏2024-07-22 16.43.26" style="zoom:25%;" />

<ul>
<li><p><strong>Graph Attention Networks（GAT）：</strong></p>
<blockquote>
<p>Idea: <strong>Not all node’s neighbors are equally important</strong>；the NN should devote more computing power on that small but important part of the data. 【NN应该在数据中那个小而重要的部分上投入更多的计算能力。】</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2016.44.18.png" alt="截屏2024-07-22 16.44.18" style="zoom:25%;" />

<ul>
<li>Key benefit: Allows for (implicitly) specifying different importance values ($\alpha_{vu}$) to different neighbors</li>
</ul>
</li>
<li><p><strong>over-smoothing problem【过度平滑</strong>】：当stack的GNN层过多时，节点的感知域【Receptive field】可能过大甚至覆盖整个Graph，导致所有节点都感知整个图，使得所有节点趋于相同的embedding；</p>
<blockquote>
<p>Key：the embedding of a node is <strong>determined by its receptive field</strong></p>
</blockquote>
<ul>
<li>过多堆叠GNN Layer往往没用（不像CNN Layer那么有效），所以需要在GNN Layer较少（Shallow）的时候试图增强其感知能力</li>
<li>通过增强其message&#x2F;Aggregation模块的NN的学习能力&#x2F;通过前后concat线性层&#x2F;通过添加 <strong>skip connection</strong>（集成学习视角）</li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-22%2017.05.24.png" alt="截屏2024-07-22 17.05.24" style="zoom:25%;" />
</li>
<li><p>图增强&#x2F;处理：应对稀疏图&#x2F;稠密图&#x2F;图过大等问题进行特征增强&#x2F;图处理</p>
<ul>
<li>Graph Manipulation: Feature augmentation &#x2F; Structure manipulation</li>
<li>后面的内容会cover这部分</li>
</ul>
</li>
</ul>
<h2 id="Day3"><a href="#Day3" class="headerlink" title="Day3"></a>Day3</h2><h4 id="主要内容：GNN"><a href="#主要内容：GNN" class="headerlink" title="主要内容：GNN"></a>主要内容：GNN</h4><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723085117822.png" alt="image-20240723085117822"></p>
<h4 id="Graph-Manipulation【图处理】"><a href="#Graph-Manipulation【图处理】" class="headerlink" title="Graph Manipulation【图处理】"></a>Graph Manipulation【图处理】</h4><ul>
<li><p>Graph Feature manipulation </p>
<ul>
<li>The input graph lacks features —— <strong>feature augmentation</strong></li>
</ul>
</li>
<li><p>Graph Structure manipulation </p>
<ul>
<li><p><strong>The graph is too sparse</strong> —— Add virtual nodes &#x2F; edges </p>
<blockquote>
<p>稀疏图：通常添加两跳虚边；例如在作者-文章的二部图中可以连接同一篇paper的coop的作者</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2009.22.39.png" alt="截屏2024-07-23 09.22.39" style="zoom:15%;" />
</li>
<li><p><strong>The graph is too dense</strong> —— Sample neighbors when doing message passing </p>
<blockquote>
<p>消息传递时随机采样而不使用所有的节点；和使用所有的子节点进行训练的期望一致，但是可以减少训练开销</p>
</blockquote>
</li>
<li><p><strong>The graph is too large</strong> —— Sample subgraphs to compute embeddings</p>
</li>
</ul>
</li>
</ul>
<h4 id="unsupervised-self-supervised"><a href="#unsupervised-self-supervised" class="headerlink" title="unsupervised &#x2F; self- supervised"></a>unsupervised &#x2F; self- supervised</h4><blockquote>
<p>我理解的这两个概念的区分：无监督更多的不设置标签例如clustering；自监督需要自己创建的标签；</p>
</blockquote>
<ol>
<li><strong>Node Level：</strong>使用节点的统计量作为标签进行训练， such as clustering coefficient, PageRank, …</li>
<li><strong>Edge Level：</strong>把一些edge进行遮蔽hide&#x2F;mask进行作为标签进行训练</li>
<li><strong>Graph Level：</strong>图同构信息</li>
</ol>
<h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><ul>
<li>二分类模型的评估：混淆矩阵 &#x2F; TPR&#x3D;&#x3D;RecallRate &#x2F; ROC（AUC）</li>
<li>ROC曲线的xy坐标为两个rate：FPR以及TPR，性能越好的分类模型TPR&gt;&gt;FPR；如何绘制？对一个模型取不同的<strong>阈值</strong>分别计算FPR以及TPR坐标图中的一个点，形成曲线</li>
</ul>
<h4 id="图数据的划分"><a href="#图数据的划分" class="headerlink" title="图数据的划分"></a>图数据的划分</h4><blockquote>
<p>和其它的NN不相同，图数据集不是分离的数据点，所以需要特殊的数据划分(train&#x2F;valid&#x2F;test)；这一部分的内容设计更加复杂的划分方法，在很多论文有所涉及，这里论述的只是最基础的划分方法。</p>
</blockquote>
<ul>
<li><p><strong>Transductive Setting</strong>【直推】</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723104152426.png" alt="image-20240723104152426" style="zoom:33%;" />

<ol>
<li><p><strong>定义</strong>：在Transductive setting中，训练数据和测试数据的节点都属于同一个图。<strong>模型可以在训练时访问整个图的结构信息</strong>，包括测试节点及其连接，但不能访问测试节点的标签。</p>
</li>
<li><p><strong>特点</strong>：</p>
<ul>
<li><strong>同一个图</strong>：训练和测试都在同一个图上进行，测试节点在训练时是已知的。</li>
<li><strong>全局信息</strong>：模型可以利用整个图的结构信息来进行学习，这包括训练节点和测试节点之间的连接。</li>
<li><strong>目标</strong>：学习节点的嵌入或特征，使得在测试节点上的分类或回归任务表现良好。【Only applicable to node &#x2F; edge prediction tasks】无法在Graph-Level分类相关的任务使用。</li>
</ul>
</li>
</ol>
</li>
<li><p><strong>Inductive Setting</strong>【归纳】</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723104136425.png" alt="image-20240723104136425" style="zoom: 33%;" />

<ol>
<li><p><strong>定义</strong>：在Inductive setting中，训练数据和测试数据的节点属于不同的图。<strong>模型在训练时只能访问训练图的结构和标签信息，测试时模型需要在完全未知的图或新节点上进行预测。</strong></p>
</li>
<li><p>特点：</p>
<ul>
<li><strong>不同的图</strong>：训练和测试在不同的图上进行，或者在同一个图上但测试节点在训练时是未知的。</li>
<li><strong>局部信息</strong>：模型不能利用测试图的结构信息进行训练，只能基于训练图进行学习。</li>
<li><strong>目标</strong>：在新的图或新节点上<strong>泛化良好</strong>，即使这些图或节点在训练时不可见。【Applicable to node &#x2F; edge &#x2F; graph tasks】</li>
</ul>
</li>
</ol>
</li>
<li><p>具体示例：<strong>Transductive</strong> link prediction split：分为四类边【训练用信息边、训练用监督边、验证用边、测试用边】</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723105202851.png" alt="image-20240723105202851" style="zoom: 25%;" />



<h4 id="The-Power-of-GNN"><a href="#The-Power-of-GNN" class="headerlink" title="The Power of GNN"></a>The Power of GNN</h4><ul>
<li>对每个节点的计算可以构建一个计算图【Computational Graph】，对应一颗以该节点为根的子树；在该结构的<strong>aggragation聚合层</strong>为<strong>单射</strong>时其表达能力最强【单射可以区分不同的结构】</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2011.54.37.png" alt="截屏2024-07-23 11.54.37" style="zoom: 25%;" />

<ul>
<li><p>Key observation: **Expressive power of GNNs can be characterized by that of neighbor aggregation functions they use.**【GNNs 的表达能力可以用其使用的 neighbor aggregation functions 所表征】</p>
</li>
<li><p><strong>e.g. failure 案例研究：</strong></p>
<ul>
<li><strong>GCN：</strong>使用平均池化</li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723120112059.png" alt="image-20240723120112059" style="zoom:25%;" />

<ul>
<li><strong>GraphSAGE：</strong>使用最大池化</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-23%2012.00.31.png" alt="截屏2024-07-23 12.00.31" style="zoom:25%;" />

<ul>
<li><p>如上案例所示，GCN and GraphSAGE’s aggregation functions <strong>fail to distinguish some basic multi-sets【可重复元素集合】</strong>; <strong>hence not injective.【因此非most powerful】</strong></p>
</li>
<li><p>THE <strong>most expressive GNN</strong> in the class of message-passing GNNs：<strong>Graph Isomorphism Network (GIN)</strong> </p>
<blockquote>
<p>GIN‘s neighbor aggregation function is injective.</p>
</blockquote>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240723123526056.png" alt="image-20240723123526056" style="zoom:25%;" /></li>
</ul>
<h2 id="Day3-Lab"><a href="#Day3-Lab" class="headerlink" title="Day3-Lab"></a>Day3-Lab</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1zunZQaGzLr782y3tkq3492rvw9UyY30I">Link</a></p>
<p>在Colab 2中，将<strong>使用PyTorch Geometric (PyG) 构建自己的图神经网络</strong>，并将该模型<strong>应用于两个Open Graph Benchmark (OGB)数据集</strong>。这两个数据集将用于基准测试你的模型在两个不同图任务上的性能：1）<strong>节点属性预测</strong>，预测单个节点的属性；2）<strong>图属性预测</strong>，预测整个图或子图的属性。</p>
</blockquote>
<h4 id="实现一个简单的GCN用于节点预测"><a href="#实现一个简单的GCN用于节点预测" class="headerlink" title="实现一个简单的GCN用于节点预测"></a>实现一个简单的GCN用于节点预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GCN</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, output_dim, num_layers, dropout, return_embeds=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GCN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># GCNConv层列表</span></span><br><span class="line">        self.convs = torch.nn.ModuleList()</span><br><span class="line">        <span class="comment"># 1D批归一化层列表</span></span><br><span class="line">        self.bns = torch.nn.ModuleList()</span><br><span class="line">        <span class="comment"># log softmax层</span></span><br><span class="line">        self.softmax = torch.nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建第一个GCN层</span></span><br><span class="line">        self.convs.append(torch_geometric.nn.GCNConv(input_dim, hidden_dim))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建剩余的GCN层和批归一化层</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers - <span class="number">1</span>):</span><br><span class="line">            self.convs.append(torch_geometric.nn.GCNConv(hidden_dim, hidden_dim))</span><br><span class="line">            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建输出层</span></span><br><span class="line">        self.convs.append(torch_geometric.nn.GCNConv(hidden_dim, output_dim))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 零化概率</span></span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 是否返回节点嵌入</span></span><br><span class="line">        self.return_embeds = return_embeds</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_parameters</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 重置所有GCN层的参数</span></span><br><span class="line">        <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs:</span><br><span class="line">            conv.reset_parameters()</span><br><span class="line">        <span class="comment"># 重置所有批归一化层的参数</span></span><br><span class="line">        <span class="keyword">for</span> bn <span class="keyword">in</span> self.bns:</span><br><span class="line">            bn.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj_t</span>):</span><br><span class="line">        out = x</span><br><span class="line">        <span class="comment"># 应用第一个GCN层和ReLU激活</span></span><br><span class="line">        out = self.convs[<span class="number">0</span>](out, adj_t)</span><br><span class="line">        out = torch.nn.functional.relu(out)</span><br><span class="line">        out = torch.nn.functional.dropout(out, p=self.dropout, training=self.training)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用剩余的GCN层、批归一化、ReLU和dropout</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.convs) - <span class="number">1</span>):</span><br><span class="line">            out = self.convs[i](out, adj_t)</span><br><span class="line">            out = self.bns[i-<span class="number">1</span>](out)</span><br><span class="line">            out = torch.nn.functional.relu(out)</span><br><span class="line">            out = torch.nn.functional.dropout(out, p=self.dropout, training=self.training)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用最终的GCN层</span></span><br><span class="line">        out = self.convs[-<span class="number">1</span>](out, adj_t)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果return_embeds为False，则应用log softmax</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.return_embeds:</span><br><span class="line">            out = self.softmax(out)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>



<h2 id="Day4"><a href="#Day4" class="headerlink" title="Day4"></a>Day4</h2><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.08.33.png" alt="截屏2024-07-29 11.08.33"></p>
<h4 id="Heterogenous-graphs-异质图"><a href="#Heterogenous-graphs-异质图" class="headerlink" title="Heterogenous graphs[异质图]"></a>Heterogenous graphs[异质图]</h4><ul>
<li>异质图（Heterogeneous Graph）是指<strong>由不同类型的节点和边构成的图结构</strong>(a graph with multiple relation types)。 在异质图中，节点和边可以具有多样化的属性和关系，代表了不同实体以及它们之间的复杂关联。</li>
<li><strong>定义：</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-26%2010.50.16.png" alt="截屏2024-07-26 10.50.16" style="zoom:25%;" />

<ul>
<li><p>现实问题的Graph大多是异质图</p>
</li>
<li><p>可以将异质图中的节点&#x2F;边的类型编码为节点&#x2F;边的特征（例如one-hot）使得其变为标准图</p>
</li>
<li><p>How To Solve？</p>
</li>
</ul>
<h4 id="关系图卷积网络（Relational-Graph-Convolutional-Network-R-GCN）"><a href="#关系图卷积网络（Relational-Graph-Convolutional-Network-R-GCN）" class="headerlink" title="关系图卷积网络（Relational Graph Convolutional Network, R-GCN）"></a><strong>关系图卷积网络（Relational Graph Convolutional Network, R-GCN）</strong></h4><blockquote>
<p>是一种扩展了传统图卷积网络（GCN）的模型，专门用于处理异质图中的多种关系。<strong>R-GCN通过在图卷积操作中引入关系类型来捕捉不同类型节点和边之间的复杂关系。</strong>这使得R-GCN在处理包含多种关系的复杂图数据时更加有效。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.02.59.png" alt="截屏2024-07-29 10.02.59" style="zoom:25%;" />

<ul>
<li>R-GCN存在问题？参数过多：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.05.34.png" alt="截屏2024-07-29 10.05.34" style="zoom:25%;" />

<ul>
<li><p>每一层的每一种relation类型都对应一个权重$\mathbf{W}_r^{(l)}$，如果关系类型过多，<strong>模型参数量会非常大，容易导致过拟合。</strong></p>
</li>
<li><p>为了防止过拟合，常用的两种正则化方法是<strong>使用块对角矩阵Block Diagonal Matrices</strong>和<strong>基&#x2F;字典学习</strong>：</p>
<ol>
<li><p><strong>使用块对角矩阵：</strong>这种方法通过强制权重矩阵 $\mathbf{W}_r^{(l)}$ 具有块对角结构，从而减少参数数量。具体来说，块对角矩阵<strong>将权重矩阵分成若干个较小的子矩阵（块</strong>），并使得这些子矩阵之间没有相互影响。</p>
<p> 关键思想：<strong>稀疏化权重矩阵</strong></p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.18.21.png" alt="截屏2024-07-29 10.18.21" style="zoom:25%;" />
</li>
<li><p><strong>基&#x2F;字典学习（Basis&#x2F;Dictionary Learning）</strong>:每种关系类型的权重矩阵$\mathbf{W}_r^{(l)}$<strong>由若干基权重矩阵的线性组合表示</strong>。具体来说，定义一组基权重矩阵${ \mathbf{B}_1, \mathbf{B}_2, \ldots, \mathbf{B}_K }$，然后通过关系特定的系数组合这些基权重矩阵来构造每个关系类型的权重矩阵。</p>
<p> 关键思想：使不同的权重矩阵<strong>共享参数</strong></p>
</li>
</ol>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2010.20.11.png" alt="截屏2024-07-29 10.20.11" style="zoom:25%;" /></li>
</ul>
<h4 id="GNN-vs-Heterogenous-GNN"><a href="#GNN-vs-Heterogenous-GNN" class="headerlink" title="GNN vs Heterogenous GNN"></a>GNN vs Heterogenous GNN</h4><ul>
<li><p>两者的区别？</p>
<ol>
<li><p><strong>Message：</strong>消息传递阶段，H-GNN对关系类型建模，不同的关系类型具有不同的linear权重</p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729111639298.png" alt="image-20240729111639298" style="zoom:25%;" />

 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.16.31.png" alt="截屏2024-07-29 11.16.31" style="zoom: 25%;" />
</li>
<li><p><strong>Aggregation：</strong>聚合阶段，H-GNN同样对关系类型进行建模，使用<strong>两阶段的聚合方法</strong>：先对同种关系类型的消息进行聚合，然后进行总的聚合（例如concat）</p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.18.03.png" alt="截屏2024-07-29 11.18.03" style="zoom:25%;" />

 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.18.22.png" alt="截屏2024-07-29 11.18.22" style="zoom:25%;" />
</li>
<li><p><strong>Prediction：</strong>在预测阶段，需要考虑不同的关系类型</p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729111944075.png" alt="image-20240729111944075" style="zoom:25%;" /></li>
</ol>
</li>
</ul>
<h2 id="Day5"><a href="#Day5" class="headerlink" title="Day5"></a>Day5</h2><h4 id="Knowledge-Graphs"><a href="#Knowledge-Graphs" class="headerlink" title="Knowledge Graphs"></a>Knowledge Graphs</h4><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2011.22.37.png" alt="截屏2024-07-29 11.22.37"></p>
<h4 id="关系模式"><a href="#关系模式" class="headerlink" title="关系模式"></a>关系模式</h4><ol>
<li><p><strong>对称关系（Symmetric Relations）</strong></p>
<p> <strong>定义</strong>：如果关系 ( r(h, t) ) 成立，那么 ( r(t, h) ) 也成立。</p>
<p> <strong>符号表示</strong>：$$ r(h, t) \Rightarrow r(t, h) \forall h, t $$</p>
<p> <strong>示例</strong>：</p>
<ul>
<li><strong>家人（Family）</strong>：如果Alice是Bob的家人，那么Bob也是Alice的家人。</li>
<li><strong>室友（Roommate）</strong>：如果Alice是Bob的室友，那么Bob也是Alice的室友。</li>
</ul>
</li>
<li><p><strong>反对称关系（Antisymmetric Relations）</strong></p>
<p> <strong>定义</strong>：如果关系 ( r(h, t) ) 成立，那么 ( r(t, h) ) 不成立。</p>
<p> <strong>符号表示</strong>：$$ r(h, t) \Rightarrow \neg r(t, h) \forall h, t $$</p>
<p> <strong>示例</strong>：</p>
<ul>
<li><strong>上位词（Hypernym）</strong>：一个词具有更广泛的意义。例如，“狗（dog）”是“贵宾犬（poodle）”的上位词，但反之不成立。</li>
</ul>
</li>
<li><p><strong>逆关系（Inverse Relations）</strong></p>
<p> <strong>定义</strong>：如果关系 ( r(h, t) ) 成立，那么存在一个逆关系 ( r’(t, h) ) 也成立。</p>
<p> <strong>符号表示</strong>：$$ r(h, t) \Rightarrow r’(t, h) $$</p>
<p> <strong>示例</strong>：</p>
<ul>
<li><strong>导师与学生（Advisor and Advisee）</strong>：如果Dr. Smith是John的导师，那么John是Dr. Smith的学生。</li>
</ul>
</li>
<li><p><strong>传递关系（Composition&#x2F;Transitive Relations）</strong></p>
<p> <strong>定义</strong>：如果 ( r(x, y) ) 和 ( r(y, z) ) 都成立，那么 ( r(x, z) ) 也成立。</p>
<p> <strong>符号表示</strong>：$$ r(x, y) \land r(y, z) \Rightarrow r(x, z) \forall x, y, z $$</p>
<p> <strong>示例</strong>：</p>
<ul>
<li><strong>家庭关系</strong>：我母亲的丈夫是我的父亲。如果Alice是Bob的母亲，Bob是Charlie的丈夫，那么Alice是Charlie的母亲。</li>
</ul>
</li>
<li><p><strong>一对多关系（1-to-N Relations）</strong></p>
<p><strong>定义</strong>：一个实体可以与多个实体通过相同关系相关联。</p>
<p><strong>符号表示</strong>：$$ r(h, t_1) , r(h, t_2), \ldots , r(h, t_n) $$ 都成立。</p>
<p><strong>示例</strong>：</p>
<ul>
<li><strong>学生关系（StudentsOf）</strong>：一个教师有多个学生。例如，教师Dr. Smith有学生John、Emily、Sarah。</li>
</ul>
</li>
</ol>
<h4 id="KG-Completion"><a href="#KG-Completion" class="headerlink" title="KG Completion"></a>KG Completion</h4><ul>
<li><p>KG的补全：例如Is link (h,r,t) in the KG?</p>
</li>
<li><p>Introduce <strong>TransE &#x2F; TransR &#x2F; DistMult &#x2F; ComplEx models</strong>【四种<strong>知识图谱嵌入</strong>的方法】 with different embedding space and expressiveness</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/image-20240729122411091.png" alt="image-20240729122411091" style="zoom: 25%;" />



<h4 id="KG-Reasoning"><a href="#KG-Reasoning" class="headerlink" title="KG Reasoning"></a>KG Reasoning</h4><ul>
<li><strong>知识图谱具体的推理任务需要根据一系列的query来表述</strong>，也就是说在给定的<strong>不完整并且大规模的知识图谱</strong>中对输入的自然语言形式的query进行推理并返回结果，而query可以分为<strong>One-hop Queries，Path Queries和Conjunctive Queries</strong>三种，分别是<strong>单次跳转的推理、多次跳转的推理、联合推理</strong>，可以用下面的图来表示三种query之间的关系：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.05.59.png" alt="截屏2024-07-29 13.05.59" style="zoom: 33%;" />

<ul>
<li><p><strong>可以使用知识图谱嵌入</strong>：考虑将知识图谱上的问答转化到<strong>向量空间</strong>中进行，而具体的方法就是将query也转换成一个向量，并使用嵌入模型的打分函数来评估结果。一个查询在向量空间中可以表示为：</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.10.50.png" alt="截屏2024-07-29 13.10.50" style="zoom: 33%;" /></li>
</ul>
<h4 id="Query2Box"><a href="#Query2Box" class="headerlink" title="Query2Box"></a>Query2Box</h4><blockquote>
<p>论文🔗：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.05969">QUERY2BOX: REASONING OVER KNOWLEDGE GRAPHS IN VECTOR SPACE USING BOX EMBEDDINGS</a></p>
</blockquote>
<ul>
<li>问题：在处理<strong>联合推理</strong>时每个节点都代表多个entity，如何<strong>在隐空间定义其交叉操作？</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.18.02.png" alt="截屏2024-07-29 13.18.02" style="zoom:25%;" />

<ul>
<li><strong>核心思想：</strong>Embed queries with hyper-rectangles (boxes)【<strong>将查询嵌入为超矩形即box</strong>】</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.22.48.png" alt="截屏2024-07-29 13.22.48" style="zoom:33%;" />

<ul>
<li>如何定义多个Box的<strong>交集【intersection】操作</strong>？</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.28.52.png" alt="截屏2024-07-29 13.28.52" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.28.08.png" alt="截屏2024-07-29 13.28.08" style="zoom:25%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.30.35.png" alt="截屏2024-07-29 13.30.35" style="zoom:25%;" />



<ul>
<li><strong>对于Union操作的扩展</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.40.29.png" alt="截屏2024-07-29 13.40.29" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.43.28.png" alt="截屏2024-07-29 13.43.28" style="zoom:25%;" />

<ul>
<li><p><strong>Query2Box的训练</strong></p>
<ol>
<li>在Query2Box模型中，需要<strong>训练的参数主要有所有的实体的嵌入向量，所有的关系的嵌入向量和Intersection运算中的各种参数。</strong></li>
<li>一个很直观的想法是，在训练Qeury2Box模型的过程中，对于一个查询q的嵌入向量，我们要让属于q中的实体v对应的打分函数最大化，而要让不在其中的打分函数最小化，为此需要用到负采样，也就是在训练的过程中，对于每个正样本v随机选取一个负样本与之对应，<strong>具体的训练过程可以分为以下几个步骤：</strong></li>
</ol>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.52.13.png" alt="截屏2024-07-29 13.52.13" style="zoom: 50%;" />

<ol start="3">
<li>在训练之前我们需要提取出一系列查询，而这个过程称为<strong>Query Generation</strong>，可以通过一系列模板生成：</li>
</ol>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.53.39.png" alt="截屏2024-07-29 13.53.39" style="zoom:25%;" /></li>
</ul>
<h2 id="Day6"><a href="#Day6" class="headerlink" title="Day6"></a>Day6</h2><h4 id="Fast-neural-subgraph-matching-and-Counting"><a href="#Fast-neural-subgraph-matching-and-Counting" class="headerlink" title="Fast neural subgraph matching and Counting"></a>Fast neural subgraph matching and Counting</h4><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.25.png" alt="截屏2024-07-29 13.58.25"></p>
<h4 id="子图"><a href="#子图" class="headerlink" title="子图"></a>子图</h4><ul>
<li><p>图的组成结构，可以区分和描述一个图</p>
</li>
<li><p>两种定义方法：</p>
<ol>
<li><p><strong>Node-induced subgraph：节点诱导子图</strong></p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.44.51.png" alt="截屏2024-07-30 15.44.51" style="zoom: 25%;" />
</li>
<li><p><strong>Edge-induced subgraph：边诱导子图</strong></p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.45.04.png" alt="截屏2024-07-30 15.45.04" style="zoom: 25%;" />
</li>
<li><p>两种方法生成的子图都有其应用范围：例如<strong>化学结构适合节点诱导</strong>，而<strong>知识图谱适合边诱导</strong>（因为知识图谱focus on边表示的逻辑关系）</p>
</li>
</ol>
</li>
</ul>
<h4 id="Graph-Isomorphism-图同构"><a href="#Graph-Isomorphism-图同构" class="headerlink" title="Graph Isomorphism 图同构"></a>Graph Isomorphism 图同构</h4><ul>
<li>定义：core idea <strong>双射 bijection</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.51.25.png" alt="截屏2024-07-30 15.51.25" style="zoom: 25%;" />

<ul>
<li>图同构的判断问题是NP-hard</li>
<li>引申：<strong>子图同构问题：</strong>图G1 和 图G2的子图 同构：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-30%2015.54.07.png" alt="截屏2024-07-30 15.54.07" style="zoom:25%;" />

<h4 id="Network-motifs-网络图案"><a href="#Network-motifs-网络图案" class="headerlink" title="Network motifs 网络图案"></a>Network motifs 网络图案</h4><ul>
<li><p>定义：<strong>recurring, significant patterns of interconnections <strong>在网络中</strong>反复出现的、具有显著意义的</strong>连接模式。</p>
</li>
<li><p>特点特性：</p>
<ol>
<li><strong>Pattern:</strong> Small (node-induced) Subgraph <strong>较小的节点诱导子图</strong>，节点诱导必须满足所有诱导节点之间的边都一致</li>
<li><strong>Recurring:</strong> Found Many Times, i.e., with High Frequency <strong>重复出现：多次被发现，即具有高频率</strong><ul>
<li><strong>定义</strong>：一个模式在网络中被认为是motif，如果它在网络中多次出现，其出现频率明显高于偶然情况。</li>
<li><strong>如何定义频率</strong>：通过在网络中计算特定模式的出现次数来定义频率。</li>
</ul>
</li>
<li><strong>Significant:</strong> More Frequent Than Expected, i.e., in Randomly Generated Graphs<strong>显著性：出现频率高于预期，即在随机生成的图中</strong></li>
</ol>
</li>
<li><p><strong>如何判断motifs？</strong>使用<strong>相同的图统计指标</strong>生成随机图，利用<strong>统计指标</strong>（例如z分数：对所有的待评估图案作为一个向量，计算z分数后归一化）评估 motif 的显著性。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2011.33.50.png" alt="截屏2024-08-05 11.33.50" style="zoom:33%;" />

<h4 id="Neural-Subgraph-Match"><a href="#Neural-Subgraph-Match" class="headerlink" title="Neural Subgraph Match"></a>Neural Subgraph Match</h4><ul>
<li>如何基于神经网络（GNN）进行子图匹配（which is np-hard）</li>
</ul>
<h4 id="方法：使用Order-Embedding-有序嵌入空间"><a href="#方法：使用Order-Embedding-有序嵌入空间" class="headerlink" title="方法：使用Order Embedding 有序嵌入空间"></a>方法：使用Order Embedding 有序嵌入空间</h4><ul>
<li>核心思想：把所有的图<strong>嵌入为高维向量</strong>，把<strong>子图同构的关系转换为向量的所有维数均小于的关系</strong>。</li>
<li>优点：<ol>
<li>这种嵌入空间可以表示子图同构的关系，<strong>符合传递性、反对称性等要求；</strong></li>
<li>由于空图嵌入为0，并且空图是所有图的子图，所以<strong>所有图嵌入向量应该是全正向量</strong>。</li>
</ol>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2013.13.51.png" alt="截屏2024-08-05 13.13.51" style="zoom:33%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-05%2013.12.57.png" alt="截屏2024-08-05 13.12.57" style="zoom:33%;" />

<ul>
<li><p>如何训练？</p>
<ol>
<li><p>利用GNN对查询图和目标图进行嵌入。</p>
</li>
<li><p>构建训练样本，包括<strong>正样本和负样本：</strong>其中正样本包括一个查询图$G_q$以及一个目标图$G_t$，并且查询图是目标图的同构子图，可以通过BFS等策略进行构建。</p>
</li>
<li><p><strong>最大化边缘损失。</strong>只有当任意维数不符合小于条件时，存在损失。</p>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2006.28.40.png" alt="截屏2024-08-06 06.28.40" style="zoom:33%;" /></li>
</ol>
</li>
</ul>
<h4 id="如何找出-Frequent-SubGraphs【即-motifs】"><a href="#如何找出-Frequent-SubGraphs【即-motifs】" class="headerlink" title="如何找出 Frequent SubGraphs【即 motifs】"></a>如何找出 Frequent SubGraphs【即 motifs】</h4><ul>
<li>一般的办法是：枚举一幅图的所有相同size的子图，然后数其中motifs的数量。[computationally hard]</li>
</ul>
<h4 id="SPMiner"><a href="#SPMiner" class="headerlink" title="SPMiner"></a>SPMiner</h4><ul>
<li>SPMiner 使用神经网络模型将图结构分解、编码，并通过模式增长搜索频繁子图。<ol>
<li>将图 ( G ) 分解成节点锚定的重叠邻域。具体来说，即围绕每个节点及其邻居构造一个子图，这些子图可以彼此重叠。</li>
<li>将这些子图嵌入到一个有序的嵌入空间中。</li>
<li><strong>搜索过程</strong>：通过扩展模式来找到频繁子图。</li>
</ol>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2007.40.51.png" alt="截屏2024-08-06 07.40.51" style="zoom: 25%;" />

<ul>
<li><strong>第三步：搜索过程详解</strong><ul>
<li>从初始节点出发，每一步进行扩展。目标是在指定的k步之后，其右上方区域的节点数量最大化。</li>
<li>简单来说，就是找到一种最优的游走方法，使得右上方区域的节点数量最大化（代表该motif是最多出现的子图模式）。</li>
<li><strong>每一步该如何选择：启发式</strong>规则：例如贪心（每次最大化）、动态规划。</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2007.51.22.png" alt="截屏2024-08-06 07.51.22" style="zoom:33%;" />





<h2 id="Day7"><a href="#Day7" class="headerlink" title="Day7"></a>Day7</h2><h3 id="利用GNN进行推荐"><a href="#利用GNN进行推荐" class="headerlink" title="利用GNN进行推荐"></a>利用GNN进行推荐</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.45.png" alt="截屏2024-07-29 13.58.45"></p>
<h4 id="推荐系统中的Graph"><a href="#推荐系统中的Graph" class="headerlink" title="推荐系统中的Graph"></a>推荐系统中的Graph</h4><ul>
<li>推荐系统可以自然得被建模为一个user-item的二部图</li>
<li>从而使得推荐的过程作为一个<strong>Link-prediction</strong>的任务：预测user-item的缺失边从而进行推荐</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2009.10.27.png" alt="截屏2024-08-06 09.10.27" style="zoom: 25%;" />

<ul>
<li>局限性：无法计算每个user和item之间的f分数，计算量过大；解决：使用两阶段方法，即先召回后排序</li>
</ul>
<h4 id="Embedding-based-models"><a href="#Embedding-based-models" class="headerlink" title="Embedding-based models"></a>Embedding-based models</h4><ul>
<li><p>为了实现 u 和 v 之间的 score，将 u 和 v 嵌入相同维数的向量空间</p>
</li>
<li><p><strong>训练目标：使得 recall@K 召回的成功率最大。</strong>（但这是<strong>不可微分</strong>的，无法使用梯度下降进行优化）</p>
<blockquote>
<p>ML-based的优化目标要求可微，可以使用等价的优化目标[surrogate loss functions]，which should align well with the original training objective[这应该与最初的优化目标非常一致。]</p>
</blockquote>
<ol>
<li><p><strong>Binary Loss</strong></p>
<ul>
<li>对于正负样本而言，就是一个二分类问题。</li>
</ul>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2009.40.51.png" alt="截屏2024-08-06 09.40.51" style="zoom:33%;" />

<ul>
<li>局限性：这样考虑所有user的loss之和，没有考虑user之间的个性差异。</li>
</ul>
</li>
<li><p><strong>Bayesian Personalized Ranking (BPR) loss</strong> </p>
<ul>
<li>defined in a <strong>personalized</strong> manner.</li>
<li>对于每一个user分别计算损失，<strong>目标是使得正边分数减去负边分数尽可能大</strong></li>
</ul>
 <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2010.36.13.png" alt="截屏2024-08-06 10.36.13" style="zoom: 33%;" /></li>
</ol>
</li>
<li><p><strong>性能：</strong></p>
<ul>
<li>Embedding-based models have achieved SoTA in recommender systems. [性能很好]</li>
<li>为什么 Embedding-based models work well？<ul>
<li>潜在的思想：<strong>利用协同过滤的思想，对一个user的推荐借助了其它特征相似的user。</strong></li>
<li>embedding <strong>进行嵌入实际上是嵌入为相对低维的向量，这种低维的向量无法对所有的用户&#x2F;商品特征直接记忆或者建模，必须通过捕捉到其相似性&#x2F;相似特征</strong>，这也是vector-embedding的优势和作用所在。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="representative-GNN-approaches-for-recommender-systems"><a href="#representative-GNN-approaches-for-recommender-systems" class="headerlink" title="representative GNN approaches for recommender systems"></a>representative GNN approaches for recommender systems</h4><blockquote>
<p>推荐阅读：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39388410/article/details/106970194">图神经网络用于推荐系统问题（NGCF，LightGCN，UltraGCN）</a></p>
</blockquote>
<ul>
<li><p>传统的协同过滤方式[使用简单的binaryloss作为优化目标进行优化]：通过score function训练vector encoder，这种方法比较shallow浅层，无法捕捉到<strong>高阶的图结构信息</strong>[例如多跳信息]。</p>
</li>
<li><p>两种优化方法：(1）NGCF：使用GNN捕捉u-i二部图的多跳结构信息 （2）</p>
</li>
</ul>
<h4 id="一、Neural-Graph-Collaborative-Filtering-NGCF"><a href="#一、Neural-Graph-Collaborative-Filtering-NGCF" class="headerlink" title="一、Neural Graph Collaborative Filtering (NGCF)"></a>一、Neural Graph Collaborative Filtering (NGCF)</h4><ul>
<li>主要的思想是<strong>通过GNN来学习user&#x2F;item的embedding向量</strong>，从而捕捉到更深层次的图结构信息[High-order graph structure is captured through iterative neighbor aggregation.]。【例如对于GNN可以通过一个item来连接不同的用户，从而捕捉该信息】</li>
<li>框架：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2011.12.09.png" alt="截屏2024-08-06 11.12.09" style="zoom:25%;" />

<ul>
<li>其中的item&#x2F;user embedding以及GNN parameters 都是可以学习的。</li>
</ul>
<h4 id="二、LightGCN"><a href="#二、LightGCN" class="headerlink" title="二、LightGCN"></a>二、LightGCN</h4><blockquote>
<p>论文🔗：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.02126">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</a></p>
<p>一种简化版的图卷积神经网络（GCN），专门用于推荐系统。与传统的GCN相比，LightGCN通过简化模型结构，<strong>去掉非线性激活和特征变换部分</strong>，使得模型更加轻量级且易于训练，同时保持了推荐效果。</p>
</blockquote>
<ul>
<li><strong>动机：</strong>前述的 NGCF 同时迭代学习：（1）item&#x2F;user embedding （2）GNN parameters  <ul>
<li>存在问题：<strong>embedding的参数量  &gt;&gt; GNN的参数量</strong> [原因：embedding参数量达到（#user&#x2F;item * embedding dim），其中#user&#x2F;item非常非常大]，说明embedding已经有足够表达能力，试图消除GNN中的可学习参数。</li>
</ul>
</li>
<li>user-item二部图的<strong>邻接矩阵、Embedding矩阵</strong>：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2011.28.00.png" alt="截屏2024-08-06 11.28.00" style="zoom: 25%;" />

<ul>
<li>LightGCN直接把GCN中的<strong>非线性激活、特征变换部分、自信息部分</strong>删除[作者通过消融实验验证了这个部分是没有帮助的]</li>
<li>embedding的传递仅通过邻居节点embedding的聚合实现【不包含自信息部分】，并且这个聚合使用平均效果就已经很好了。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2014.01.56.png" alt="截屏2024-08-06 14.01.56" style="zoom: 50%;" />

<ul>
<li>最后的embedding通过<strong>每一层计算的embedding的加权进行计算：</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2014.04.23.png" alt="截屏2024-08-06 14.04.23" style="zoom:50%;" />



<h4 id="如何将扩展到大规模的推荐系统中？PinSage"><a href="#如何将扩展到大规模的推荐系统中？PinSage" class="headerlink" title="如何将扩展到大规模的推荐系统中？PinSage"></a>如何将扩展到大规模的推荐系统中？PinSage</h4><blockquote>
<p><strong>Pinterest</strong> 是一款类似于微博的社交软件，需要推荐<strong>超大规模（web-scale）的数据</strong>。<strong>PinSage</strong>成功应用在了<strong>Pinterest的推荐系统</strong></p>
</blockquote>
<ul>
<li>主要的思想类似于 GraphSAGE</li>
<li>主要的改进包括：<ol>
<li><strong>对比学习</strong>的思想：选取两篇博文作为样本，相似的作为正样本。学习目标在于将相似样本的嵌入空间尽可能接近，否则尽可能远离。</li>
<li>不使用全部的邻节点，而是进行采样。并且和GraphSAGE不同的是，进行<strong>重要性采样</strong>，即根据重要性排序后进行top-n采样。</li>
<li>在BRP损失中，需要为每一个user采样负样本，开销较大。这里使用了<strong>Shared negative samples</strong> across users in a mini-batch，即负样本共享的方法，所有用户共享负样本，只采样一次。</li>
<li>由于推荐系统在工业上需要进行细粒度的召回[从百万中取回数十商品]，所以随机采样的negative samples过于简单，导致推荐系统性能比较差。PinSage使用了<strong>Hard negative samples</strong>的方法，迫使模型可以区分困难的负样本。<ul>
<li><strong>如何生成</strong>困难的负样本？从用户节点进行<strong>随机游走打分</strong>，找出<strong>关联度较高但不是最高</strong>的节点。</li>
<li><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-06%2018.41.18.png" alt="截屏2024-08-06 18.41.18" style="zoom: 25%;" /></li>
</ul>
</li>
<li>渐进式训练(Curriculum training)：如果训练全程都使用hard负样本，会导致模型收敛速度减半，训练时长加倍，因此PinSage采用了一种Curriculum训练的方式，即第一轮训练只使用简单负样本，帮助模型参数快速收敛到一个loss比较低的范围；后续训练中逐步加入hard负样本，让模型学会将很相似的物品与些微相似的区分开，方式是第n轮训练时给每个物品的负样本集合中增加n-1个hard负样本。</li>
</ol>
</li>
</ul>
<h2 id="Day8"><a href="#Day8" class="headerlink" title="Day8"></a>Day8</h2><h3 id="Deep-generative-models-for-graphs"><a href="#Deep-generative-models-for-graphs" class="headerlink" title="Deep generative models for graphs"></a>Deep generative models for graphs</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-07-29%2013.58.53.png" alt="截屏2024-07-29 13.58.53"></p>
<h4 id="图生成问题"><a href="#图生成问题" class="headerlink" title="图生成问题"></a>图生成问题</h4><ul>
<li>使用<strong>图生成模型</strong>进行图生成；</li>
<li>实际应用：药物发现等；</li>
</ul>
<p>在图的生成模型（Graph Generative Model）中，<strong>p_data(G)</strong> 和 <strong>p_model(G)</strong> 是两个重要的概率分布概念，用于描述不同来源的图 ( G ) 的生成过程。</p>
<ul>
<li>数据分布：<ol>
<li>$p_{data}(G)$ 表示真实数据的图 $G$ 的分布，也就是从真实世界的图数据中提取的分布。这个分布反映了自然界或某个特定领域中实际存在的图结构的统计特性。</li>
<li>$ p_{model}(G)$ 是生成模型对图 $G$ 的分布。它表示生成模型在学习了训练数据后，生成图 $G$ 的概率分布。</li>
<li>图生成模型的目标是<strong>通过训练，使 $p_{model}(G)$ 尽可能接近 $p_{data}(G) $（密度估计），从而生成与真实数据分布一致的图；并从该分布中采样进行图生成任务（采样）</strong></li>
</ol>
</li>
</ul>
<ul>
<li><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.01.10.png" alt="截屏2024-08-09 10.01.10" style="zoom: 25%;" /></li>
</ul>
<ul>
<li>针对上图中Goal部分的两个目标（<strong>密度估计以及采样</strong>），提出以下两个方法：</li>
<li>如何使得$ p_{model}(G)$ 尽可能接近$p_{data}(G)$ ？<strong>最大似然概率</strong>，找到参数使得生成已观测的数据分布的样本的概率最大化。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.04.19.png" alt="截屏2024-08-09 10.04.19" style="zoom: 25%;" />

<ul>
<li><p>如何从$ p_{model}(G)$ 中进行采样？通过对<strong>随机单位噪声</strong>采样，通过model对噪声进行变换，使得其符合目标复杂分布：</p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.11.01.png" alt="截屏2024-08-09 10.11.01" style="zoom:25%;" /></li>
</ul>
<h4 id="将图的生成过程建模为一个序列的生成过程"><a href="#将图的生成过程建模为一个序列的生成过程" class="headerlink" title="将图的生成过程建模为一个序列的生成过程"></a>将图的生成过程建模为一个序列的生成过程</h4><ul>
<li>一个图的生成过程有<strong>两个层次：Node-Level 以及 Edge-Level</strong>，即从点和边的两个添加过程。</li>
<li>Node-Level：每一次添加一个节点。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.26.05.png" alt="截屏2024-08-09 10.26.05" style="zoom: 25%;" />

<ul>
<li>Edge-Level：每一次对添加的一个节点连接其和其它节点的边，这是一个序列。例如，在添加节点4后，将其和其它三个节点的边建模为一个序列。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2010.30.13.png" alt="截屏2024-08-09 10.30.13" style="zoom: 25%;" />

<ul>
<li><p>这样可<strong>以将图的生成建模为一个二维序列的生成过程。</strong></p>
</li>
<li><p>直觉：使用<strong>序列生成模型</strong></p>
</li>
</ul>
<h4 id="使用RNN进行序列生成：GraphRNN"><a href="#使用RNN进行序列生成：GraphRNN" class="headerlink" title="使用RNN进行序列生成：GraphRNN"></a>使用RNN进行序列生成：GraphRNN</h4><ul>
<li>使用两个RNN：<strong>NodeRNN以及EdgeRNN</strong>，分别用于生成节点序列以及连接边序列</li>
<li>工作原理：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.12.05.png" alt="截屏2024-08-09 13.12.05" style="zoom:25%;" />

<ul>
<li>分别使用SOS以及EOS token作为RNN的开始和结束的标志。</li>
<li>每一次的Edge-RNN的生成结束后，<strong>需要将Edge-RNN的Hidden State输入Node-RNN</strong>。这么做的直觉是：<strong>需要将边的连接信息输入Node-RNN来生成下一个节点，也就是说节点的生成是要依赖于边的连接信息的。</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.15.08.png" alt="截屏2024-08-09 13.15.08" style="zoom:25%;" />

<ul>
<li><strong>什么时候结束生成？</strong>当Node-RNN生成一个节点，但是这个节点在Edge-RNN中没有生成任何连接边时（如下图中的节点4所指示），说明图的生成结束。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.17.04.png" alt="截屏2024-08-09 13.17.04" style="zoom:25%;" />



<ul>
<li><p><strong>节点的生成顺序？</strong></p>
<ul>
<li>如果不规定节点生成顺序，n个节点最多有$O(n!)$多个顺序。</li>
<li>使用<strong>BFS</strong>顺序来规约节点的顺序。</li>
</ul>
</li>
<li><p><strong>评估模型生成好坏的指标？</strong></p>
<ul>
<li>视觉相似性。肉眼看起来生成的效果。</li>
<li>Graph统计指标的相似性。例如节点平均度等等。</li>
</ul>
</li>
</ul>
<h4 id="Graph-Convolutional-Policy-Network-GCPN"><a href="#Graph-Convolutional-Policy-Network-GCPN" class="headerlink" title="Graph Convolutional Policy Network (GCPN)"></a>Graph Convolutional Policy Network (GCPN)</h4><ul>
<li><p>GraphRNN的局限：当需要完成一些<strong>Goal-Directed图生成任务</strong>时（要使得生成的图<strong>符合或者最优化某些目标</strong>，例如<strong>在药物发现任务中，要最大化生成图结构的类药性</strong>），GraphRNN无法做到最优化这些目标。</p>
<blockquote>
<p>GraphRNN在生成图时主要关注图的结构和连通性，而不一定直接考虑生成图是否符合某些特定的优化目标。这种局限使得GraphRNN生成的图可能没有最优的性质，尤其是当目标特性与生成的图结构复杂且不直接相关时。</p>
</blockquote>
</li>
<li><p>GCPN主要使用了<strong>图表示学习以及强化学习</strong>的技术</p>
</li>
<li><p>图生成的过程中：<strong>将节点进行Node Embedding，并进行Link-Prediction任务。</strong></p>
  <img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.50.05.png" alt="截屏2024-08-09 13.50.05" style="zoom: 25%;" />
</li>
<li><p>是有强化学习使得agent模型和环境进行交互，通过反馈来完成需要最优化的指标。</p>
</li>
<li><p><strong>如何训练？</strong>两个主要部分：（1）使用给定图进行<strong>监督学习</strong>（2）<strong>强化学习。</strong></p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.54.46.png" alt="截屏2024-08-09 13.54.46" style="zoom: 25%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-09%2013.55.58.png" alt="截屏2024-08-09 13.55.58" style="zoom:25%;" />

<h2 id="Day8-1"><a href="#Day8-1" class="headerlink" title="Day8"></a>Day8</h2><h3 id="Advanced-topics-in-GNNs"><a href="#Advanced-topics-in-GNNs" class="headerlink" title="Advanced topics in GNNs"></a>Advanced topics in GNNs</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2008.51.38.png" alt="截屏2024-08-11 08.51.38"></p>
<h4 id="GNNs-存在的不足之处"><a href="#GNNs-存在的不足之处" class="headerlink" title="GNNs 存在的不足之处"></a>GNNs 存在的不足之处</h4><p><strong>（1）GNN中，如果节点具有相同的邻结构，那么它们将总是具有相同的embedding。</strong></p>
<ul>
<li>这么做有时候是不合理的，因为<strong>有时候需要对Graph中不同位置出现的相同邻结构的节点赋予不同的embdding</strong>。</li>
<li>即：从GNN的角度来看，<strong>Graph中不同位置出现的相同邻结构的节点是不可区分的</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2012.16.22.png" alt="截屏2024-08-11 12.16.22" style="zoom:25%;" />

<p><strong>（2）原则：如果节点具有不同的邻结构，GNN需要节点具有不同的embedding。</strong></p>
<ul>
<li>但是这个原则有时会被违反，例如下图所示的两个节点，由于它们的计算图一样， 所以GNN计算的embedding是一样的。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2012.19.11.png" alt="截屏2024-08-11 12.19.11" style="zoom:25%;" />

<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2012.21.14.png" alt="截屏2024-08-11 12.21.14" style="zoom:25%;" />

<p><strong>如何解决？</strong></p>
<ol>
<li>对于问题（1），<strong>引入 position-aware GNNs 使得其关注不同节点的位置信息。</strong></li>
<li>对于问题（2），<strong>引入 身份感知GNN（Identity-aware GNNs）</strong></li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2012.22.13.png" alt="截屏2024-08-11 12.22.13" style="zoom:25%;" />

<h4 id="Position-aware-GNNs"><a href="#Position-aware-GNNs" class="headerlink" title="Position-aware GNNs"></a>Position-aware GNNs</h4><blockquote>
<p>Paper🔗：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.04817">Position-aware Graph Neural Networks</a></p>
</blockquote>
<blockquote>
<h4 id="GPT-for-This："><a href="#GPT-for-This：" class="headerlink" title="GPT for This："></a>GPT for This：</h4><p><strong>Position-aware GNNs</strong> 是一种改进的图神经网络（GNN）模型，<strong>通过增强对节点在图中位置的感知能力来提升模型的表达能力</strong>。这种方法主要解决了传统GNN模型在处理图结构时对节点位置敏感度不足的问题。</p>
<h5 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h5><p>在图神经网络中，节点的特征表示和消息传递主要基于其邻居的特征。然而，这种方法可能忽略了节点在图中的相对位置和结构信息，特别是节点与图中其他节点的位置关系。Position-aware GNNs 通过引入位置感知机制，来改进这一点。</p>
<h5 id="关键特性"><a href="#关键特性" class="headerlink" title="关键特性"></a>关键特性</h5><ol>
<li><strong>位置编码（Position Encoding）</strong>：<ul>
<li><strong>定义</strong>：位置编码是将节点在图中的位置或相对位置的信息嵌入到节点的特征中。这可以包括节点的度数、在图中的路径距离或其他表示位置的特征。</li>
<li><strong>目的</strong>：增强模型对节点在图中具体位置的感知，帮助模型更好地捕捉图的结构信息。</li>
</ul>
</li>
<li><strong>位置感知机制</strong>：<ul>
<li><strong>方法</strong>：在传统的消息传递过程中，节点的特征信息不仅仅包括其邻居的特征，还包括位置编码信息。这种方法可以通过在每层GNN中加入位置编码来实现。</li>
<li><strong>集成</strong>：位置编码信息可以通过各种方式集成到GNN模型中，如与节点的原始特征拼接、加权求和或通过特定的网络层处理。</li>
</ul>
</li>
</ol>
</blockquote>
<ul>
<li>原因：GNNs 通常可以有效地捕捉Graph的结构信息（对于相同邻结构的节点使用相同的训练Label，并且相同邻结构的节点具有相同的计算图），但是很难捕捉Graph的位置信息。</li>
<li><strong>在 Position-aware 的任务中，GNNs 失效。</strong>【对于相同邻结构的节点，其具有相同的计算图，但是由于处于不同的位置，其具有不同的位置标签信息】</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2014.12.23.png" alt="截屏2024-08-11 14.12.23" style="zoom:25%;" />

<h5 id="锚点"><a href="#锚点" class="headerlink" title="锚点"></a>锚点</h5><ul>
<li>选择节点&#x2F;节点集合作为<strong>锚点&#x2F;锚点集合</strong>，节点在Graph中的位置可以表示为：到每一个锚点的距离。</li>
</ul>
<h5 id="P-GNN-过程"><a href="#P-GNN-过程" class="headerlink" title="P-GNN 过程"></a>P-GNN 过程</h5><ol>
<li><strong>样本化锚点集</strong>：选择 $O(\log⁡n)$个锚点集。</li>
<li><strong>节点嵌入</strong>：计算每个节点到这些锚点集的距离，并将这些距离作为节点的嵌入特征。</li>
<li>主要思想：<strong>随机采样锚点集合，并使用节点到锚点集合的距离作为节点的嵌入表征。</strong></li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2014.38.38.png" alt="截屏2024-08-11 14.38.38" style="zoom: 33%;" />



<h4 id="Identity-aware-GNNs"><a href="#Identity-aware-GNNs" class="headerlink" title="Identity-aware GNNs"></a>Identity-aware GNNs</h4><blockquote>
<p>paper 🔗：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.10320">Identity-aware Graph Neural Networks</a></p>
</blockquote>
<ul>
<li><p>问题：<strong>GNNs 对于不同的结构的节点，倘若其计算图相同，就会导致这些节点嵌入到相同的向量（不可区分这些节点）</strong>。这是GNNs存在的缺陷，无法识别到潜在的不同结构的图特征。对这样的节点的不可区分性，会使得GNNs在node&#x2F;link&#x2F;graph level的prediction等任务丧失性能。</p>
</li>
<li><p>解决：<strong>使用节点着色【node-coloring】-对节点特征进行增强：</strong></p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2016.18.49.png" alt="截屏2024-08-11 16.18.49" style="zoom:25%;" />

<ul>
<li>node-coloring 是 inductive的，即可以扩展到测试集的。因为(1)其具有排序不变性质</li>
</ul>
<h5 id="如何在-ID-GNN-中运用节点着色的思想：异构消息传递【Heterogenous-message-passing】"><a href="#如何在-ID-GNN-中运用节点着色的思想：异构消息传递【Heterogenous-message-passing】" class="headerlink" title="如何在 ID-GNN 中运用节点着色的思想：异构消息传递【Heterogenous message passing】"></a>如何在 ID-GNN 中运用节点着色的思想：异构消息传递【Heterogenous message passing】</h5><ul>
<li>通常在GNN中例如GCN，我们<strong>在同一层对所有节点使用相同的NN。</strong></li>
<li>ID-GNN中：对不同颜色的节点，使用不同的NN，即使用不同的消息传递&#x2F;聚合方法。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2016.25.03.png" alt="截屏2024-08-11 16.25.03" style="zoom:25%;" />

<h5 id="ID-GNN-Fast"><a href="#ID-GNN-Fast" class="headerlink" title="ID-GNN-Fast"></a>ID-GNN-Fast</h5><ul>
<li>如下图所示，计算图中<strong>每一层的着色节点数量表征着长度为层数的环的个数</strong>（例如，第三层有两个着色节点，代表图中有两个从根节点出发的长度为3的环）</li>
<li>可以将<strong>这种性质看作为节点的特征</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2016.33.54.png" alt="截屏2024-08-11 16.33.54" style="zoom: 25%;" />

<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul>
<li>总的来说，ID-GNN 是 GNN框架的一种通用的强大的扩展，可以嵌入GCN、GraphSAGE …..，只需要在消息传递的时候对节点使用异构的网络。并且通常可以得到收益。</li>
</ul>
<h4 id="GNNs-的鲁棒性研究"><a href="#GNNs-的鲁棒性研究" class="headerlink" title="GNNs 的鲁棒性研究"></a>GNNs 的鲁棒性研究</h4><ul>
<li>对抗样本for GNN</li>
<li>对抗攻击：转换为一个<strong>优化问题：</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2017.38.05.png" alt="截屏2024-08-11 17.38.05" style="zoom: 25%;" />

<h2 id="Day9"><a href="#Day9" class="headerlink" title="Day9"></a>Day9</h2><h3 id="Graph-Transformers"><a href="#Graph-Transformers" class="headerlink" title="Graph Transformers"></a>Graph Transformers</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-11%2018.07.48.png" alt="截屏2024-08-11 18.07.48"></p>
<h4 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h4><ol>
<li>针对每一个序列输入，嵌入为 embedding 向量。</li>
<li>使用模型参数对每一个输入计算query、key、value：</li>
<li><strong>使用query和key计算权重分数（可以是内积之后使用softmax转换为概率）</strong>，使用权重分数乘以value值，得到输出向量。</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2014.31.36.png" style="zoom:25%;" />

<h4 id="GNN-v-s-Transformer"><a href="#GNN-v-s-Transformer" class="headerlink" title="GNN v.s. Transformer"></a>GNN v.s. Transformer</h4><ul>
<li>Difference: GNNs use message passing, Transformer uses self-attention</li>
<li>自注意力的计算可以写作：通过矩阵进行并行计算</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2015.14.54.png" alt="截屏2024-08-12 15.14.54" style="zoom: 33%;" />

<ul>
<li>将上述的计算过程可以<strong>写为 MSG-AGG 的形式：</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2015.15.39.png" alt="截屏2024-08-12 15.15.39" style="zoom:33%;" />

<ul>
<li><p>说明：<strong>自注意力就是一个 GNN，其中每一个token代表一个node</strong></p>
<ul>
<li>每一个 node，其它<strong>邻节点消息传递的方式是生成key和value。</strong></li>
<li>自身的消息传递（自循环）是生成query。</li>
<li>使用Q、K、V对所有消息进行聚合。</li>
</ul>
</li>
<li><p>显然这个GNN是一个完全图，所有的节点都相互连接。</p>
</li>
</ul>
<h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><blockquote>
<p>在任何一门语言中，词语的位置和顺序对句子意思表达都是至关重要的。传统RNN模型天然有序，在处理句子时，以序列的模式逐个处理句子中的词语，这使得词语的顺序信息在处理过程中被天然的保存下来，并不需要额外的处理。</p>
<p>由于Transformer模型没有RNN（循环神经网络）或CNN（卷积神经网络）结构，句子中的词语都是同时进入网络进行处理，所以没有明确的关于单词在源句子中位置的相对或绝对的信息。为了让模型理解序列中每个单词的位置（顺序），Transformer论文中提出了使用一种叫做 Positional Encoding（位置编码） 的技术。这种技术通过为每个单词添加一个额外的编码来表示它在序列中的位置，这样模型就能够理解单词在序列中的相对位置。</p>
</blockquote>
<ul>
<li>如下图所示，如果token序列的顺序改变，预测结果相同。说明预测没有考虑到token的位置顺序关系。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2015.48.57.png" alt="截屏2024-08-12 15.48.57" style="zoom: 25%;" />

<ul>
<li><p><strong>Transformer不知道token的位置关系，需要额外使用位置编码作为特征。</strong></p>
</li>
<li><p>两种方法：</p>
<ol>
<li>作为可学习的参数。</li>
<li>作为预训练的参数。</li>
</ol>
</li>
</ul>
<h4 id="使用Transformers处理Graph"><a href="#使用Transformers处理Graph" class="headerlink" title="使用Transformers处理Graph"></a>使用Transformers处理Graph</h4><ul>
<li>不同组件的对应关系如下：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2016.14.59.png" alt="截屏2024-08-12 16.14.59" style="zoom: 33%;" />

<ul>
<li><strong>图的节点特征 —— token</strong><ul>
<li>容易实现，只需要将node features作为transformers的输入token seq即可。</li>
</ul>
</li>
<li><strong>节点的邻接信息 —— 位置编码</strong><ul>
<li>如何实现？</li>
<li><strong>（1）使用相对距离：</strong>复用上一节课程的position-aware位置感知的GNN的思想。在图中采样锚点集合，使用节点到锚点的距离作为其位置的表征。</li>
<li><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2016.25.47.png" alt="截屏2024-08-12 16.25.47" style="zoom: 25%;" /></li>
<li>（2）<strong>使用拉普拉斯矩阵的特征向量进行位置编码：</strong></li>
<li><strong>拉普拉斯矩阵</strong>？也称为<strong>调和矩阵</strong>。即<strong>度数矩阵-邻接矩阵</strong>。（它的<strong>特征值和特征向量可以提供图的结构信息</strong>，例如图的连通性和社群结构。）</li>
<li><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2016.33.05.png" alt="截屏2024-08-12 16.33.05" style="zoom:33%;" /></li>
<li>使用拉普拉斯矩阵的特征向量（适合transformer的向量input），其表征了图的结构信息。<strong>其中对应特征值大的特征向量表示全局结构，特征值小的特征向量表示局部结构。</strong></li>
<li><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2016.39.45.png" alt="截屏2024-08-12 16.39.45" style="zoom:25%;" /></li>
</ul>
</li>
<li><strong>边的特征 —— 自注意力</strong>（自注意力使得信息在节点之间交互，所以对应连接边的作用）<ul>
<li>在使用Transformer处理图时，如何考虑边特征信息？</li>
<li>如下图所示，<strong>将边的特征加入注意力的计算中：</strong>在计算了节点 i 和节点 j 的注意力分数后，加入两节点的边特征信息。</li>
<li><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/截屏2024-08-12 17.06.09.png" alt="截屏2024-08-12 17.06.09" style="zoom:33%;" /></li>
</ul>
</li>
</ul>
<h4 id="SignNet"><a href="#SignNet" class="headerlink" title="SignNet"></a>SignNet</h4><ul>
<li><p><strong>问题：</strong>上述在使用Transformers处理图的过程中，使用拉普拉斯矩阵的特征向量作为位置编码。这么做有一个缺陷：由于特征向量具有符号不确定性（特征向量的方向无法确定，正负对称性），所以可能导致位置编码会出现不一致的情况，会严重影响模型的预测能力。</p>
</li>
<li><p>解决：可以使用一种通用的思想：SignNet用于解决符号不确定的问题。</p>
</li>
<li><p><strong>设计目标：</strong>设计一个<strong>函数映射（神经网络）</strong>使得<strong>对不同符号的向量可以映射到同一个位置</strong>。（这个函数映射必须要有可表达能力）</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2018.11.00.png" alt="截屏2024-08-12 18.11.00" style="zoom:33%;" />

<ul>
<li>显然，一个偶函数可以表达如下：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2018.15.23.png" alt="截屏2024-08-12 18.15.23" style="zoom:25%;" />

<ul>
<li>SignNet核心思想如下：为了摆脱符号依赖，拆解偶函数。其中的$\phi$是一个公用的神经网络。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2018.16.30.png" alt="截屏2024-08-12 18.16.30" style="zoom:25%;" />

<ul>
<li>表达能力：SignNet can express all sign invariant functions!!</li>
</ul>
<h4 id="流程（使用SignNet）"><a href="#流程（使用SignNet）" class="headerlink" title="流程（使用SignNet）"></a>流程（使用SignNet）</h4><ul>
<li>位置编码使用：<strong>SignNet输出的符号无关的位置信息。</strong></li>
<li>SignNet参数和预测模型参数一同梯度回传和训练。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2018.20.19.png" alt="截屏2024-08-12 18.20.19" style="zoom: 33%;" />



<h2 id="Day10"><a href="#Day10" class="headerlink" title="Day10"></a>Day10</h2><h3 id="Scaling-to-large-graphs"><a href="#Scaling-to-large-graphs" class="headerlink" title="Scaling to large graphs"></a>Scaling to large graphs</h3><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2019.39.15.png" alt="截屏2024-08-12 19.39.15"></p>
<blockquote>
<p>由于目前的应用（例如推荐系统、社交网络分析等等），图的规模巨大（10M-10B node&#x2F;edge），需要考虑如何将GNNs适配到大规模的图上。</p>
</blockquote>
<h4 id="整图全批次（full-batch）训练"><a href="#整图全批次（full-batch）训练" class="headerlink" title="整图全批次（full-batch）训练"></a>整图全批次（full-batch）训练</h4><ul>
<li>在第 $k$ 层，得到每个节点的embedding向量。</li>
<li><strong>同时对所有节点进行更新（消息传递和聚合）</strong>，得到第 $k+1$ 层的embedding向量。</li>
<li>但是不适用于大规模的图训练（<strong>GPU内存不足</strong>以存储整个图）</li>
</ul>
<h4 id="GPU内存不足？SubGraph-Sampling子图采样"><a href="#GPU内存不足？SubGraph-Sampling子图采样" class="headerlink" title="GPU内存不足？SubGraph Sampling子图采样"></a>GPU内存不足？SubGraph Sampling子图采样</h4><ul>
<li>将大规模的图进行采样，使用子图存入GPU中用于训练。</li>
<li><strong>如何采样（什么样的子图好）？</strong>should retain edge connectivity structure of the original graph as much as possible.【应该尽可能保持图中的边连接结构】</li>
</ul>
<h4 id="Cluster-GCN"><a href="#Cluster-GCN" class="headerlink" title="Cluster-GCN"></a>Cluster-GCN</h4><ul>
<li>核心思想：为了对整图进行采样，<strong>使用 group detection（例如节点聚类，分为一个一个的群体）的方法，将图分为一些列的节点簇，在它们上面分别进行训练。</strong></li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-12%2022.49.47.png" alt="截屏2024-08-12 22.49.47" style="zoom:25%;" />

<ul>
<li>Cluster-GCN在进行GNNs训练时，不考虑不同节点簇之间的连接关系。</li>
<li><strong>有利于捕捉图的局部（group内）连接结构信息。</strong></li>
<li>缺陷：<ol>
<li>显然，<strong>无法捕捉groups之间的连接结构，损害 GNNs 性能。</strong></li>
<li>由于子图的采样只涉及了图的一小部份节点和边，导致不同的group之间计算的梯度可能方差比较大，训练收敛比较慢。</li>
</ol>
</li>
</ul>
<h4 id="通过简化-GNNs-的模型架构来增强其可扩展性"><a href="#通过简化-GNNs-的模型架构来增强其可扩展性" class="headerlink" title="通过简化 GNNs 的模型架构来增强其可扩展性"></a>通过简化 GNNs 的模型架构来增强其可扩展性</h4><h5 id="LightGCN"><a href="#LightGCN" class="headerlink" title="LightGCN"></a>LightGCN</h5><ul>
<li><p>前文中提到过的LightGCN的主要思想就是<strong>去除节点特征的非线性变换</strong>来减少计算复杂度。这使得模型更加轻量级，适合处理大规模图数据。</p>
</li>
<li><p>通过邻接矩阵和度矩阵得到 <strong>正则后的邻接矩阵</strong>$\widetilde{A}$；通过初始的node embedding得到<strong>嵌入矩阵</strong>$E^{}$</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-13%2011.15.08.png" alt="截屏2024-08-13 11.15.08" style="zoom:25%;" />

<ul>
<li><strong>普通 GCN 的矩阵计算形式（矩阵计算并行化程度高，加快计算速度）</strong>如下式所示。</li>
</ul>
<p>$$<br>E^{(k+1)} &#x3D; \text{ReLU}(\widetilde{A}E^{(k)}W^{(k)})<br>$$</p>
<ul>
<li>其中的 正则邻接矩阵$\widetilde{A}$ 对嵌入矩阵 $E^{(k)}$进行扩散，权重矩阵$W^(k)$进行线性特征变换。ReLU进行非线性变换</li>
<li>LightGCN 发现删除权重矩阵$W^(k)$ 几乎不会影响模型的性能。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-13%2011.25.16.png" alt="截屏2024-08-13 11.25.16" style="zoom:25%;" />

<ul>
<li><p>即变为：<br>  $$<br>  E^{(k+1)} &#x3D; \widetilde{A}E^{(k)}<br>  $$</p>
</li>
<li><p>所以LightGCN几乎只对节点的嵌入特征进行扩散（即相邻的节点趋向于拥有相似的嵌入特征）。</p>
</li>
</ul>
<h2 id="Day11-In-Context-Learning-Over-Graph"><a href="#Day11-In-Context-Learning-Over-Graph" class="headerlink" title="Day11 In-Context Learning Over Graph"></a>Day11 In-Context Learning Over Graph</h2><h4 id="上下文学习（ICL）"><a href="#上下文学习（ICL）" class="headerlink" title="上下文学习（ICL）"></a>上下文学习（ICL）</h4><blockquote>
<p>综述论文🔗：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.00234">A Survey on In-context Learning</a></p>
</blockquote>
<p>ICL，即In-Context Learning，是一种让<strong>大型语言模型（LLMs）通过少量标注样本在特定任务上进行学习的方法。</strong>这种方法的核心思想是，<strong>通过设计任务相关的指令形成提示模板，利用少量标注样本作为提示，引导模型在新的测试数据上生成预测结果。</strong></p>
<ul>
<li>ICL主要思路是：<strong>给出少量的标注样本，设计任务相关的指令形成提示模板，用于指导待测试样本生成相应的结果。</strong></li>
<li>ICL的过程：并<strong>不涉及到梯度的更新，因为整个过程不属于fine-tuning范畴【预训练的大模型不需要进行微调，只需要根据提示，也就是上下文（in-context），就能够完成特定的任务。】。</strong>而是将一些带有标签的样本拼接起来，作为prompt的一部分，引导模型在新的测试数据输入上生成预测结果。</li>
<li>ICL方法：表现大幅度超越了Zero-Shot-Learning，为少样本学习提供了新的研究思路。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-14%2010.43.12.png" alt="截屏2024-08-14 10.43.12" style="zoom: 50%;" />

<ul>
<li><p>ICL需要一个<strong>包含一些用自然语言模板编写的演示示例的提示上下文。</strong>以这个提示和查询为输入，大型语言模型负责做出预测。</p>
</li>
<li><p>形式化定义如下：</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-14%2012.29.23.png" alt="截屏2024-08-14 12.29.23" style="zoom:33%;" />

<h4 id="如下在Graph上进行ICL？"><a href="#如下在Graph上进行ICL？" class="headerlink" title="如下在Graph上进行ICL？"></a>如下在Graph上进行ICL？</h4><ul>
<li>使用一个<strong>两阶段模型</strong>：</li>
</ul>
<img src="/Users/lbyyds/Library/Application Support/typora-user-images/截屏2024-08-14 12.38.34.png" alt="截屏2024-08-14 12.38.34" style="zoom:33%;" />

<p><strong>（1）构造Data Graphs</strong>：以Link Prediction为例，进行图采样选取prompt examples集合。</p>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-14%2012.41.40.png" alt="截屏2024-08-14 12.41.40" style="zoom: 33%;" />

<p><strong>（2）构造Task Graphs：</strong>使用GNN对数据图进行encode</p>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-14%2012.46.47.png" alt="截屏2024-08-14 12.46.47" style="zoom:33%;" />





<h2 id="Day12-Algorithmic-reasoning-with-GNNs"><a href="#Day12-Algorithmic-reasoning-with-GNNs" class="headerlink" title="Day12 Algorithmic reasoning with GNNs"></a>Day12 Algorithmic reasoning with GNNs</h2><p><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2013.39.02.png" alt="截屏2024-08-20 13.39.02"></p>
<h4 id="算法推理和GNNs"><a href="#算法推理和GNNs" class="headerlink" title="算法推理和GNNs"></a>算法推理和GNNs</h4><ul>
<li>经典意义上的算法例如贪心、分值、递归等等，但是<strong>GNNs和这些经典算法的关系并不清晰。</strong></li>
<li>探究：如何从算法的角度解释GNNs？它们之间的关系？GNNs架构和算法的关系和转换？</li>
</ul>
<h4 id="GNNs-和-1-WL-同构测试"><a href="#GNNs-和-1-WL-同构测试" class="headerlink" title="GNNs 和 1-WL 同构测试"></a>GNNs 和 1-WL 同构测试</h4><ul>
<li><p>简单回顾1-WL同构测试（用于图同构的判断，主要过程是：<strong>节点颜色初始化-使用邻居节点进行颜色哈希和传递-直到颜色稳定-读取图节点颜色直方图-判断图同构</strong>）</p>
<blockquote>
<h5 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h5><ol>
<li><strong>初始标记</strong>: 每个节点根据其初始属性（或没有属性时，所有节点赋予相同的初始标签）进行标记，通常使用单一的标签开始。</li>
<li><strong>邻居聚合</strong>: 对于每个节点，将其当前标签与其邻居的标签集合进行组合，形成一个新的标签。例如，可以将节点的标签和所有邻居的标签进行排序，然后将排序后的标签连接在一起。</li>
<li><strong>标签压缩</strong>: 将每个节点的组合标签映射到一个新的紧凑标签（例如通过哈希函数）。这一过程通常称为“标签压缩”。</li>
<li><strong>重复迭代</strong>: 对整个图重复步骤2和步骤3若干次。在每次迭代中，节点的标签会越来越复杂，越来越能反映其在图中的结构位置。</li>
<li><strong>比较标签分布</strong>: 在经过若干次迭代后，比较两个图中所有节点的标签分布。如果在某次迭代后，两个图的标签分布不同，则可以断定这两个图不可能是同构的。</li>
</ol>
</blockquote>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2013.53.06.png" alt="截屏2024-08-20 13.53.06" style="zoom:33%;" />

<ul>
<li><strong>GIN（Graph Isomorphism Network） is as expressive as the 1-WL test</strong> <ul>
<li>GIN Replaces HASH function with learnable MLP （将1-WL测试的哈希函数替换为了可学习的MLP）</li>
<li>GIN is a “neural version” of the 1-WL algorithm</li>
<li>这证明了：GNNs 模拟了一些经典算法的实现</li>
<li>事实上：An untrained GNN (random MLP &#x3D; random hash) is close to the 1-WL test【GNNs的参数初始化时相当于一个随机哈希函数】</li>
</ul>
</li>
</ul>
<h4 id="神经网络作为算法"><a href="#神经网络作为算法" class="headerlink" title="神经网络作为算法"></a>神经网络作为算法</h4><ul>
<li>这一节讨论的不是神经网络的表达能力（expressive power），而是其在少数据样本下的能力。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.31.38.png" alt="截屏2024-08-20 14.31.38" style="zoom:33%;" />

<ul>
<li>MLPs 可以<strong>轻松【easily，在少数据下】</strong>模拟一些smooth functions（例如线性、log函数等）；但是对于相对复杂的函数，例如求和、for循环等，需要大数据样本的学习才能进行模拟。</li>
</ul>
<h5 id="特征抽取问题：MLP可以轻松模拟"><a href="#特征抽取问题：MLP可以轻松模拟" class="headerlink" title="特征抽取问题：MLP可以轻松模拟"></a>特征抽取问题：MLP可以轻松模拟</h5><ul>
<li>即从多个输入中选取特征（类似于一个线性函数），MLP可以轻松模拟</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.35.45.png" alt="截屏2024-08-20 14.35.45" style="zoom: 33%;" />

<h5 id="对于概要统计问题：MLP无法轻松模拟"><a href="#对于概要统计问题：MLP无法轻松模拟" class="headerlink" title="对于概要统计问题：MLP无法轻松模拟"></a>对于概要统计问题：MLP无法轻松模拟</h5><ul>
<li>例如求坐标最大值，相当于一个for-loop求解最大值的算法，MLP无法直接进行模拟。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.39.47.png" alt="截屏2024-08-20 14.39.47" style="zoom:33%;" />



<h5 id="转换：使用嵌套MLP"><a href="#转换：使用嵌套MLP" class="headerlink" title="转换：使用嵌套MLP"></a>转换：使用嵌套MLP</h5><ul>
<li>最大值函数可以通过对数和指数函数来近似表示（通过指数函数来放大差异，从而使得输出只由最大的$x_i$决定）</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.43.29.png" alt="截屏2024-08-20 14.43.29" style="zoom: 33%;" />

<ul>
<li>其中的对数函数和指数函数都可以使用MLP模拟，建立DeepSet模型：</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.44.45.png" alt="截屏2024-08-20 14.44.45" style="zoom:33%;" />

<h5 id="Relational-Argmax问题"><a href="#Relational-Argmax问题" class="headerlink" title="Relational Argmax问题"></a>Relational Argmax问题</h5><ul>
<li>更加复杂的pairwise问题，找到距离最远的两个坐标</li>
<li><strong>相当于一个两个for-loop嵌套的问题</strong>：对于每一个点，使用一个for- loop找到最远点；对于所有点的最远距离，使用一个for-loop找到最大值。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.52.20.png" alt="截屏2024-08-20 14.52.20" style="zoom: 33%;" />

<ul>
<li>DeepSet无法解决两个for-loop嵌套的问题。</li>
</ul>
<h5 id="使用GNNs："><a href="#使用GNNs：" class="headerlink" title="使用GNNs："></a>使用GNNs：</h5><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2014.59.20.png" alt="截屏2024-08-20 14.59.20" style="zoom:33%;" />

<h5 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h5><img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2015.00.16.png" alt="截屏2024-08-20 15.00.16" style="zoom:33%;" />

<h4 id="重点：Algorithmic-Alignment"><a href="#重点：Algorithmic-Alignment" class="headerlink" title="重点：Algorithmic Alignment"></a>重点：Algorithmic Alignment</h4><ul>
<li>在使用一个NN结构模拟目标算法时，或者使用算法来公式化一个NN架构时，使用如下的等价方法。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2015.15.52.png" alt="截屏2024-08-20 15.15.52" style="zoom:33%;" />







<h2 id="Day13"><a href="#Day13" class="headerlink" title="Day13"></a>Day13</h2><h4 id="GNNs-Design-space"><a href="#GNNs-Design-space" class="headerlink" title="GNNs Design space"></a>GNNs Design space</h4><ul>
<li>GNNs（Graph Neural Networks）的Design space指的是<strong>设计和构建GNN模型时的可选组件和方法的范围或集合。</strong>这个空间包括了<strong>不同的设计选择、架构模块和操作</strong>，它们共同决定了GNN的表现和适用性。Design space的探索旨在优化GNN模型以更好地适应不同类型的图数据和任务需求。</li>
<li>对于不同的任务，不同的GNNs性能不同。没有一个通用的SOTA模型可以适用于所有图任务。</li>
</ul>
<h4 id="GNNs-Task-Space"><a href="#GNNs-Task-Space" class="headerlink" title="GNNs Task Space"></a>GNNs Task Space</h4><ul>
<li><p>如何<strong>将GNN适用的任务进行分类</strong>？如果仅仅按照node&#x2F;link&#x2F;graph level进行分类，精确性太差。</p>
</li>
<li><p>如何定义<strong>可以量化的图任务相似度的指标</strong>？作用：可以用于对不同的图任务快速挑选合适的GNNs模型；否则很困难。</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2015.50.54.png" alt="截屏2024-08-20 15.50.54" style="zoom:33%;" />

<h5 id="锚点模型"><a href="#锚点模型" class="headerlink" title="锚点模型"></a>锚点模型</h5><ul>
<li><strong>挑选固定的锚定模型</strong>，作为不同任务的特征之一进行排序和相似性计算。</li>
<li>思考：例如对于任务A和任务B都属于node classification任务，但是不清楚其相似性。可以使用挑选出的锚点模型进行判断。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2015.52.33.png" alt="截屏2024-08-20 15.52.33" style="zoom:33%;" />

<ul>
<li><strong>如何挑选这些“Anchor”模型？</strong>直觉是：Anchor模型必须具有良好的表征性和多样性。<ul>
<li>可以通过给定一个小的数据机，随机采样N个模型训练并性能排序，最后选择性能广度较大的M个模型。（目的是尽可能包含性能好的和性能差的模型）</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2016.09.18.png" alt="截屏2024-08-20 16.09.18" style="zoom:33%;" />

<ul>
<li><strong>在新的task进入时，使用锚定模型计算性能，并通过和其它tasks计算相似度，从而推荐相似任务的最优模型给新启动的task。</strong>（类似于推荐系统的冷启动）</li>
</ul>
<h4 id="如何评估GNNs的Design？"><a href="#如何评估GNNs的Design？" class="headerlink" title="如何评估GNNs的Design？"></a>如何评估GNNs的Design？</h4><ul>
<li>随机采样 <strong>任务-模型对【task-model pair】</strong>，在这些pairs中进行消融分析（例如对于BN，分别训练含有BN和不含有BN，对其结果进行统计和评估。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2016.17.07.png" alt="截屏2024-08-20 16.17.07" style="zoom:33%;" />

<ul>
<li>可能对于不同的任务和数据集，使用不同的Design具有不同的性能。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/02lb/img_picGo@main/img_data/%E6%88%AA%E5%B1%8F2024-08-20%2016.18.59.png" alt="截屏2024-08-20 16.18.59" style="zoom:33%;" />









<p>——————————————————🌹🌹🌹🌹🌹🌹 END 🌹🌹🌹🌹🌹🌹🌹————————————————————</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="https://github.com/02lb">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">图表示学习&#x2F;图机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GuideLines"><span class="toc-number">1.0.0.1.</span> <span class="toc-text">GuideLines</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day1"><span class="toc-number">1.1.</span> <span class="toc-text">Day1:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Node-Embedding"><span class="toc-number">1.1.1.</span> <span class="toc-text">Node Embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Encoder-Decoder-Framework"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Encoder - Decoder Framework</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%B8%B8%E8%B5%B0%EF%BC%9ARandom-Walk%EF%BC%9A"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">随机游走：Random Walk：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%9D%E6%83%B3%EF%BC%88Idea%EF%BC%89"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text">思想（Idea）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%EF%BC%88Expressivity%EF%BC%89"><span class="toc-number">1.1.1.2.2.</span> <span class="toc-text">表达能力（Expressivity）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7-Negative-Sampling%EF%BC%9A"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">负采样 Negative Sampling：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.3.1.</span> <span class="toc-text">负采样的原理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%95%B0%E9%87%8F-K-%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.3.2.</span> <span class="toc-text">负样本数量 (K) 的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">1.1.1.3.3.</span> <span class="toc-text">负样本的选择</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%B4%9F%E9%87%87%E6%A0%B7%E5%85%AC%E5%BC%8F%E5%8F%8A%E5%85%B6%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.1.3.4.</span> <span class="toc-text">解释负采样公式及其原理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Node-Embedding-%EF%BC%9A-DeepWalk-Node2Vec"><span class="toc-number">1.1.2.</span> <span class="toc-text">Node Embedding ： DeepWalk &#x2F; Node2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Embedding-%EF%BC%9A%E5%9B%BE%E5%B5%8C%E5%85%A5"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">Graph Embedding ：图嵌入</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day1-Lab"><span class="toc-number">1.2.</span> <span class="toc-text">Day1-Lab:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day2"><span class="toc-number">1.3.</span> <span class="toc-text">Day2:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CORE%EF%BC%9AGraph-Neural-Network"><span class="toc-number">1.3.1.</span> <span class="toc-text">CORE：Graph Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-vs-Image"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Graph vs Image</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-Basics"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">GNN Basics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Classic-GNN"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Classic GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day3"><span class="toc-number">1.4.</span> <span class="toc-text">Day3</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9%EF%BC%9AGNN"><span class="toc-number">1.4.0.1.</span> <span class="toc-text">主要内容：GNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Manipulation%E3%80%90%E5%9B%BE%E5%A4%84%E7%90%86%E3%80%91"><span class="toc-number">1.4.0.2.</span> <span class="toc-text">Graph Manipulation【图处理】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#unsupervised-self-supervised"><span class="toc-number">1.4.0.3.</span> <span class="toc-text">unsupervised &#x2F; self- supervised</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-number">1.4.0.4.</span> <span class="toc-text">评估指标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%92%E5%88%86"><span class="toc-number">1.4.0.5.</span> <span class="toc-text">图数据的划分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#The-Power-of-GNN"><span class="toc-number">1.4.0.6.</span> <span class="toc-text">The Power of GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day3-Lab"><span class="toc-number">1.5.</span> <span class="toc-text">Day3-Lab</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84GCN%E7%94%A8%E4%BA%8E%E8%8A%82%E7%82%B9%E9%A2%84%E6%B5%8B"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">实现一个简单的GCN用于节点预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day4"><span class="toc-number">1.6.</span> <span class="toc-text">Day4</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Heterogenous-graphs-%E5%BC%82%E8%B4%A8%E5%9B%BE"><span class="toc-number">1.6.0.1.</span> <span class="toc-text">Heterogenous graphs[异质图]</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%88Relational-Graph-Convolutional-Network-R-GCN%EF%BC%89"><span class="toc-number">1.6.0.2.</span> <span class="toc-text">关系图卷积网络（Relational Graph Convolutional Network, R-GCN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-vs-Heterogenous-GNN"><span class="toc-number">1.6.0.3.</span> <span class="toc-text">GNN vs Heterogenous GNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day5"><span class="toc-number">1.7.</span> <span class="toc-text">Day5</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Knowledge-Graphs"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">Knowledge Graphs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">关系模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KG-Completion"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">KG Completion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#KG-Reasoning"><span class="toc-number">1.7.0.4.</span> <span class="toc-text">KG Reasoning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Query2Box"><span class="toc-number">1.7.0.5.</span> <span class="toc-text">Query2Box</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day6"><span class="toc-number">1.8.</span> <span class="toc-text">Day6</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Fast-neural-subgraph-matching-and-Counting"><span class="toc-number">1.8.0.1.</span> <span class="toc-text">Fast neural subgraph matching and Counting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%90%E5%9B%BE"><span class="toc-number">1.8.0.2.</span> <span class="toc-text">子图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Isomorphism-%E5%9B%BE%E5%90%8C%E6%9E%84"><span class="toc-number">1.8.0.3.</span> <span class="toc-text">Graph Isomorphism 图同构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Network-motifs-%E7%BD%91%E7%BB%9C%E5%9B%BE%E6%A1%88"><span class="toc-number">1.8.0.4.</span> <span class="toc-text">Network motifs 网络图案</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Neural-Subgraph-Match"><span class="toc-number">1.8.0.5.</span> <span class="toc-text">Neural Subgraph Match</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%EF%BC%9A%E4%BD%BF%E7%94%A8Order-Embedding-%E6%9C%89%E5%BA%8F%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4"><span class="toc-number">1.8.0.6.</span> <span class="toc-text">方法：使用Order Embedding 有序嵌入空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%89%BE%E5%87%BA-Frequent-SubGraphs%E3%80%90%E5%8D%B3-motifs%E3%80%91"><span class="toc-number">1.8.0.7.</span> <span class="toc-text">如何找出 Frequent SubGraphs【即 motifs】</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SPMiner"><span class="toc-number">1.8.0.8.</span> <span class="toc-text">SPMiner</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day7"><span class="toc-number">1.9.</span> <span class="toc-text">Day7</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8GNN%E8%BF%9B%E8%A1%8C%E6%8E%A8%E8%8D%90"><span class="toc-number">1.9.1.</span> <span class="toc-text">利用GNN进行推荐</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84Graph"><span class="toc-number">1.9.1.1.</span> <span class="toc-text">推荐系统中的Graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Embedding-based-models"><span class="toc-number">1.9.1.2.</span> <span class="toc-text">Embedding-based models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#representative-GNN-approaches-for-recommender-systems"><span class="toc-number">1.9.1.3.</span> <span class="toc-text">representative GNN approaches for recommender systems</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81Neural-Graph-Collaborative-Filtering-NGCF"><span class="toc-number">1.9.1.4.</span> <span class="toc-text">一、Neural Graph Collaborative Filtering (NGCF)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81LightGCN"><span class="toc-number">1.9.1.5.</span> <span class="toc-text">二、LightGCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%B0%86%E6%89%A9%E5%B1%95%E5%88%B0%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%EF%BC%9FPinSage"><span class="toc-number">1.9.1.6.</span> <span class="toc-text">如何将扩展到大规模的推荐系统中？PinSage</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day8"><span class="toc-number">1.10.</span> <span class="toc-text">Day8</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-generative-models-for-graphs"><span class="toc-number">1.10.1.</span> <span class="toc-text">Deep generative models for graphs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E7%94%9F%E6%88%90%E9%97%AE%E9%A2%98"><span class="toc-number">1.10.1.1.</span> <span class="toc-text">图生成问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E5%9B%BE%E7%9A%84%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B%E5%BB%BA%E6%A8%A1%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%BA%8F%E5%88%97%E7%9A%84%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B"><span class="toc-number">1.10.1.2.</span> <span class="toc-text">将图的生成过程建模为一个序列的生成过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8RNN%E8%BF%9B%E8%A1%8C%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90%EF%BC%9AGraphRNN"><span class="toc-number">1.10.1.3.</span> <span class="toc-text">使用RNN进行序列生成：GraphRNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Graph-Convolutional-Policy-Network-GCPN"><span class="toc-number">1.10.1.4.</span> <span class="toc-text">Graph Convolutional Policy Network (GCPN)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day8-1"><span class="toc-number">1.11.</span> <span class="toc-text">Day8</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Advanced-topics-in-GNNs"><span class="toc-number">1.11.1.</span> <span class="toc-text">Advanced topics in GNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-%E5%AD%98%E5%9C%A8%E7%9A%84%E4%B8%8D%E8%B6%B3%E4%B9%8B%E5%A4%84"><span class="toc-number">1.11.1.1.</span> <span class="toc-text">GNNs 存在的不足之处</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Position-aware-GNNs"><span class="toc-number">1.11.1.2.</span> <span class="toc-text">Position-aware GNNs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GPT-for-This%EF%BC%9A"><span class="toc-number">1.11.1.3.</span> <span class="toc-text">GPT for This：</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.11.1.3.1.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E7%89%B9%E6%80%A7"><span class="toc-number">1.11.1.3.2.</span> <span class="toc-text">关键特性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%94%9A%E7%82%B9"><span class="toc-number">1.11.1.3.3.</span> <span class="toc-text">锚点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#P-GNN-%E8%BF%87%E7%A8%8B"><span class="toc-number">1.11.1.3.4.</span> <span class="toc-text">P-GNN 过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Identity-aware-GNNs"><span class="toc-number">1.11.1.4.</span> <span class="toc-text">Identity-aware GNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8-ID-GNN-%E4%B8%AD%E8%BF%90%E7%94%A8%E8%8A%82%E7%82%B9%E7%9D%80%E8%89%B2%E7%9A%84%E6%80%9D%E6%83%B3%EF%BC%9A%E5%BC%82%E6%9E%84%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E3%80%90Heterogenous-message-passing%E3%80%91"><span class="toc-number">1.11.1.4.1.</span> <span class="toc-text">如何在 ID-GNN 中运用节点着色的思想：异构消息传递【Heterogenous message passing】</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#ID-GNN-Fast"><span class="toc-number">1.11.1.4.2.</span> <span class="toc-text">ID-GNN-Fast</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.11.1.4.3.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7%E7%A0%94%E7%A9%B6"><span class="toc-number">1.11.1.5.</span> <span class="toc-text">GNNs 的鲁棒性研究</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day9"><span class="toc-number">1.12.</span> <span class="toc-text">Day9</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Graph-Transformers"><span class="toc-number">1.12.1.</span> <span class="toc-text">Graph Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.12.1.1.</span> <span class="toc-text">自注意力机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNN-v-s-Transformer"><span class="toc-number">1.12.1.2.</span> <span class="toc-text">GNN v.s. Transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.12.1.3.</span> <span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Transformers%E5%A4%84%E7%90%86Graph"><span class="toc-number">1.12.1.4.</span> <span class="toc-text">使用Transformers处理Graph</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SignNet"><span class="toc-number">1.12.1.5.</span> <span class="toc-text">SignNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B%EF%BC%88%E4%BD%BF%E7%94%A8SignNet%EF%BC%89"><span class="toc-number">1.12.1.6.</span> <span class="toc-text">流程（使用SignNet）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day10"><span class="toc-number">1.13.</span> <span class="toc-text">Day10</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scaling-to-large-graphs"><span class="toc-number">1.13.1.</span> <span class="toc-text">Scaling to large graphs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E5%9B%BE%E5%85%A8%E6%89%B9%E6%AC%A1%EF%BC%88full-batch%EF%BC%89%E8%AE%AD%E7%BB%83"><span class="toc-number">1.13.1.1.</span> <span class="toc-text">整图全批次（full-batch）训练</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GPU%E5%86%85%E5%AD%98%E4%B8%8D%E8%B6%B3%EF%BC%9FSubGraph-Sampling%E5%AD%90%E5%9B%BE%E9%87%87%E6%A0%B7"><span class="toc-number">1.13.1.2.</span> <span class="toc-text">GPU内存不足？SubGraph Sampling子图采样</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Cluster-GCN"><span class="toc-number">1.13.1.3.</span> <span class="toc-text">Cluster-GCN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E7%AE%80%E5%8C%96-GNNs-%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E6%9D%A5%E5%A2%9E%E5%BC%BA%E5%85%B6%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-number">1.13.1.4.</span> <span class="toc-text">通过简化 GNNs 的模型架构来增强其可扩展性</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#LightGCN"><span class="toc-number">1.13.1.4.1.</span> <span class="toc-text">LightGCN</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day11-In-Context-Learning-Over-Graph"><span class="toc-number">1.14.</span> <span class="toc-text">Day11 In-Context Learning Over Graph</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E5%AD%A6%E4%B9%A0%EF%BC%88ICL%EF%BC%89"><span class="toc-number">1.14.0.1.</span> <span class="toc-text">上下文学习（ICL）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%B8%8B%E5%9C%A8Graph%E4%B8%8A%E8%BF%9B%E8%A1%8CICL%EF%BC%9F"><span class="toc-number">1.14.0.2.</span> <span class="toc-text">如下在Graph上进行ICL？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day12-Algorithmic-reasoning-with-GNNs"><span class="toc-number">1.15.</span> <span class="toc-text">Day12 Algorithmic reasoning with GNNs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%8E%A8%E7%90%86%E5%92%8CGNNs"><span class="toc-number">1.15.0.1.</span> <span class="toc-text">算法推理和GNNs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-%E5%92%8C-1-WL-%E5%90%8C%E6%9E%84%E6%B5%8B%E8%AF%95"><span class="toc-number">1.15.0.2.</span> <span class="toc-text">GNNs 和 1-WL 同构测试</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.15.0.2.1.</span> <span class="toc-text">具体步骤</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%BA%E7%AE%97%E6%B3%95"><span class="toc-number">1.15.0.3.</span> <span class="toc-text">神经网络作为算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E9%97%AE%E9%A2%98%EF%BC%9AMLP%E5%8F%AF%E4%BB%A5%E8%BD%BB%E6%9D%BE%E6%A8%A1%E6%8B%9F"><span class="toc-number">1.15.0.3.1.</span> <span class="toc-text">特征抽取问题：MLP可以轻松模拟</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%B9%E4%BA%8E%E6%A6%82%E8%A6%81%E7%BB%9F%E8%AE%A1%E9%97%AE%E9%A2%98%EF%BC%9AMLP%E6%97%A0%E6%B3%95%E8%BD%BB%E6%9D%BE%E6%A8%A1%E6%8B%9F"><span class="toc-number">1.15.0.3.2.</span> <span class="toc-text">对于概要统计问题：MLP无法轻松模拟</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%B5%8C%E5%A5%97MLP"><span class="toc-number">1.15.0.3.3.</span> <span class="toc-text">转换：使用嵌套MLP</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Relational-Argmax%E9%97%AE%E9%A2%98"><span class="toc-number">1.15.0.3.4.</span> <span class="toc-text">Relational Argmax问题</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8GNNs%EF%BC%9A"><span class="toc-number">1.15.0.3.5.</span> <span class="toc-text">使用GNNs：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">1.15.0.3.6.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E7%82%B9%EF%BC%9AAlgorithmic-Alignment"><span class="toc-number">1.15.0.4.</span> <span class="toc-text">重点：Algorithmic Alignment</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Day13"><span class="toc-number">1.16.</span> <span class="toc-text">Day13</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-Design-space"><span class="toc-number">1.16.0.1.</span> <span class="toc-text">GNNs Design space</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GNNs-Task-Space"><span class="toc-number">1.16.0.2.</span> <span class="toc-text">GNNs Task Space</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%94%9A%E7%82%B9%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.16.0.2.1.</span> <span class="toc-text">锚点模型</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0GNNs%E7%9A%84Design%EF%BC%9F"><span class="toc-number">1.16.0.3.</span> <span class="toc-text">如何评估GNNs的Design？</span></a></li></ol></li></ol></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://02lb.github.io/2024/07/21/GraphML-CS224w/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&text=GraphML-CS224w[图机器学习]"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&is_video=false&description=GraphML-CS224w[图机器学习]"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=GraphML-CS224w[图机器学习]&body=Check out this article: https://02lb.github.io/2024/07/21/GraphML-CS224w/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&title=GraphML-CS224w[图机器学习]"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://02lb.github.io/2024/07/21/GraphML-CS224w/&name=GraphML-CS224w[图机器学习]&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://02lb.github.io/2024/07/21/GraphML-CS224w/&t=GraphML-CS224w[图机器学习]"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2024
    Lee
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/02lb">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
